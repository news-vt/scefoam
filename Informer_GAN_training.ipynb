{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ============================== Device Configuration ==============================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"[Info] Using device: {device}\")\n",
    "\n",
    "# ============================== Global Config & Paths ==============================\n",
    "TRAIN_FOLDER    = \"bdd100k/videos/train\"\n",
    "VAL_FOLDER      = \"bdd100k/videos/val\"\n",
    "CHECKPOINT_DIR  = \"checkpoints\"\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "# Training Parameters\n",
    "MAX_TRAIN_VIDS = 5  # Set to None to use all training videos\n",
    "MAX_VAL_VIDS   = 5  # Set to None to use all validation videos\n",
    "NUM_EPOCHS     = 20    # Increased epochs for better training\n",
    "FRAME_1_SCALE  = 0.25\n",
    "FRAME_2_SCALE  = 0.25\n",
    "SIM_THRESHOLD  = 0.96\n",
    "BATCH_SIZE     = 8     # Reduced batch size if memory allows\n",
    "LEARNING_RATE  = 1e-4\n",
    "\n",
    "# Patch Configuration\n",
    "PATCH_SIZE = 10  # Each patch is 10x10 pixels\n",
    "\n",
    "# Model Hyperparameters\n",
    "EMBED_DIM  = 256  # Transformer embedding dimension\n",
    "NUM_HEADS  = 8\n",
    "NUM_LAYERS = 6    # Reduced layers for faster training; adjust as needed\n",
    "D_FF       = 512  # Feedforward layer size\n",
    "DROPOUT    = 0.1\n",
    "\n",
    "# Original Frame Sizes\n",
    "ORIG_INPUT_HEIGHT, ORIG_INPUT_WIDTH  = 1280, 720\n",
    "ORIG_OUTPUT_HEIGHT, ORIG_OUTPUT_WIDTH = 1280, 720\n",
    "\n",
    "# Scaled Frame Sizes\n",
    "SCALED_INPUT_HEIGHT = int(ORIG_INPUT_HEIGHT * FRAME_1_SCALE)\n",
    "SCALED_INPUT_WIDTH  = int(ORIG_INPUT_WIDTH * FRAME_1_SCALE)\n",
    "SCALED_OUTPUT_HEIGHT = int(ORIG_OUTPUT_HEIGHT * FRAME_2_SCALE)\n",
    "SCALED_OUTPUT_WIDTH  = int(ORIG_OUTPUT_WIDTH * FRAME_2_SCALE)\n",
    "\n",
    "# Number of Patches\n",
    "NUM_PATCHES_INPUT_H  = SCALED_INPUT_HEIGHT // PATCH_SIZE\n",
    "NUM_PATCHES_INPUT_W  = SCALED_INPUT_WIDTH  // PATCH_SIZE\n",
    "NUM_PATCHES_OUTPUT_H = SCALED_OUTPUT_HEIGHT // PATCH_SIZE\n",
    "NUM_PATCHES_OUTPUT_W = SCALED_OUTPUT_WIDTH  // PATCH_SIZE\n",
    "\n",
    "NUM_PATCHES_INPUT  = NUM_PATCHES_INPUT_H * NUM_PATCHES_INPUT_W\n",
    "NUM_PATCHES_OUTPUT = NUM_PATCHES_OUTPUT_H * NUM_PATCHES_OUTPUT_W\n",
    "\n",
    "assert NUM_PATCHES_INPUT == NUM_PATCHES_OUTPUT, \"Input and output patch numbers must be the same.\"\n",
    "\n",
    "# Patch Vector Sizes\n",
    "PATCH_VECTOR_SIZE_INPUT  = 3 * PATCH_SIZE * PATCH_SIZE  # 3 channels (RGB)\n",
    "PATCH_VECTOR_SIZE_OUTPUT = PATCH_VECTOR_SIZE_INPUT      # Assuming same\n",
    "\n",
    "# Model Input and Output Sizes (Per Patch)\n",
    "INPUT_SIZE  = PATCH_VECTOR_SIZE_INPUT   # 300\n",
    "OUTPUT_SIZE = PATCH_VECTOR_SIZE_OUTPUT  # 300\n",
    "\n",
    "# Print Expected Dimensions for Verification\n",
    "print(\"\\n===== FRAME & PATCH INFO =====\")\n",
    "print(f\"Frame Size (H x W): {ORIG_INPUT_HEIGHT} x {ORIG_INPUT_WIDTH}\")\n",
    "print(f\"Patch Size: {PATCH_SIZE} x {PATCH_SIZE}\")\n",
    "print(f\"Num Patches (H x W): {NUM_PATCHES_INPUT_H} x {NUM_PATCHES_INPUT_W} = {NUM_PATCHES_INPUT}\")\n",
    "\n",
    "print(\"\\n===== INPUT / OUTPUT DIMENSIONS =====\")\n",
    "print(f\"Patch Vector Size (Flattened): {PATCH_VECTOR_SIZE_INPUT}\")\n",
    "print(f\"Input Size to Model (Per Patch): {INPUT_SIZE}\")\n",
    "print(f\"Output Size from Model (Per Patch): {OUTPUT_SIZE}\")\n",
    "\n",
    "print(\"\\n===== TRANSFORMER HYPERPARAMETERS =====\")\n",
    "print(f\"Embedding Dimension (D_MODEL): {EMBED_DIM}\")\n",
    "print(f\"Num Attention Heads: {NUM_HEADS}\")\n",
    "print(f\"Num Transformer Layers: {NUM_LAYERS}\")\n",
    "print(f\"Feedforward Layer Dim: {D_FF}\")\n",
    "print(f\"Dropout Rate: {DROPOUT}\")\n",
    "\n",
    "# ============================== Informer Model Definition ==============================\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)\n",
    "        self.linear1  = nn.Linear(d_model, d_ff)\n",
    "        self.dropout   = nn.Dropout(dropout)\n",
    "        self.linear2  = nn.Linear(d_ff, d_model)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # Self-Attention\n",
    "        src2, _ = self.self_attn(src, src, src)\n",
    "        src = src + self.dropout1(src2)\n",
    "        src = self.norm1(src)\n",
    "        # Feedforward\n",
    "        src2 = self.linear2(self.dropout(F.relu(self.linear1(src))))\n",
    "        src = src + self.dropout2(src2)\n",
    "        src = self.norm2(src)\n",
    "        return src\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super().__init__()\n",
    "        self.self_attn      = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)\n",
    "        self.multihead_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)\n",
    "        self.linear1        = nn.Linear(d_model, d_ff)\n",
    "        self.dropout         = nn.Dropout(dropout)\n",
    "        self.linear2        = nn.Linear(d_ff, d_model)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, tgt, memory):\n",
    "        # Self-Attention\n",
    "        tgt2, _ = self.self_attn(tgt, tgt, tgt)\n",
    "        tgt = tgt + self.dropout1(tgt2)\n",
    "        tgt = self.norm1(tgt)\n",
    "        # Cross-Attention\n",
    "        tgt2, _ = self.multihead_attn(tgt, memory, memory)\n",
    "        tgt = tgt + self.dropout2(tgt2)\n",
    "        tgt = self.norm2(tgt)\n",
    "        # Feedforward\n",
    "        tgt2 = self.linear2(self.dropout(F.relu(self.linear1(tgt))))\n",
    "        tgt = tgt + self.dropout3(tgt2)\n",
    "        tgt = self.norm3(tgt)\n",
    "        return tgt\n",
    "\n",
    "def create_2d_position_encoding(num_patches_h, num_patches_w, embed_dim, device):\n",
    "    \"\"\"\n",
    "    Generates a 2D positional encoding tensor.\n",
    "    \"\"\"\n",
    "    pos_h = torch.arange(num_patches_h, device=device).unsqueeze(1).repeat(1, num_patches_w)\n",
    "    pos_w = torch.arange(num_patches_w, device=device).unsqueeze(0).repeat(num_patches_h, 1)\n",
    "\n",
    "    # Normalize positions\n",
    "    pos_h = pos_h.float() / num_patches_h\n",
    "    pos_w = pos_w.float() / num_patches_w\n",
    "\n",
    "    # Flatten grid into (num_patches, 2)\n",
    "    pos = torch.cat([pos_h.unsqueeze(-1), pos_w.unsqueeze(-1)], dim=-1).view(-1, 2)\n",
    "\n",
    "    # Initialize positional encoding tensor\n",
    "    pe = torch.zeros(pos.size(0), embed_dim, device=device)\n",
    "\n",
    "    # Apply sine and cosine functions\n",
    "    div_term = torch.exp(torch.arange(0, embed_dim // 2, device=device).float() * (-np.log(10000.0) / (embed_dim // 2)))\n",
    "    pe[:, 0::2] = torch.sin(pos[:, 0].unsqueeze(1) * div_term)\n",
    "    pe[:, 1::2] = torch.cos(pos[:, 1].unsqueeze(1) * div_term)\n",
    "\n",
    "    return pe  # Shape: (num_patches, embed_dim)\n",
    "\n",
    "class InformerEncoder(nn.Module):\n",
    "    def __init__(self, input_size, d_model, n_heads, d_ff, num_layers, dropout, num_patches_h, num_patches_w):\n",
    "        super().__init__()\n",
    "        self.input_projection = nn.Linear(input_size, d_model)\n",
    "        self.pos_encoding     = create_2d_position_encoding(num_patches_h, num_patches_w, d_model, device)\n",
    "        self.layers           = nn.ModuleList([\n",
    "            EncoderLayer(d_model, n_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.dropout          = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        \"\"\"\n",
    "        src: (num_patches, batch_size, input_size)\n",
    "        \"\"\"\n",
    "        src = self.input_projection(src)  # (num_patches, batch_size, d_model)\n",
    "        src = src + self.pos_encoding.unsqueeze(1)  # Add positional encoding\n",
    "        src = self.dropout(src)\n",
    "        for layer in self.layers:\n",
    "            src = layer(src)\n",
    "        return src\n",
    "\n",
    "class InformerDecoder(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, num_layers, dropout, output_size, num_patches_h, num_patches_w):\n",
    "        super().__init__()\n",
    "        self.target_projection = nn.Linear(output_size, d_model)\n",
    "        self.pos_encoding      = create_2d_position_encoding(num_patches_h, num_patches_w, d_model, device)\n",
    "        self.layers            = nn.ModuleList([\n",
    "            DecoderLayer(d_model, n_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.output_layer      = nn.Linear(d_model, output_size)\n",
    "\n",
    "    def forward(self, tgt, memory):\n",
    "        \"\"\"\n",
    "        tgt: (num_patches, batch_size, output_size)\n",
    "        memory: (num_patches, batch_size, d_model)\n",
    "        \"\"\"\n",
    "        tgt = self.target_projection(tgt)  # (num_patches, batch_size, d_model)\n",
    "        tgt = tgt + self.pos_encoding.unsqueeze(1)  # Add positional encoding\n",
    "        for layer in self.layers:\n",
    "            tgt = layer(tgt, memory)\n",
    "        out = self.output_layer(tgt)  # (num_patches, batch_size, output_size)\n",
    "        return out\n",
    "\n",
    "class Informer(nn.Module):\n",
    "    def __init__(self, input_size, d_model, n_heads, d_ff, num_layers, dropout, output_size, num_patches_h, num_patches_w):\n",
    "        super().__init__()\n",
    "        self.encoder = InformerEncoder(input_size, d_model, n_heads, d_ff, num_layers, dropout, num_patches_h, num_patches_w)\n",
    "        self.decoder = InformerDecoder(d_model, n_heads, d_ff, num_layers, dropout, output_size, num_patches_h, num_patches_w)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        \"\"\"\n",
    "        src: (num_patches, batch_size, input_size)\n",
    "        tgt: (num_patches, batch_size, output_size)\n",
    "        \"\"\"\n",
    "        memory = self.encoder(src)\n",
    "        out    = self.decoder(tgt, memory)\n",
    "        return out\n",
    "\n",
    "# ============================== Initialize the Model ==============================\n",
    "model = Informer(\n",
    "    input_size      = INPUT_SIZE,  # 300\n",
    "    d_model         = EMBED_DIM,   # 256\n",
    "    n_heads         = NUM_HEADS,   # 8\n",
    "    d_ff            = D_FF,        # 512\n",
    "    num_layers      = NUM_LAYERS,  # 6\n",
    "    dropout         = DROPOUT,     # 0.1\n",
    "    output_size     = OUTPUT_SIZE, # 300\n",
    "    num_patches_h   = NUM_PATCHES_INPUT_H,\n",
    "    num_patches_w   = NUM_PATCHES_INPUT_W\n",
    ").to(device)\n",
    "\n",
    "# Initialize weights\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.MultiheadAttention):\n",
    "        nn.init.xavier_uniform_(m.out_proj.weight)\n",
    "        if m.out_proj.bias is not None:\n",
    "            nn.init.zeros_(m.out_proj.bias)\n",
    "\n",
    "model.apply(init_weights)\n",
    "print(\"[Info] Model initialized and weights set.\")\n",
    "\n",
    "# ============================== Loss and Optimizer ==============================\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "print(\"[Info] Loss function and optimizer defined.\")\n",
    "\n",
    "# ============================== Custom Dataset Definition ==============================\n",
    "\n",
    "class PatchPairDataset(Dataset):\n",
    "    def __init__(self, pairs):\n",
    "        \"\"\"\n",
    "        pairs: List of tuples (input_patches, target_patches)\n",
    "               Each input_patches and target_patches are numpy arrays of shape (num_patches, embed_dim)\n",
    "        \"\"\"\n",
    "        self.input_patches  = [torch.tensor(inp, dtype=torch.float32) for inp, _ in pairs]\n",
    "        self.target_patches = [torch.tensor(tgt, dtype=torch.float32) for _, tgt in pairs]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_patches)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inp = self.input_patches[idx]   # Shape: (num_patches, embed_dim)\n",
    "        tgt = self.target_patches[idx]  # Shape: (num_patches, embed_dim)\n",
    "        return inp, tgt\n",
    "\n",
    "# ============================== Data Preparation ==============================\n",
    "\n",
    "def frame_similarity(img1, img2):\n",
    "    \"\"\"\n",
    "    Compute frame similarity based on absolute pixel difference.\n",
    "    Returns similarity percentage.\n",
    "    \"\"\"\n",
    "    img1 = img1.cpu().numpy() if isinstance(img1, torch.Tensor) else img1\n",
    "    img2 = img2.cpu().numpy() if isinstance(img2, torch.Tensor) else img2\n",
    "\n",
    "    if len(img1.shape) == 3:\n",
    "        img1 = cv2.cvtColor(img1, cv2.COLOR_RGB2GRAY)\n",
    "    if len(img2.shape) == 3:\n",
    "        img2 = cv2.cvtColor(img2, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    diff = cv2.absdiff(img1, img2)\n",
    "    similarity = 100 - (np.sum(diff) / (img1.shape[0] * img1.shape[1] * 255) * 100)\n",
    "    return similarity\n",
    "\n",
    "def split_into_patches(tensor, patch_size):\n",
    "    \"\"\"\n",
    "    Split a tensor into non-overlapping patches.\n",
    "    tensor: (C, H, W)\n",
    "    Returns:\n",
    "        patches: (num_patches, C, patch_size, patch_size)\n",
    "    \"\"\"\n",
    "    C, H, W = tensor.shape\n",
    "    patches = tensor.unfold(1, patch_size, patch_size).unfold(2, patch_size, patch_size)\n",
    "    patches = patches.permute(1, 2, 0, 3, 4).contiguous()\n",
    "    return patches.view(-1, C, patch_size, patch_size)  # (num_patches, C, patch_size, patch_size)\n",
    "\n",
    "def prepare_video_for_model(mov_path, threshold=0.95):\n",
    "    \"\"\"\n",
    "    Processes video frames into input-output patch pairs.\n",
    "    Returns a list of tuples: [(input_patches, target_patches), ...]\n",
    "    \"\"\"\n",
    "    device_local = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # 1) Retrieve all frames from the video\n",
    "    cap = cv2.VideoCapture(mov_path)\n",
    "    frames = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frames.append(frame_rgb)\n",
    "    cap.release()\n",
    "\n",
    "    if len(frames) < 2:\n",
    "        return []\n",
    "\n",
    "    # 2) Build (input -> output) pairs based on similarity\n",
    "    pairs = []\n",
    "    for i in range(len(frames) - 1):\n",
    "        f1, f2 = frames[i], frames[i + 1]\n",
    "\n",
    "        # Compute similarity\n",
    "        sim = frame_similarity(f1, f2)\n",
    "        if sim < threshold:\n",
    "            continue  # Skip if frames are not similar enough\n",
    "\n",
    "        # Convert to tensors\n",
    "        f1_tensor = torch.from_numpy(f1).permute(2, 0, 1).float().unsqueeze(0).to(device_local)  # (1, 3, H, W)\n",
    "        f2_tensor = torch.from_numpy(f2).permute(2, 0, 1).float().unsqueeze(0).to(device_local)\n",
    "\n",
    "        # Scale frames\n",
    "        f1_scaled = F.interpolate(f1_tensor, size=(SCALED_INPUT_HEIGHT, SCALED_INPUT_WIDTH), mode='bilinear', align_corners=False).squeeze(0) / 255.0  # (3, H, W)\n",
    "        f2_scaled = F.interpolate(f2_tensor, size=(SCALED_OUTPUT_HEIGHT, SCALED_OUTPUT_WIDTH), mode='bilinear', align_corners=False).squeeze(0) / 255.0\n",
    "\n",
    "        # Split into patches\n",
    "        f1_patches = split_into_patches(f1_scaled, PATCH_SIZE)  # (num_patches, 3, 10, 10)\n",
    "        f2_patches = split_into_patches(f2_scaled, PATCH_SIZE)  # (num_patches, 3, 10, 10)\n",
    "\n",
    "        # Flatten patches\n",
    "        f1_patches_flat  = f1_patches.view(NUM_PATCHES_INPUT, -1)  # (576, 300)\n",
    "        f2_patches_flat  = f2_patches.view(NUM_PATCHES_OUTPUT, -1)  # (576, 300)\n",
    "\n",
    "        # Convert to NumPy and append pairs\n",
    "        np_in  = f1_patches_flat.cpu().numpy()  # (576, 300)\n",
    "        np_out = f2_patches_flat.cpu().numpy()  # (576, 300)\n",
    "        pairs.append((np_in, np_out))\n",
    "\n",
    "    return pairs\n",
    "\n",
    "# ============================== Training & Validation Functions ==============================\n",
    "\n",
    "def train_on_dataset(dataset):\n",
    "    \"\"\"\n",
    "    Trains the model on the given dataset.\n",
    "    \"\"\"\n",
    "    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)\n",
    "    model.train()\n",
    "    loss_sum = 0.0\n",
    "    loss_count = 0\n",
    "\n",
    "    for in_batch, out_batch in loader:\n",
    "        in_batch  = in_batch.to(device)   # (batch_size, num_patches, embed_dim)\n",
    "        out_batch = out_batch.to(device)  # (batch_size, num_patches, embed_dim)\n",
    "\n",
    "        batch_size, num_patches, embed_dim = in_batch.shape\n",
    "\n",
    "        # Reshape to (num_patches, batch_size, embed_dim)\n",
    "        in_seq  = in_batch.permute(1, 0, 2)   # (576, batch_size, 256)\n",
    "        out_seq = out_batch.permute(1, 0, 2)  # (576, batch_size, 256)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(in_seq, out_seq)         # (576, batch_size, 256)\n",
    "        loss = criterion(pred, out_seq)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_sum += loss.item()\n",
    "        loss_count += 1\n",
    "\n",
    "    avg_loss = loss_sum / loss_count if loss_count else 0.0\n",
    "    return avg_loss\n",
    "\n",
    "def val_on_dataset(dataset):\n",
    "    \"\"\"\n",
    "    Validates the model on the given dataset.\n",
    "    \"\"\"\n",
    "    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
    "    model.eval()\n",
    "    loss_sum = 0.0\n",
    "    loss_count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for vin, vout in loader:\n",
    "            vin  = vin.to(device)   # (batch_size, num_patches, embed_dim)\n",
    "            vout = vout.to(device)  # (batch_size, num_patches, embed_dim)\n",
    "\n",
    "            batch_size, num_patches, embed_dim = vin.shape\n",
    "\n",
    "            # Reshape to (num_patches, batch_size, embed_dim)\n",
    "            vin_seq  = vin.permute(1, 0, 2)   # (576, batch_size, 256)\n",
    "            vout_seq = vout.permute(1, 0, 2)  # (576, batch_size, 256)\n",
    "\n",
    "            pred = model(vin_seq, vout_seq)  # (576, batch_size, 256)\n",
    "            loss = criterion(pred, vout_seq)\n",
    "\n",
    "            loss_sum += loss.item()\n",
    "            loss_count += 1\n",
    "\n",
    "    avg_loss = loss_sum / loss_count if loss_count else 0.0\n",
    "    return avg_loss\n",
    "\n",
    "def save_checkpoint(epoch):\n",
    "    \"\"\"\n",
    "    Saves the model checkpoint.\n",
    "    \"\"\"\n",
    "    ckpt_path = os.path.join(CHECKPOINT_DIR, f\"informer_patches_epoch_{epoch}.pth\")\n",
    "    torch.save({\n",
    "        'epoch'               : epoch,\n",
    "        'model_state_dict'    : model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "    }, ckpt_path)\n",
    "    print(f\"[Info] Checkpoint saved at epoch {epoch}.\")\n",
    "\n",
    "# ============================== Training Loop ==============================\n",
    "\n",
    "# Gather Training and Validation Videos\n",
    "train_movs = [f for f in os.listdir(TRAIN_FOLDER) if f.lower().endswith('.mov')]\n",
    "train_movs.sort()\n",
    "\n",
    "val_movs = [f for f in os.listdir(VAL_FOLDER) if f.lower().endswith('.mov')]\n",
    "val_movs.sort()\n",
    "\n",
    "# Limit the number of videos for faster training\n",
    "if MAX_TRAIN_VIDS is not None:\n",
    "    train_movs = train_movs[:MAX_TRAIN_VIDS]\n",
    "\n",
    "if MAX_VAL_VIDS is not None:\n",
    "    val_movs = val_movs[:MAX_VAL_VIDS]\n",
    "\n",
    "print(f\"\\n[Info] Number of training videos: {len(train_movs)}\")\n",
    "print(f\"[Info] Number of validation videos: {len(val_movs)}\")\n",
    "\n",
    "# Initialize Plotting Variables\n",
    "train_losses = []\n",
    "val_losses   = []\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Training Phase\n",
    "    train_pairs = []\n",
    "    with tqdm(train_movs, desc=f\"Epoch [{epoch}/{NUM_EPOCHS}] - Training\", unit=\"video\") as train_pbar:\n",
    "        for file_name in train_pbar:\n",
    "            mov_path = os.path.join(TRAIN_FOLDER, file_name)\n",
    "            frame_pairs = prepare_video_for_model(mov_path, SIM_THRESHOLD)\n",
    "            if len(frame_pairs) == 0:\n",
    "                print(f\"[Warning] No valid frame pairs found in {file_name}. Skipping.\")\n",
    "                continue\n",
    "            train_pairs.extend(frame_pairs)\n",
    "\n",
    "    if not train_pairs:\n",
    "        print(\"[Warning] No training pairs found for this epoch.\")\n",
    "        avg_train_loss = float('inf')\n",
    "    else:\n",
    "        train_dataset = PatchPairDataset(train_pairs)\n",
    "        avg_train_loss = train_on_dataset(train_dataset)\n",
    "        print(f\"[Epoch {epoch}] Training Loss: {avg_train_loss:.6f}\")\n",
    "\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # Validation Phase\n",
    "    val_pairs = []\n",
    "    with tqdm(val_movs, desc=f\"Epoch [{epoch}/{NUM_EPOCHS}] - Validation\", unit=\"video\") as val_pbar:\n",
    "        for fname in val_pbar:\n",
    "            mov_path = os.path.join(VAL_FOLDER, fname)\n",
    "            frame_pairs = prepare_video_for_model(mov_path, SIM_THRESHOLD)\n",
    "            if len(frame_pairs) == 0:\n",
    "                print(f\"[Warning] No valid frame pairs found in {fname}. Skipping.\")\n",
    "                continue\n",
    "            val_pairs.extend(frame_pairs)\n",
    "\n",
    "    if not val_pairs:\n",
    "        print(\"[Warning] No validation pairs found for this epoch.\")\n",
    "        avg_val_loss = float('inf')\n",
    "    else:\n",
    "        val_dataset = PatchPairDataset(val_pairs)\n",
    "        avg_val_loss = val_on_dataset(val_dataset)\n",
    "        print(f\"[Epoch {epoch}] Validation Loss: {avg_val_loss:.6f}\")\n",
    "\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    # Save Checkpoint\n",
    "    save_checkpoint(epoch)\n",
    "\n",
    "    # Log Time Taken\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"[Epoch {epoch}] Completed in {elapsed_time:.2f}s\\n\")\n",
    "\n",
    "# ============================== Plot Training Curves ==============================\n",
    "\n",
    "e_range = range(1, NUM_EPOCHS + 1)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(e_range, train_losses, label='Train Loss', marker='o')\n",
    "plt.plot(e_range, val_losses, label='Validation Loss', marker='x')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Informer Model Training Progress')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"[Info] Training completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN + Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_cnn_transformer.py\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import math\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "# ============================== Device Configuration ==============================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"[Info] Using device: {device}\")\n",
    "\n",
    "# Enable CUDNN benchmark for optimized performance on fixed-size inputs\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# ============================== Global Config & Paths ==============================\n",
    "TRAIN_FOLDER    = \"bdd100k/videos/train\"  # Path to training videos\n",
    "VAL_FOLDER      = \"bdd100k/videos/val\"    # Path to validation videos\n",
    "CHECKPOINT_DIR  = \"checkpoints\"           # Directory to save checkpoints\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "# ============================== Training Parameters ==============================\n",
    "FRAME_SCALE_INPUT  = 0.25  # Scaling factor for input frames\n",
    "FRAME_SCALE_OUTPUT = 0.5   # Scaling factor for output frames (upscaling by 2)\n",
    "NUM_EPOCHS         = 20    # Number of training epochs\n",
    "SIM_THRESHOLD      = 0.96  # Frame similarity threshold to create pairs\n",
    "BATCH_SIZE         = 8     # Adjust based on GPU memory\n",
    "LEARNING_RATE      = 1e-4\n",
    "\n",
    "# ============================== Original Frame Sizes ==============================\n",
    "ORIG_INPUT_HEIGHT, ORIG_INPUT_WIDTH  = 1280, 720\n",
    "ORIG_OUTPUT_HEIGHT, ORIG_OUTPUT_WIDTH = 1280, 720\n",
    "\n",
    "# ============================== Scaled Frame Sizes ==============================\n",
    "SCALED_INPUT_HEIGHT  = int(ORIG_INPUT_HEIGHT * FRAME_SCALE_INPUT)\n",
    "SCALED_INPUT_WIDTH   = int(ORIG_INPUT_WIDTH * FRAME_SCALE_INPUT)\n",
    "SCALED_OUTPUT_HEIGHT = int(ORIG_OUTPUT_HEIGHT * FRAME_SCALE_OUTPUT)\n",
    "SCALED_OUTPUT_WIDTH  = int(ORIG_OUTPUT_WIDTH * FRAME_SCALE_OUTPUT)\n",
    "\n",
    "# Print Expected Dimensions for Verification\n",
    "print(\"\\n===== FRAME INFO =====\")\n",
    "print(f\"Original Input Frame Size (H x W): {ORIG_INPUT_HEIGHT} x {ORIG_INPUT_WIDTH}\")\n",
    "print(f\"Original Output Frame Size (H x W): {ORIG_OUTPUT_HEIGHT} x {ORIG_OUTPUT_WIDTH}\")\n",
    "print(f\"Scaled Input Frame Size (H x W): {SCALED_INPUT_HEIGHT} x {SCALED_INPUT_WIDTH}\")\n",
    "print(f\"Scaled Output Frame Size (H x W): {SCALED_OUTPUT_HEIGHT} x {SCALED_OUTPUT_WIDTH}\")\n",
    "\n",
    "# ============================== Model Definition ==============================\n",
    "\n",
    "class CNNEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim=256):\n",
    "        super(CNNEncoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)  # (B, 64, H/2, W/2)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(64, embed_dim, kernel_size=3, stride=2, padding=1)  # (B, embed_dim, H/4, W/4)\n",
    "        self.bn2 = nn.BatchNorm2d(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)  # (B, 64, H/2, W/2)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)  # (B, embed_dim, H/4, W/4)\n",
    "        return x\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, max_len=60000):  # increased max_len to 60000\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # Even indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # Odd indices\n",
    "        pe = pe.unsqueeze(1)  # Shape: (max_len, 1, embed_dim)\n",
    "        self.register_buffer('pe', pe)\n",
    "    def forward(self, x):\n",
    "        if x.size(0) > self.pe.size(0):\n",
    "            raise ValueError(f\"Sequence length {x.size(0)} exceeds maximum {self.pe.size(0)}.\")\n",
    "        return x + self.pe[:x.size(0), :]\n",
    "\n",
    "\n",
    "class TransformerModule(nn.Module):\n",
    "    def __init__(self, embed_dim=128, num_heads=4, num_layers=3, dim_feedforward=256, dropout=0.1):\n",
    "        super(TransformerModule, self).__init__()\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        # Use the updated maximum length here:\n",
    "        self.pos_encoder = PositionalEncoding(embed_dim, max_len=60000)\n",
    "        self.embed_dim = embed_dim\n",
    "    def forward(self, x):\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CNNDecoder(nn.Module):\n",
    "    def __init__(self, embed_dim=256, output_channels=3):\n",
    "        super(CNNDecoder, self).__init__()\n",
    "        self.deconv1 = nn.ConvTranspose2d(\n",
    "            embed_dim, 128, kernel_size=3, stride=2, padding=1, output_padding=1\n",
    "        )  # (B, 128, H/2, W/2)\n",
    "        self.bn1 = nn.BatchNorm2d(128)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.deconv2 = nn.ConvTranspose2d(\n",
    "            128, 64, kernel_size=3, stride=2, padding=1, output_padding=1\n",
    "        )  # (B, 64, H, W)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.deconv3 = nn.ConvTranspose2d(\n",
    "            64, output_channels, kernel_size=7, stride=2, padding=3, output_padding=1\n",
    "        )  # (B, 3, 2H, 2W)\n",
    "        self.sigmoid = nn.Sigmoid()  # Ensures output is in [0,1]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.deconv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)  # (B, 128, H/2, W/2)\n",
    "        \n",
    "        x = self.deconv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)  # (B, 64, H, W)\n",
    "        \n",
    "        x = self.deconv3(x)\n",
    "        x = self.sigmoid(x)  # (B, 3, 2H, 2W)\n",
    "        return x\n",
    "\n",
    "class CNN_Transformer_Model(nn.Module):\n",
    "    def __init__(self, embed_dim=256, num_heads=8, num_layers=6, dim_feedforward=512, dropout=0.1):\n",
    "        super(CNN_Transformer_Model, self).__init__()\n",
    "        self.encoder = CNNEncoder(embed_dim)\n",
    "        self.transformer = TransformerModule(embed_dim, num_heads, num_layers, dim_feedforward, dropout)\n",
    "        self.decoder = CNNDecoder(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, 3, H, W)\n",
    "        Returns:\n",
    "            Reconstructed tensor of shape (batch_size, 3, 2H, 2W)\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        encoded = self.encoder(x)  # (B, embed_dim, H/4, W/4)\n",
    "\n",
    "        # Capture spatial dimensions\n",
    "        H_4, W_4 = encoded.size(2), encoded.size(3)  # H/4, W/4\n",
    "\n",
    "        # Flatten spatial dimensions to create a sequence\n",
    "        seq_len = H_4 * W_4\n",
    "        encoded = encoded.view(batch_size, encoded.size(1), seq_len)  # (B, embed_dim, seq_len)\n",
    "        encoded = encoded.permute(2, 0, 1)  # (seq_len, B, embed_dim)\n",
    "\n",
    "        # Transformer\n",
    "        transformed = self.transformer(encoded)  # (seq_len, B, embed_dim)\n",
    "\n",
    "        # Reshape back to feature maps\n",
    "        transformed = transformed.permute(1, 2, 0)  # (B, embed_dim, seq_len)\n",
    "        transformed = transformed.view(batch_size, -1, H_4, W_4)  # (B, embed_dim, H/4, W/4)\n",
    "\n",
    "        # Decoder\n",
    "        reconstructed = self.decoder(transformed)  # (B, 3, 2H, 2W)\n",
    "        return reconstructed\n",
    "\n",
    "# ============================== Initialize the Model ==============================\n",
    "\n",
    "model = CNN_Transformer_Model(embed_dim=256, num_heads=8, num_layers=6, dim_feedforward=512, dropout=0.1).to(device)\n",
    "\n",
    "# Initialize weights\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        nn.init.ones_(m.weight)\n",
    "        nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        if isinstance(m.bias, torch.Tensor):\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "model.apply(init_weights)\n",
    "print(\"[Info] Model initialized and weights set.\")\n",
    "\n",
    "# ============================== Loss and Optimizer ==============================\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "print(\"[Info] Loss function and optimizer defined.\")\n",
    "\n",
    "# ============================== Mixed Precision Training Setup ==============================\n",
    "scaler = GradScaler()\n",
    "\n",
    "# ============================== Custom Dataset Definition ==============================\n",
    "\n",
    "class FramePairDataset(Dataset):\n",
    "    def __init__(self, frame_pairs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            frame_pairs: List of tuples (input_frame, target_frame)\n",
    "                         Each frame is a numpy array of shape (C, H, W) with values in [0,1]\n",
    "        \"\"\"\n",
    "        self.input_frames  = [torch.tensor(inp, dtype=torch.float32) for inp, _ in frame_pairs]\n",
    "        self.target_frames = [torch.tensor(tgt, dtype=torch.float32) for _, tgt in frame_pairs]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_frames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_frames[idx], self.target_frames[idx]\n",
    "\n",
    "# ============================== Data Preparation ==============================\n",
    "\n",
    "def frame_similarity(img1, img2):\n",
    "    \"\"\"\n",
    "    Computes similarity between two frames based on absolute pixel difference.\n",
    "    Returns similarity percentage.\n",
    "    \"\"\"\n",
    "    img1 = img1.cpu().numpy() if isinstance(img1, torch.Tensor) else img1\n",
    "    img2 = img2.cpu().numpy() if isinstance(img2, torch.Tensor) else img2\n",
    "\n",
    "    if len(img1.shape) == 3:\n",
    "        img1_gray = cv2.cvtColor(img1, cv2.COLOR_RGB2GRAY)\n",
    "    else:\n",
    "        img1_gray = img1\n",
    "\n",
    "    if len(img2.shape) == 3:\n",
    "        img2_gray = cv2.cvtColor(img2, cv2.COLOR_RGB2GRAY)\n",
    "    else:\n",
    "        img2_gray = img2\n",
    "\n",
    "    diff = cv2.absdiff(img1_gray, img2_gray)\n",
    "    similarity = 100 - (np.sum(diff) / (img1_gray.shape[0] * img1_gray.shape[1] * 255) * 100)\n",
    "    return similarity\n",
    "\n",
    "def prepare_video_for_model(video_path, threshold=0.95):\n",
    "    \"\"\"\n",
    "    Processes video frames into input-output frame pairs based on similarity.\n",
    "    Args:\n",
    "        video_path (str): Path to the video file.\n",
    "        threshold (float): Minimum similarity to consider frames as a pair.\n",
    "    Returns:\n",
    "        List of tuples: [(input_frame, target_frame), ...]\n",
    "    \"\"\"\n",
    "    device_local = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # 1) Retrieve all frames from the video\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frames.append(frame_rgb)\n",
    "    cap.release()\n",
    "\n",
    "    if len(frames) < 2:\n",
    "        return []\n",
    "\n",
    "    # 2) Build (input -> output) pairs based on similarity\n",
    "    pairs = []\n",
    "    for i in range(len(frames) - 1):\n",
    "        f1, f2 = frames[i], frames[i + 1]\n",
    "\n",
    "        # Compute similarity\n",
    "        sim = frame_similarity(f1, f2)\n",
    "        if sim < threshold:\n",
    "            continue  # Skip if frames are not similar enough\n",
    "\n",
    "        # Convert to tensors and normalize\n",
    "        f1_tensor = torch.from_numpy(f1).permute(2, 0, 1).float().to(device_local) / 255.0  # (3, H, W)\n",
    "        f2_tensor = torch.from_numpy(f2).permute(2, 0, 1).float().to(device_local) / 255.0  # (3, H, W)\n",
    "\n",
    "        # Scale frames\n",
    "        f1_scaled = F.interpolate(\n",
    "            f1_tensor.unsqueeze(0),\n",
    "            size=(SCALED_INPUT_HEIGHT, SCALED_INPUT_WIDTH),\n",
    "            mode='bilinear',\n",
    "            align_corners=False\n",
    "        ).squeeze(0)  # (3, H, W)\n",
    "\n",
    "        f2_scaled = F.interpolate(\n",
    "            f2_tensor.unsqueeze(0),\n",
    "            size=(SCALED_OUTPUT_HEIGHT, SCALED_OUTPUT_WIDTH),\n",
    "            mode='bilinear',\n",
    "            align_corners=False\n",
    "        ).squeeze(0)  # (3, H, W)\n",
    "\n",
    "        # Append to pairs list\n",
    "        pairs.append((f1_scaled.cpu().numpy(), f2_scaled.cpu().numpy()))\n",
    "\n",
    "    return pairs\n",
    "\n",
    "# ============================== Training & Validation Functions ==============================\n",
    "\n",
    "def train_on_dataset(model, loader, criterion, optimizer, scaler, device):\n",
    "    \"\"\"\n",
    "    Trains the model on the given dataset.\n",
    "    Args:\n",
    "        model (nn.Module): The CNN-Transformer model.\n",
    "        loader (DataLoader): DataLoader for training data.\n",
    "        criterion: Loss function.\n",
    "        optimizer: Optimizer.\n",
    "        scaler: Gradient scaler for mixed precision.\n",
    "        device: CUDA device.\n",
    "    Returns:\n",
    "        float: Average training loss.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    loss_sum = 0.0\n",
    "    loss_count = 0\n",
    "\n",
    "    for in_batch, out_batch in tqdm(loader, desc=\"Training Batches\", unit=\"batch\"):\n",
    "        in_batch  = in_batch.to(device, non_blocking=True)   # (B, 3, H, W)\n",
    "        out_batch = out_batch.to(device, non_blocking=True)  # (B, 3, 2H, 2W)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with autocast():\n",
    "            pred = model(in_batch)  # (B, 3, 2H, 2W)\n",
    "            loss = criterion(pred, out_batch)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        loss_sum += loss.item()\n",
    "        loss_count += 1\n",
    "\n",
    "    avg_loss = loss_sum / loss_count if loss_count else 0.0\n",
    "    return avg_loss\n",
    "\n",
    "def validate_on_dataset(model, loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Validates the model on the given dataset.\n",
    "    Args:\n",
    "        model (nn.Module): The CNN-Transformer model.\n",
    "        loader (DataLoader): DataLoader for validation data.\n",
    "        criterion: Loss function.\n",
    "        device: CUDA device.\n",
    "    Returns:\n",
    "        float: Average validation loss.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    loss_sum = 0.0\n",
    "    loss_count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for in_batch, out_batch in tqdm(loader, desc=\"Validation Batches\", unit=\"batch\"):\n",
    "            in_batch  = in_batch.to(device, non_blocking=True)   # (B, 3, H, W)\n",
    "            out_batch = out_batch.to(device, non_blocking=True)  # (B, 3, 2H, 2W)\n",
    "\n",
    "            with autocast():\n",
    "                pred = model(in_batch)  # (B, 3, 2H, 2W)\n",
    "                loss = criterion(pred, out_batch)\n",
    "\n",
    "            loss_sum += loss.item()\n",
    "            loss_count += 1\n",
    "\n",
    "    avg_loss = loss_sum / loss_count if loss_count else 0.0\n",
    "    return avg_loss\n",
    "\n",
    "def save_checkpoint(epoch, model, optimizer, scaler, best_val_loss, checkpoint_dir=\"checkpoints\"):\n",
    "    \"\"\"\n",
    "    Saves the model checkpoint.\n",
    "    Args:\n",
    "        epoch (int): Current epoch number.\n",
    "        model (nn.Module): The CNN-Transformer model.\n",
    "        optimizer: Optimizer.\n",
    "        scaler: Gradient scaler.\n",
    "        best_val_loss (float): Best validation loss achieved so far.\n",
    "        checkpoint_dir (str): Directory to save checkpoints.\n",
    "    \"\"\"\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    ckpt_path = os.path.join(checkpoint_dir, f\"cnn_transformer_epoch_{epoch}.pth\")\n",
    "    torch.save({\n",
    "        'epoch'               : epoch,\n",
    "        'model_state_dict'    : model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scaler_state_dict'   : scaler.state_dict(),\n",
    "        'best_val_loss'       : best_val_loss\n",
    "    }, ckpt_path)\n",
    "    print(f\"[Info] Checkpoint saved at epoch {epoch} with validation loss {best_val_loss:.6f}.\")\n",
    "\n",
    "# ============================== Training Loop ==============================\n",
    "\n",
    "def main_training():\n",
    "    # Gather Training and Validation Videos\n",
    "    train_videos = [f for f in os.listdir(TRAIN_FOLDER) if f.lower().endswith(('.mov', '.mp4', '.avi'))]\n",
    "    train_videos.sort()\n",
    "\n",
    "    val_videos = [f for f in os.listdir(VAL_FOLDER) if f.lower().endswith(('.mov', '.mp4', '.avi'))]\n",
    "    val_videos.sort()\n",
    "\n",
    "    # Limit the number of videos if specified\n",
    "    if MAX_TRAIN_VIDS is not None:\n",
    "        train_videos = train_videos[:MAX_TRAIN_VIDS]\n",
    "\n",
    "    if MAX_VAL_VIDS is not None:\n",
    "        val_videos = val_videos[:MAX_VAL_VIDS]\n",
    "\n",
    "    print(f\"\\n[Info] Number of training videos: {len(train_videos)}\")\n",
    "    print(f\"[Info] Number of validation videos: {len(val_videos)}\")\n",
    "\n",
    "    # Initialize Tracking Variables\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    # Training Loop\n",
    "    for epoch in range(1, NUM_EPOCHS + 1):\n",
    "        start_time = time.time()\n",
    "\n",
    "        # ------------------------------ Training Phase ------------------------------\n",
    "        print(f\"\\n[Epoch {epoch}/{NUM_EPOCHS}] - Training Phase\")\n",
    "        train_pairs = []\n",
    "        for video_file in tqdm(train_videos, desc=\"Processing Training Videos\", unit=\"video\"):\n",
    "            video_path = os.path.join(TRAIN_FOLDER, video_file)\n",
    "            frame_pairs = prepare_video_for_model(video_path, SIM_THRESHOLD)\n",
    "            if not frame_pairs:\n",
    "                print(f\"[Warning] No valid frame pairs found in {video_file}. Skipping.\")\n",
    "                continue\n",
    "            train_pairs.extend(frame_pairs)\n",
    "\n",
    "        print(f\"[Info] Number of training frame pairs: {len(train_pairs)}\")\n",
    "        if not train_pairs:\n",
    "            print(\"[Warning] No training pairs found for this epoch.\")\n",
    "            avg_train_loss = float('inf')\n",
    "        else:\n",
    "            train_dataset = FramePairDataset(train_pairs)\n",
    "            train_loader = DataLoader(\n",
    "                train_dataset,\n",
    "                batch_size=BATCH_SIZE,\n",
    "                shuffle=True,\n",
    "                num_workers=0,          # Set num_workers=0 for Windows\n",
    "                pin_memory=True\n",
    "            )\n",
    "            avg_train_loss = train_on_dataset(model, train_loader, criterion, optimizer, scaler, device)\n",
    "            print(f\"[Epoch {epoch}] Average Training Loss: {avg_train_loss:.6f}\")\n",
    "\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # ------------------------------ Validation Phase ------------------------------\n",
    "        print(f\"\\n[Epoch {epoch}/{NUM_EPOCHS}] - Validation Phase\")\n",
    "        val_pairs = []\n",
    "        for video_file in tqdm(val_videos, desc=\"Processing Validation Videos\", unit=\"video\"):\n",
    "            video_path = os.path.join(VAL_FOLDER, video_file)\n",
    "            frame_pairs = prepare_video_for_model(video_path, SIM_THRESHOLD)\n",
    "            if not frame_pairs:\n",
    "                print(f\"[Warning] No valid frame pairs found in {video_file}. Skipping.\")\n",
    "                continue\n",
    "            val_pairs.extend(frame_pairs)\n",
    "\n",
    "        print(f\"[Info] Number of validation frame pairs: {len(val_pairs)}\")\n",
    "        if not val_pairs:\n",
    "            print(\"[Warning] No validation pairs found for this epoch.\")\n",
    "            avg_val_loss = float('inf')\n",
    "        else:\n",
    "            val_dataset = FramePairDataset(val_pairs)\n",
    "            val_loader = DataLoader(\n",
    "                val_dataset,\n",
    "                batch_size=BATCH_SIZE,\n",
    "                shuffle=False,\n",
    "                num_workers=0,          # Set num_workers=0 for Windows\n",
    "                pin_memory=True\n",
    "            )\n",
    "            avg_val_loss = validate_on_dataset(model, val_loader, criterion, device)\n",
    "            print(f\"[Epoch {epoch}] Average Validation Loss: {avg_val_loss:.6f}\")\n",
    "\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        # ------------------------------ Checkpointing ------------------------------\n",
    "        # if avg_val_loss < best_val_loss:\n",
    "        #     best_val_loss = avg_val_loss\n",
    "        #     save_checkpoint(epoch, model, optimizer, scaler, best_val_loss, CHECKPOINT_DIR)\n",
    "        #     print(f\"[Info] New best model saved at epoch {epoch} with validation loss {best_val_loss:.6f}.\")\n",
    "        best_val_loss = avg_val_loss\n",
    "        save_checkpoint(epoch, model, optimizer, scaler, best_val_loss, CHECKPOINT_DIR)\n",
    "        print(f\"[Info] New best model saved at epoch {epoch} with validation loss {best_val_loss:.6f}.\")\n",
    "        \n",
    "        # ------------------------------ Logging ------------------------------\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"[Epoch {epoch}] Completed in {elapsed_time:.2f} seconds.\")\n",
    "\n",
    "    # ============================== Plot Loss Curves ==============================\n",
    "    e_range = range(1, NUM_EPOCHS + 1)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(e_range, train_losses, label='Training Loss', marker='o')\n",
    "    plt.plot(e_range, val_losses, label='Validation Loss', marker='x')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MSE Loss')\n",
    "    plt.title('Training and Validation Loss over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    print(\"[Info] Training process completed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_training()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE + TRANSFORMER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# train_vae_transformer.py\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "# ============================== Global Flags and Hyperparameters ==============================\n",
    "RUN_VALIDATION = False               # Set to False to skip validation and speed up training.\n",
    "TEACHER_FORCING_RATIO = 0.5           # (Not used in this implementation but available for future modifications)\n",
    "lambda_recon = 1.0                    # Weight for reconstruction (L1) loss.\n",
    "lambda_latent = 1.0                   # Weight for latent prediction (MSE) loss.\n",
    "lambda_kl = 0.001                     # Weight for KL divergence loss.\n",
    "\n",
    "# ============================== Device Configuration ==============================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"[Info] Using device: {device}\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# ============================== Global Config & Paths ==============================\n",
    "TRAIN_FOLDER = \"bdd100k/videos/train\"  # Path to training videos\n",
    "VAL_FOLDER = \"bdd100k/videos/val\"      # Path to validation videos\n",
    "CHECKPOINT_DIR = \"checkpoints\"         # Directory to save checkpoints\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "MAX_TRAIN_VIDS = 10   # Limit number of training videos\n",
    "MAX_VAL_VIDS = 10     # Limit number of validation videos\n",
    "\n",
    "# ============================== Training Parameters ==============================\n",
    "# Use scaling factors: 0.5 yields target size of 640x360.\n",
    "FRAME_SCALE_INPUT = 0.5  \n",
    "FRAME_SCALE_OUTPUT = 0.5  \n",
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = 1e-4\n",
    "BUFFER_SIZE = 50     # Number of frames to unroll (transitions) per rolling chunk\n",
    "\n",
    "# ============================== Original and Target Frame Sizes ==============================\n",
    "ORIG_HEIGHT, ORIG_WIDTH = 720, 1280\n",
    "SCALED_INPUT_HEIGHT = int(ORIG_HEIGHT * FRAME_SCALE_INPUT)    # e.g., 360\n",
    "SCALED_INPUT_WIDTH  = int(ORIG_WIDTH * FRAME_SCALE_INPUT)     # e.g., 640\n",
    "SCALED_OUTPUT_HEIGHT = int(ORIG_HEIGHT * FRAME_SCALE_OUTPUT)   # e.g., 360\n",
    "SCALED_OUTPUT_WIDTH  = int(ORIG_WIDTH * FRAME_SCALE_OUTPUT)    # e.g., 640\n",
    "TARGET_SIZE = (SCALED_INPUT_HEIGHT, SCALED_INPUT_WIDTH)\n",
    "\n",
    "print(\"\\n===== FRAME INFO =====\")\n",
    "print(f\"Original Frame Size: {ORIG_HEIGHT} x {ORIG_WIDTH}\")\n",
    "print(f\"Target Frame Size: {TARGET_SIZE[0]} x {TARGET_SIZE[1]}\")\n",
    "\n",
    "# ============================== Model Definition ==============================\n",
    "latent_dim = 128  # Reduced latent dimension for memory savings.\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, max_len=60000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(1)\n",
    "        self.register_buffer('pe', pe)\n",
    "    def forward(self, x):\n",
    "        if x.size(0) > self.pe.size(0):\n",
    "            raise ValueError(f\"Sequence length {x.size(0)} exceeds maximum {self.pe.size(0)}.\")\n",
    "        return x + self.pe[:x.size(0), :]\n",
    "\n",
    "# VAE Encoder: Reduced channels.\n",
    "class VAEEncoder(nn.Module):\n",
    "    def __init__(self, latent_dim=128):\n",
    "        super(VAEEncoder, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 4, 2, 1),    # (B,32,H/2,W/2)\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(32, 64, 4, 2, 1),   # (B,64,H/4,W/4)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1),  # (B,128,H/8,W/8)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        # Here, H and W become SCALED_INPUT_HEIGHT/8 and SCALED_INPUT_WIDTH/8, respectively.\n",
    "        self.fc_mu = nn.Linear(128 * (SCALED_INPUT_HEIGHT // 8) * (SCALED_INPUT_WIDTH // 8), latent_dim)\n",
    "        self.fc_logvar = nn.Linear(128 * (SCALED_INPUT_HEIGHT // 8) * (SCALED_INPUT_WIDTH // 8), latent_dim)\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        # Clamp logvar to prevent exploding values (which can result in NaN losses)\n",
    "        logvar = torch.clamp(logvar, min=-10, max=10)\n",
    "        return mu, logvar\n",
    "\n",
    "# VAE Decoder: Reduced channels.\n",
    "class VAEDecoder(nn.Module):\n",
    "    def __init__(self, latent_dim=128):\n",
    "        super(VAEDecoder, self).__init__()\n",
    "        self.fc = nn.Linear(latent_dim, 128 * (SCALED_INPUT_HEIGHT // 8) * (SCALED_INPUT_WIDTH // 8))\n",
    "        self.deconv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1),  # Upsample by factor 2\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64, 32, 4, 2, 1),   # Upsample by factor 2\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(32, 3, 4, 2, 1),      # Upsample by factor 2 to get original resolution\n",
    "            nn.Sigmoid(),  # Sigmoid ensures outputs are in [0,1]\n",
    "        )\n",
    "    def forward(self, z):\n",
    "        x = self.fc(z)\n",
    "        x = x.view(z.size(0), 128, (SCALED_INPUT_HEIGHT // 8), (SCALED_INPUT_WIDTH // 8))\n",
    "        x = self.deconv(x)\n",
    "        return x\n",
    "\n",
    "# Transformer Module\n",
    "class LatentTransformer(nn.Module):\n",
    "    def __init__(self, latent_dim=128, num_heads=4, num_layers=3, dim_feedforward=256, dropout=0.1, max_len=100):\n",
    "        super(LatentTransformer, self).__init__()\n",
    "        self.pos_encoder = PositionalEncoding(latent_dim, max_len=max_len)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=latent_dim, nhead=num_heads,\n",
    "                                                    dim_feedforward=dim_feedforward, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "    def forward(self, z_seq):\n",
    "        z_seq = self.pos_encoder(z_seq)\n",
    "        out = self.transformer_encoder(z_seq)\n",
    "        return out\n",
    "\n",
    "# Overall VAE-Transformer Model.\n",
    "class VAE_Transformer(nn.Module):\n",
    "    def __init__(self, latent_dim=128, num_heads=4, num_layers=3, dim_feedforward=256, dropout=0.1, max_len=100):\n",
    "        super(VAE_Transformer, self).__init__()\n",
    "        self.encoder = VAEEncoder(latent_dim)\n",
    "        self.decoder = VAEDecoder(latent_dim)\n",
    "        self.transformer = LatentTransformer(latent_dim, num_heads, num_layers, dim_feedforward, dropout, max_len)\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    def forward(self, x_seq):\n",
    "        # x_seq: (seq_len, batch, 3, H, W) with values normalized to [0,1]\n",
    "        z_seq = []\n",
    "        mu_seq = []\n",
    "        logvar_seq = []\n",
    "        for x in x_seq:\n",
    "            mu, logvar = self.encoder(x)\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "            z_seq.append(z.unsqueeze(0))\n",
    "            mu_seq.append(mu.unsqueeze(0))\n",
    "            logvar_seq.append(logvar.unsqueeze(0))\n",
    "        z_seq = torch.cat(z_seq, dim=0)      # (seq_len, batch, latent_dim)\n",
    "        mu_seq = torch.cat(mu_seq, dim=0)\n",
    "        logvar_seq = torch.cat(logvar_seq, dim=0)\n",
    "        # Pass the latent sequence through the transformer.\n",
    "        z_trans = self.transformer(z_seq)\n",
    "        return z_seq, z_trans, mu_seq, logvar_seq\n",
    "\n",
    "# ============================== Loss Functions ==============================\n",
    "def kl_divergence(mu, logvar):\n",
    "    return -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "def reconstruction_loss(x_recon, x):\n",
    "    return F.l1_loss(x_recon, x)\n",
    "\n",
    "def latent_loss(z_pred, z_true):\n",
    "    return F.mse_loss(z_pred, z_true)\n",
    "\n",
    "# ============================== Data Preparation ==============================\n",
    "def process_frame(frame, target_size):\n",
    "    H, W = frame.shape[:2]\n",
    "    if (H, W) != target_size:\n",
    "        frame = cv2.resize(frame, (target_size[1], target_size[0]))\n",
    "    return frame\n",
    "\n",
    "def prepare_video_sequence(video_path):\n",
    "    \"\"\"\n",
    "    Load frames from a video and return raw tensors in the original [0,255] range.\n",
    "    No normalization is applied here.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frames.append(frame_rgb)\n",
    "    cap.release()\n",
    "    if len(frames) < 2:\n",
    "        return None\n",
    "    # Process first frame for input.\n",
    "    first_frame_np = process_frame(frames[0], (SCALED_INPUT_HEIGHT, SCALED_INPUT_WIDTH))\n",
    "    first_frame = (torch.from_numpy(np.ascontiguousarray(first_frame_np))\n",
    "                        .permute(2, 0, 1)\n",
    "                        .float()\n",
    "                        .pin_memory())\n",
    "    initial_input = first_frame  # raw image in [0,255]\n",
    "    target_frames = []\n",
    "    # Process subsequent frames.\n",
    "    for frame in frames[1:]:\n",
    "        processed = process_frame(frame, (SCALED_OUTPUT_HEIGHT, SCALED_OUTPUT_WIDTH))\n",
    "        tensor_frame = (torch.from_numpy(np.ascontiguousarray(processed))\n",
    "                            .permute(2, 0, 1)\n",
    "                            .float()\n",
    "                            .pin_memory())\n",
    "        target_frames.append(tensor_frame)  # raw image in [0,255]\n",
    "    return initial_input, target_frames\n",
    "\n",
    "# ============================== Training Loop with Rolling Buffer ==============================\n",
    "def train_on_video_sequence(video_path, model, optimizer, device, buffer_size=BUFFER_SIZE, teacher_forcing_ratio=TEACHER_FORCING_RATIO):\n",
    "    \"\"\"\n",
    "    Processes the video in chunks (rolling buffer) to limit GPU memory usage.\n",
    "    Each chunk contains (buffer_size + 1) frames so that we can compute buffer_size transitions.\n",
    "    \n",
    "    NOTE: Here we normalize the chunk just before feeding it to the model.\n",
    "    \"\"\"\n",
    "    data = prepare_video_sequence(video_path)\n",
    "    if data is None:\n",
    "        print(f\"[Warning] Video {video_path} has too few frames. Skipping.\")\n",
    "        return None\n",
    "    initial_input, target_frames = data\n",
    "\n",
    "    # Build a list of frames on CPU. Each element is a tensor of shape (1, 3, H, W).\n",
    "    frames_list = [initial_input.unsqueeze(0)] + [tf.unsqueeze(0) for tf in target_frames]\n",
    "    seq_len = len(frames_list)\n",
    "\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    scaler = GradScaler()  # For mixed precision\n",
    "\n",
    "    total_loss = 0.0\n",
    "    count_loss = 0\n",
    "\n",
    "    # Process video in overlapping chunks\n",
    "    for start in range(0, seq_len - 1, buffer_size):\n",
    "        end = min(start + buffer_size + 1, seq_len)  # +1 to have target for the last frame\n",
    "        # Move the current chunk from CPU to GPU and normalize by dividing by 255 so that the model sees inputs in [0,1]\n",
    "        chunk = [frame.to(device, non_blocking=True) for frame in frames_list[start:end]]\n",
    "        x_chunk = (torch.cat(chunk, dim=0).unsqueeze(1)) / 255.0  # shape: (chunk_len, 1, 3, H, W)\n",
    "\n",
    "        with autocast():\n",
    "            z_seq, z_trans, mu_seq, logvar_seq = model(x_chunk)\n",
    "            chunk_len = z_seq.size(0)\n",
    "            recon_loss = 0.0\n",
    "            latent_loss_total = 0.0\n",
    "            kl_loss_total = 0.0\n",
    "\n",
    "            # Compute loss over transitions within this chunk.\n",
    "            for t in range(chunk_len - 1):\n",
    "                z_pred = z_trans[t]   # Predicted latent for frame t+1 (shape: [1, latent_dim])\n",
    "                z_true = z_seq[t+1]   # Ground truth latent for frame t+1\n",
    "                latent_loss_total += latent_loss(z_pred, z_true)\n",
    "                # Decode the predicted latent to reconstruct frame t+1.\n",
    "                x_recon = model.decoder(z_pred.unsqueeze(0))  # output is in [0,1]\n",
    "                # Use the raw target frame normalized to [0,1]\n",
    "                x_target = chunk[t+1] / 255.0\n",
    "                recon_loss += reconstruction_loss(x_recon, x_target)\n",
    "                kl_loss_total += kl_divergence(mu_seq[t+1], logvar_seq[t+1])\n",
    "            \n",
    "            loss = lambda_recon * recon_loss + lambda_latent * latent_loss_total + lambda_kl * kl_loss_total\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        count_loss += (chunk_len - 1)\n",
    "\n",
    "        # Clean up to free GPU memory.\n",
    "        del x_chunk, z_seq, z_trans, mu_seq, logvar_seq, chunk\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    avg_loss = total_loss / count_loss if count_loss > 0 else float('inf')\n",
    "    return avg_loss\n",
    "\n",
    "def validate_on_video_sequence(video_path, model, device, buffer_size=BUFFER_SIZE):\n",
    "    \"\"\"\n",
    "    Similar to training but without gradient updates.\n",
    "    \n",
    "    Again, we normalize only as the data enters the model.\n",
    "    \"\"\"\n",
    "    data = prepare_video_sequence(video_path)\n",
    "    if data is None:\n",
    "        print(f\"[Warning] Video {video_path} has too few frames. Skipping.\")\n",
    "        return None\n",
    "    initial_input, target_frames = data\n",
    "\n",
    "    frames_list = [initial_input.unsqueeze(0)] + [tf.unsqueeze(0) for tf in target_frames]\n",
    "    seq_len = len(frames_list)\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    count_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for start in range(0, seq_len - 1, buffer_size):\n",
    "            end = min(start + buffer_size + 1, seq_len)\n",
    "            chunk = [frame.to(device, non_blocking=True) for frame in frames_list[start:end]]\n",
    "            x_chunk = (torch.cat(chunk, dim=0).unsqueeze(1)) / 255.0  # normalize input\n",
    "            z_seq, z_trans, mu_seq, logvar_seq = model(x_chunk)\n",
    "            chunk_len = z_seq.size(0)\n",
    "            recon_loss = 0.0\n",
    "            latent_loss_total = 0.0\n",
    "            kl_loss_total = 0.0\n",
    "\n",
    "            for t in range(chunk_len - 1):\n",
    "                z_pred = z_trans[t]\n",
    "                z_true = z_seq[t+1]\n",
    "                latent_loss_total += latent_loss(z_pred, z_true)\n",
    "                x_recon = model.decoder(z_pred.unsqueeze(0))  # output in [0,1]\n",
    "                x_target = chunk[t+1] / 255.0\n",
    "                recon_loss += reconstruction_loss(x_recon, x_target)\n",
    "                kl_loss_total += kl_divergence(mu_seq[t+1], logvar_seq[t+1])\n",
    "            \n",
    "            chunk_loss = lambda_recon * recon_loss + lambda_latent * latent_loss_total + lambda_kl * kl_loss_total\n",
    "            total_loss += chunk_loss.item()\n",
    "            count_loss += (chunk_len - 1)\n",
    "\n",
    "            del x_chunk, z_seq, z_trans, mu_seq, logvar_seq, chunk\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    avg_loss = total_loss / count_loss if count_loss > 0 else float('inf')\n",
    "    return avg_loss\n",
    "\n",
    "def main_training():\n",
    "    train_videos = [f for f in os.listdir(TRAIN_FOLDER) if f.lower().endswith(('.mov','.mp4','.avi'))]\n",
    "    train_videos.sort()\n",
    "    val_videos = [f for f in os.listdir(VAL_FOLDER) if f.lower().endswith(('.mov','.mp4','.avi'))]\n",
    "    val_videos.sort()\n",
    "    if MAX_TRAIN_VIDS is not None:\n",
    "        train_videos = train_videos[:MAX_TRAIN_VIDS]\n",
    "    if MAX_VAL_VIDS is not None:\n",
    "        val_videos = val_videos[:MAX_VAL_VIDS]\n",
    "    print(f\"\\n[Info] Number of training videos: {len(train_videos)}\")\n",
    "    print(f\"[Info] Number of validation videos: {len(val_videos)}\")\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    model = VAE_Transformer(latent_dim=latent_dim, num_heads=4, num_layers=3, \n",
    "                            dim_feedforward=256, dropout=0.1, max_len=100).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    for epoch in range(1, NUM_EPOCHS + 1):\n",
    "        start_time = time.time()\n",
    "        print(f\"\\n[Epoch {epoch}/{NUM_EPOCHS}] - Training Phase\")\n",
    "        epoch_train_loss = []\n",
    "        for video_file in tqdm(train_videos, desc=\"Training Videos\", unit=\"video\"):\n",
    "            video_path = os.path.join(TRAIN_FOLDER, video_file)\n",
    "            loss = train_on_video_sequence(video_path, model, optimizer, device, buffer_size=BUFFER_SIZE, teacher_forcing_ratio=TEACHER_FORCING_RATIO)\n",
    "            if loss is not None:\n",
    "                epoch_train_loss.append(loss)\n",
    "        avg_train_loss = sum(epoch_train_loss) / len(epoch_train_loss) if epoch_train_loss else float('inf')\n",
    "        train_losses.append(avg_train_loss)\n",
    "        print(f\"[Epoch {epoch}] Average Training Loss: {avg_train_loss:.6f}\")\n",
    "\n",
    "        if RUN_VALIDATION:\n",
    "            print(f\"\\n[Epoch {epoch}/{NUM_EPOCHS}] - Validation Phase\")\n",
    "            epoch_val_loss = []\n",
    "            for video_file in tqdm(val_videos, desc=\"Validation Videos\", unit=\"video\"):\n",
    "                video_path = os.path.join(VAL_FOLDER, video_file)\n",
    "                loss = validate_on_video_sequence(video_path, model, device, buffer_size=BUFFER_SIZE)\n",
    "                if loss is not None:\n",
    "                    epoch_val_loss.append(loss)\n",
    "            avg_val_loss = sum(epoch_val_loss) / len(epoch_val_loss) if epoch_val_loss else float('inf')\n",
    "            val_losses.append(avg_val_loss)\n",
    "            print(f\"[Epoch {epoch}] Average Validation Loss: {avg_val_loss:.6f}\")\n",
    "            best_val_loss = avg_val_loss\n",
    "        else:\n",
    "            print(\"[Info] Validation is turned OFF for faster training.\")\n",
    "            best_val_loss = 0.0\n",
    "\n",
    "        ckpt_path = os.path.join(CHECKPOINT_DIR, f\"vae_transformer_epoch_{epoch}.pth\")\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'best_val_loss': best_val_loss\n",
    "        }, ckpt_path)\n",
    "        print(f\"[Info] Checkpoint saved at epoch {epoch} with validation loss {best_val_loss:.6f}.\")\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"[Epoch {epoch}] Completed in {elapsed_time:.2f} seconds.\")\n",
    "\n",
    "    epochs = range(1, NUM_EPOCHS + 1)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, train_losses, label='Training Loss', marker='o')\n",
    "    if RUN_VALIDATION:\n",
    "        plt.plot(epochs, val_losses, label='Validation Loss', marker='x')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    print(\"[Info] Training process completed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_training()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN + Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# train_cnn_transformer_frame_predictor.py\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "# ============================== Global Flags and Hyperparameters ==============================\n",
    "RUN_VALIDATION = False          # Set to True if you want to run validation\n",
    "lambda_recon = 1.0              # Weight for reconstruction (L1) loss\n",
    "\n",
    "# Hyperparameters for training and architecture\n",
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = 1e-4\n",
    "BUFFER_SIZE = 50                # Number of frames per rolling chunk\n",
    "latent_dim = 256                # Dimension of the latent representation\n",
    "num_heads = 4\n",
    "num_layers = 3\n",
    "dim_feedforward = 512\n",
    "dropout = 0.1\n",
    "max_len = 100                   # Maximum sequence length for positional encoding\n",
    "\n",
    "# ============================== Device Configuration ==============================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"[Info] Using device: {device}\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# ============================== Global Config & Paths ==============================\n",
    "TRAIN_FOLDER = \"bdd100k/videos/train\"  # Path to training videos\n",
    "VAL_FOLDER = \"bdd100k/videos/val\"        # Path to validation videos\n",
    "CHECKPOINT_DIR = \"checkpoints\"           # Directory to save checkpoints\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "MAX_TRAIN_VIDS = 10  # Limit number of training videos\n",
    "MAX_VAL_VIDS = 10    # Limit number of validation videos\n",
    "\n",
    "# ============================== Frame Size Configuration ==============================\n",
    "# Original frame size\n",
    "ORIG_HEIGHT, ORIG_WIDTH = 720, 1280\n",
    "# Scaling factors (adjust as needed)\n",
    "FRAME_SCALE_INPUT = 0.5  \n",
    "FRAME_SCALE_OUTPUT = 0.5  \n",
    "SCALED_INPUT_HEIGHT = int(ORIG_HEIGHT * FRAME_SCALE_INPUT)    # e.g., 360\n",
    "SCALED_INPUT_WIDTH  = int(ORIG_WIDTH * FRAME_SCALE_INPUT)      # e.g., 640\n",
    "SCALED_OUTPUT_HEIGHT = int(ORIG_HEIGHT * FRAME_SCALE_OUTPUT)   # e.g., 360\n",
    "SCALED_OUTPUT_WIDTH  = int(ORIG_WIDTH * FRAME_SCALE_OUTPUT)     # e.g., 640\n",
    "TARGET_SIZE = (SCALED_INPUT_HEIGHT, SCALED_INPUT_WIDTH)\n",
    "\n",
    "print(\"\\n===== FRAME INFO =====\")\n",
    "print(f\"Original Frame Size: {ORIG_HEIGHT} x {ORIG_WIDTH}\")\n",
    "print(f\"Target Frame Size: {TARGET_SIZE[0]} x {TARGET_SIZE[1]}\")\n",
    "\n",
    "# ============================== Positional Encoding for the Transformer ==============================\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, max_len=60000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(1)  # shape (max_len, 1, embed_dim)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (seq_len, batch, embed_dim)\n",
    "        if x.size(0) > self.pe.size(0):\n",
    "            raise ValueError(f\"Sequence length {x.size(0)} exceeds maximum {self.pe.size(0)}.\")\n",
    "        return x + self.pe[:x.size(0)]\n",
    "\n",
    "# ============================== CNN Encoder: Normalizes & Encodes Frames ==============================\n",
    "class CNNEncoder(nn.Module):\n",
    "    def __init__(self, latent_dim=256):\n",
    "        super(CNNEncoder, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            # Input: (B, 3, H, W)\n",
    "            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),   # (B, 64, H/2, W/2)\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),   # (B, 128, H/4, W/4)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),   # (B, 256, H/8, W/8)\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        # Save the feature map shape for later use in the decoder.\n",
    "        self.feature_shape = (256, SCALED_INPUT_HEIGHT // 8, SCALED_INPUT_WIDTH // 8)\n",
    "        self.flatten_dim = self.feature_shape[0] * self.feature_shape[1] * self.feature_shape[2]\n",
    "        self.fc = nn.Linear(self.flatten_dim, latent_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (B, 3, H, W)\n",
    "        features = self.conv(x)  # (B, 256, H/8, W/8)\n",
    "        features_flat = features.view(features.size(0), -1)\n",
    "        latent = self.fc(features_flat)  # (B, latent_dim)\n",
    "        return latent\n",
    "\n",
    "# ============================== CNN Decoder: Decodes Latents & De-normalizes ==============================\n",
    "class CNNDecoder(nn.Module):\n",
    "    def __init__(self, latent_dim=256, feature_shape=(256, SCALED_INPUT_HEIGHT // 8, SCALED_INPUT_WIDTH // 8)):\n",
    "        super(CNNDecoder, self).__init__()\n",
    "        self.feature_shape = feature_shape\n",
    "        flatten_dim = feature_shape[0] * feature_shape[1] * feature_shape[2]\n",
    "        self.fc = nn.Linear(latent_dim, flatten_dim)\n",
    "        self.deconv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),  # (B, 128, H/4, W/4)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),   # (B, 64, H/2, W/2)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1),     # (B, 3, H, W)\n",
    "            nn.Sigmoid()  # Ensures outputs are in [0, 1]\n",
    "        )\n",
    "        \n",
    "    def forward(self, z):\n",
    "        # z: (B, latent_dim)\n",
    "        x = self.fc(z)\n",
    "        x = x.view(z.size(0), *self.feature_shape)\n",
    "        x = self.deconv(x)\n",
    "        # Optionally, you can de-normalize here (e.g., multiply by 255) for visualization.\n",
    "        return x\n",
    "\n",
    "# ============================== Transformer Module ==============================\n",
    "class LatentTransformer(nn.Module):\n",
    "    def __init__(self, latent_dim=256, num_heads=4, num_layers=3, dim_feedforward=512, dropout=0.1, max_len=100):\n",
    "        super(LatentTransformer, self).__init__()\n",
    "        self.pos_encoder = PositionalEncoding(latent_dim, max_len=max_len)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=latent_dim, nhead=num_heads,\n",
    "                                                   dim_feedforward=dim_feedforward, dropout=dropout)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "    def forward(self, z_seq):\n",
    "        # z_seq: (seq_len, batch, latent_dim)\n",
    "        z_seq = self.pos_encoder(z_seq)\n",
    "        out = self.transformer(z_seq)\n",
    "        return out\n",
    "\n",
    "# ============================== Overall Model: Encoder -> Transformer -> Decoder ==============================\n",
    "class CNNTransformerFramePredictor(nn.Module):\n",
    "    def __init__(self, latent_dim=256, num_heads=4, num_layers=3, \n",
    "                 dim_feedforward=512, dropout=0.1, max_len=100):\n",
    "        super(CNNTransformerFramePredictor, self).__init__()\n",
    "        self.encoder = CNNEncoder(latent_dim=latent_dim)\n",
    "        self.transformer = LatentTransformer(latent_dim=latent_dim, num_heads=num_heads, \n",
    "                                             num_layers=num_layers, dim_feedforward=dim_feedforward, \n",
    "                                             dropout=dropout, max_len=max_len)\n",
    "        self.decoder = CNNDecoder(latent_dim=latent_dim, feature_shape=self.encoder.feature_shape)\n",
    "        \n",
    "    def forward(self, x_seq):\n",
    "        \"\"\"\n",
    "        x_seq: (seq_len, batch, 3, H, W) – a sequence of normalized frames.\n",
    "        Returns:\n",
    "          latent_seq: The sequence of encoded latents.\n",
    "          transformer_out: The transformer’s output sequence.\n",
    "        \"\"\"\n",
    "        latent_seq = []\n",
    "        for x in x_seq:\n",
    "            z = self.encoder(x)  # (batch, latent_dim)\n",
    "            latent_seq.append(z.unsqueeze(0))\n",
    "        latent_seq = torch.cat(latent_seq, dim=0)  # (seq_len, batch, latent_dim)\n",
    "        transformer_out = self.transformer(latent_seq)  # (seq_len, batch, latent_dim)\n",
    "        return latent_seq, transformer_out\n",
    "\n",
    "# ============================== Loss Function ==============================\n",
    "def reconstruction_loss(x_recon, x_target):\n",
    "    return F.l1_loss(x_recon, x_target)\n",
    "\n",
    "# ============================== Data Preparation ==============================\n",
    "def process_frame(frame, target_size):\n",
    "    H, W = frame.shape[:2]\n",
    "    if (H, W) != target_size:\n",
    "        frame = cv2.resize(frame, (target_size[1], target_size[0]))\n",
    "    return frame\n",
    "\n",
    "def prepare_video_sequence(video_path):\n",
    "    \"\"\"\n",
    "    Loads all frames from the video and processes/resizes them.\n",
    "    Frames are kept in the original [0,255] range.\n",
    "    Returns a list of torch tensors.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        # Convert BGR to RGB\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frames.append(frame_rgb)\n",
    "    cap.release()\n",
    "    if len(frames) < 2:\n",
    "        return None\n",
    "    processed_frames = []\n",
    "    for frame in frames:\n",
    "        processed = process_frame(frame, (SCALED_INPUT_HEIGHT, SCALED_INPUT_WIDTH))\n",
    "        tensor_frame = (torch.from_numpy(np.ascontiguousarray(processed))\n",
    "                            .permute(2, 0, 1)\n",
    "                            .float()\n",
    "                            .pin_memory())\n",
    "        processed_frames.append(tensor_frame)\n",
    "    return processed_frames\n",
    "\n",
    "# ============================== Training and Validation Loops ==============================\n",
    "def train_on_video_sequence(video_path, model, optimizer, device, buffer_size=BUFFER_SIZE):\n",
    "    \"\"\"\n",
    "    Processes the video in overlapping chunks. For each chunk, the model predicts the next frame.\n",
    "    For each time step t in the chunk, the transformer output at time t is decoded to reconstruct frame t+1.\n",
    "    \"\"\"\n",
    "    data = prepare_video_sequence(video_path)\n",
    "    if data is None:\n",
    "        print(f\"[Warning] Video {video_path} has too few frames. Skipping.\")\n",
    "        return None\n",
    "    seq_len = len(data)\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    scaler = GradScaler()\n",
    "    total_loss = 0.0\n",
    "    count_loss = 0\n",
    "\n",
    "    # Process video in overlapping chunks\n",
    "    for start in range(0, seq_len - 1, buffer_size):\n",
    "        end = min(start + buffer_size + 1, seq_len)  # +1 to have target for the last frame in the chunk\n",
    "        # Move current chunk to GPU and normalize pixel values to [0, 1]\n",
    "        chunk = [frame.to(device, non_blocking=True) for frame in data[start:end]]\n",
    "        # x_chunk: (chunk_len, 3, H, W); add batch dimension (batch=1)\n",
    "        x_chunk = torch.stack(chunk, dim=0).unsqueeze(1)  # shape: (chunk_len, 1, 3, H, W)\n",
    "        x_chunk = x_chunk / 255.0  # normalize\n",
    "\n",
    "        with autocast():\n",
    "            # Pass the sequence through the model.\n",
    "            latent_seq, transformer_out = model(x_chunk)  # both: (seq_len, batch, latent_dim)\n",
    "            recon_loss = 0.0\n",
    "            # For each time step t, use transformer output at time t to predict frame at time t+1.\n",
    "            for t in range(latent_seq.size(0) - 1):\n",
    "                z_pred = transformer_out[t]  # shape: (batch, latent_dim)\n",
    "                # Decode the predicted latent into a frame.\n",
    "                x_recon = model.decoder(z_pred)  # (batch, 3, H, W) with values in [0,1]\n",
    "                # Ground-truth next frame (already normalized)\n",
    "                x_target = x_chunk[t + 1]\n",
    "                recon_loss += reconstruction_loss(x_recon, x_target)\n",
    "            loss = lambda_recon * recon_loss\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item() * (latent_seq.size(0) - 1)\n",
    "        count_loss += (latent_seq.size(0) - 1)\n",
    "\n",
    "        del x_chunk, latent_seq, transformer_out, chunk\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    avg_loss = total_loss / count_loss if count_loss > 0 else float('inf')\n",
    "    return avg_loss\n",
    "\n",
    "def validate_on_video_sequence(video_path, model, device, buffer_size=BUFFER_SIZE):\n",
    "    data = prepare_video_sequence(video_path)\n",
    "    if data is None:\n",
    "        print(f\"[Warning] Video {video_path} has too few frames. Skipping.\")\n",
    "        return None\n",
    "    seq_len = len(data)\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    count_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for start in range(0, seq_len - 1, buffer_size):\n",
    "            end = min(start + buffer_size + 1, seq_len)\n",
    "            chunk = [frame.to(device, non_blocking=True) for frame in data[start:end]]\n",
    "            x_chunk = torch.stack(chunk, dim=0).unsqueeze(1)  # (chunk_len, 1, 3, H, W)\n",
    "            x_chunk = x_chunk / 255.0\n",
    "            latent_seq, transformer_out = model(x_chunk)\n",
    "            recon_loss = 0.0\n",
    "            for t in range(latent_seq.size(0) - 1):\n",
    "                z_pred = transformer_out[t]\n",
    "                x_recon = model.decoder(z_pred)\n",
    "                x_target = x_chunk[t + 1]\n",
    "                recon_loss += reconstruction_loss(x_recon, x_target)\n",
    "            loss = lambda_recon * recon_loss\n",
    "            total_loss += loss.item() * (latent_seq.size(0) - 1)\n",
    "            count_loss += (latent_seq.size(0) - 1)\n",
    "            del x_chunk, latent_seq, transformer_out, chunk\n",
    "            torch.cuda.empty_cache()\n",
    "    avg_loss = total_loss / count_loss if count_loss > 0 else float('inf')\n",
    "    return avg_loss\n",
    "\n",
    "# ============================== Main Training Loop ==============================\n",
    "def main_training():\n",
    "    train_videos = [f for f in os.listdir(TRAIN_FOLDER) if f.lower().endswith(('.mov', '.mp4', '.avi'))]\n",
    "    train_videos.sort()\n",
    "    val_videos = [f for f in os.listdir(VAL_FOLDER) if f.lower().endswith(('.mov', '.mp4', '.avi'))]\n",
    "    val_videos.sort()\n",
    "    if MAX_TRAIN_VIDS is not None:\n",
    "        train_videos = train_videos[:MAX_TRAIN_VIDS]\n",
    "    if MAX_VAL_VIDS is not None:\n",
    "        val_videos = val_videos[:MAX_VAL_VIDS]\n",
    "    print(f\"\\n[Info] Number of training videos: {len(train_videos)}\")\n",
    "    print(f\"[Info] Number of validation videos: {len(val_videos)}\")\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    model = CNNTransformerFramePredictor(latent_dim=latent_dim, num_heads=num_heads, num_layers=num_layers,\n",
    "                                           dim_feedforward=dim_feedforward, dropout=dropout, max_len=max_len).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    for epoch in range(1, NUM_EPOCHS + 1):\n",
    "        start_time = time.time()\n",
    "        print(f\"\\n[Epoch {epoch}/{NUM_EPOCHS}] - Training Phase\")\n",
    "        epoch_train_loss = []\n",
    "        for video_file in tqdm(train_videos, desc=\"Training Videos\", unit=\"video\"):\n",
    "            video_path = os.path.join(TRAIN_FOLDER, video_file)\n",
    "            loss = train_on_video_sequence(video_path, model, optimizer, device, buffer_size=BUFFER_SIZE)\n",
    "            if loss is not None:\n",
    "                epoch_train_loss.append(loss)\n",
    "        avg_train_loss = sum(epoch_train_loss) / len(epoch_train_loss) if epoch_train_loss else float('inf')\n",
    "        train_losses.append(avg_train_loss)\n",
    "        print(f\"[Epoch {epoch}] Average Training Loss: {avg_train_loss:.6f}\")\n",
    "\n",
    "        if RUN_VALIDATION:\n",
    "            print(f\"\\n[Epoch {epoch}/{NUM_EPOCHS}] - Validation Phase\")\n",
    "            epoch_val_loss = []\n",
    "            for video_file in tqdm(val_videos, desc=\"Validation Videos\", unit=\"video\"):\n",
    "                video_path = os.path.join(VAL_FOLDER, video_file)\n",
    "                loss = validate_on_video_sequence(video_path, model, device, buffer_size=BUFFER_SIZE)\n",
    "                if loss is not None:\n",
    "                    epoch_val_loss.append(loss)\n",
    "            avg_val_loss = sum(epoch_val_loss) / len(epoch_val_loss) if epoch_val_loss else float('inf')\n",
    "            val_losses.append(avg_val_loss)\n",
    "            print(f\"[Epoch {epoch}] Average Validation Loss: {avg_val_loss:.6f}\")\n",
    "            best_val_loss = avg_val_loss\n",
    "        else:\n",
    "            print(\"[Info] Validation is turned OFF for faster training.\")\n",
    "            best_val_loss = 0.0\n",
    "\n",
    "        # Save checkpoint.\n",
    "        ckpt_path = os.path.join(CHECKPOINT_DIR, f\"cnn_transformer_epoch_{epoch}.pth\")\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'best_val_loss': best_val_loss\n",
    "        }, ckpt_path)\n",
    "        print(f\"[Info] Checkpoint saved at epoch {epoch} with validation loss {best_val_loss:.6f}.\")\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"[Epoch {epoch}] Completed in {elapsed_time:.2f} seconds.\")\n",
    "\n",
    "    # Plot training (and validation) loss curves.\n",
    "    epochs = range(1, NUM_EPOCHS + 1)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, train_losses, label='Training Loss', marker='o')\n",
    "    if RUN_VALIDATION:\n",
    "        plt.plot(epochs, val_losses, label='Validation Loss', marker='x')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    print(\"[Info] Training process completed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_training()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN + GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python \n",
    "\"\"\"\n",
    "train_gan_from_embeddings_ffn.py\n",
    "\n",
    "This script trains a GAN that uses a fully convolutional feed-forward encoder \n",
    "to downscale video frames into a latent feature representation.\n",
    "The generator then upsamples that latent code to reconstruct an image.\n",
    "The discriminator (using logits loss) distinguishes real frames from generated ones.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ============================== Global Flags and Hyperparameters ==============================\n",
    "NUM_EPOCHS = 50\n",
    "LEARNING_RATE = 1e-4\n",
    "BUFFER_SIZE = 50                # Number of frames per chunk (batch size for training)\n",
    "latent_dim = 64                 # We now interpret this as the number of channels in the latent feature map\n",
    "lambda_recon = 1.0              # Weight for reconstruction (L1) loss\n",
    "RUN_VALIDATION = False          # Set to True if you want to run validation\n",
    "\n",
    "# ============================== Device Configuration ==============================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"[Info] Using device: {device}\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# ============================== Global Config & Paths ==============================\n",
    "TRAIN_FOLDER = \"bdd100k/videos/train\"  # Path to training videos\n",
    "VAL_FOLDER   = \"bdd100k/videos/val\"      # Path to validation videos\n",
    "CHECKPOINT_DIR = \"checkpoints_gan\"\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "MAX_TRAIN_VIDS = 1  # Limit number of training videos\n",
    "MAX_VAL_VIDS = 1    # Limit number of validation videos\n",
    "\n",
    "# ============================== Frame Size Configuration ==============================\n",
    "ORIG_HEIGHT, ORIG_WIDTH = 720, 1280\n",
    "FRAME_SCALE_INPUT = 0.10  \n",
    "FRAME_SCALE_OUTPUT = 0.10  \n",
    "SCALED_INPUT_HEIGHT = int(ORIG_HEIGHT * FRAME_SCALE_INPUT)\n",
    "SCALED_INPUT_WIDTH  = int(ORIG_WIDTH * FRAME_SCALE_INPUT)\n",
    "TARGET_SIZE = (SCALED_INPUT_HEIGHT, SCALED_INPUT_WIDTH)\n",
    "\n",
    "print(\"\\n===== FRAME INFO =====\")\n",
    "print(f\"Original Frame Size: {ORIG_HEIGHT} x {ORIG_WIDTH}\")\n",
    "print(f\"Target Frame Size: {TARGET_SIZE[0]} x {TARGET_SIZE[1]}\")\n",
    "\n",
    "# ============================== Data Preparation ==============================\n",
    "def process_frame(frame, target_size):\n",
    "    \"\"\"Resize the frame to the target size if necessary.\"\"\"\n",
    "    H, W = frame.shape[:2]\n",
    "    if (H, W) != target_size:\n",
    "        frame = cv2.resize(frame, (target_size[1], target_size[0]))\n",
    "    return frame\n",
    "\n",
    "def prepare_video_sequence(video_path):\n",
    "    \"\"\"\n",
    "    Loads all frames from the video and processes/resizes them.\n",
    "    Returns a list of torch tensors (with shape [3, H, W] and type float).\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        # Convert BGR to RGB\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frames.append(frame_rgb)\n",
    "    cap.release()\n",
    "    if len(frames) < 1:\n",
    "        return None\n",
    "    processed_frames = []\n",
    "    for frame in frames:\n",
    "        processed = process_frame(frame, TARGET_SIZE)\n",
    "        tensor_frame = (torch.from_numpy(np.ascontiguousarray(processed))\n",
    "                            .permute(2, 0, 1)\n",
    "                            .float()\n",
    "                            .pin_memory())\n",
    "        processed_frames.append(tensor_frame)\n",
    "    return processed_frames\n",
    "\n",
    "# ============================== Network Definitions ==============================\n",
    "\n",
    "# ----- FFNEncoder: A fully convolutional encoder that downscales the input image -----\n",
    "# This network is similar in spirit to the contracting (downsampling) part of a U-net,\n",
    "# but here it simply produces a latent feature map of shape (latent_dim, H/8, W/8).\n",
    "class FFNEncoder(nn.Module):\n",
    "    def __init__(self, latent_channels=latent_dim):\n",
    "        super(FFNEncoder, self).__init__()\n",
    "        self.enc1 = nn.Sequential(\n",
    "            # Input: (B, 3, H, W) -> (B, 64, H/2, W/2)\n",
    "            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.enc2 = nn.Sequential(\n",
    "            # (B, 64, H/2, W/2) -> (B, 128, H/4, W/4)\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.enc3 = nn.Sequential(\n",
    "            # (B, 128, H/4, W/4) -> (B, latent_channels, H/8, W/8)\n",
    "            nn.Conv2d(128, latent_channels, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(latent_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.enc1(x)   # shape: (B, 64, H/2, W/2)\n",
    "        x2 = self.enc2(x1)  # shape: (B, 128, H/4, W/4)\n",
    "        x3 = self.enc3(x2)  # shape: (B, latent_channels, H/8, W/8)\n",
    "        return x3\n",
    "\n",
    "# ----- FFNGenerator: Upsamples the latent feature map back to an image -----\n",
    "# Here, we add extra convolutional blocks after each upsampling step.\n",
    "class FFNGenerator(nn.Module):\n",
    "    def __init__(self, latent_channels=latent_dim):\n",
    "        super(FFNGenerator, self).__init__()\n",
    "        self.dec1 = nn.Sequential(\n",
    "            # Upsample: (B, latent_channels, H/8, W/8) -> (B, 128, H/4, W/4)\n",
    "            nn.ConvTranspose2d(latent_channels, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # Extra layers at H/4, W/4 resolution\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.dec2 = nn.Sequential(\n",
    "            # Upsample: (B, 128, H/4, W/4) -> (B, 64, H/2, W/2)\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # Extra layers at H/2, W/2 resolution\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.dec3 = nn.Sequential(\n",
    "            # Upsample: (B, 64, H/2, W/2) -> (B, 32, H, W)\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # Extra layers at full resolution\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # Final conversion to 3 channels.\n",
    "            nn.Conv2d(32, 3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Sigmoid()  # outputs values in [0, 1]\n",
    "        )\n",
    "        \n",
    "    def forward(self, latent):\n",
    "        x = self.dec1(latent)\n",
    "        x = self.dec2(x)\n",
    "        x = self.dec3(x)\n",
    "        return x\n",
    "\n",
    "# ----- Discriminator: Distinguishes real images from generated ones -----\n",
    "# (Same as before; note that we remove the final Sigmoid so we can use BCEWithLogitsLoss.)\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_channels=3):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.AdaptiveAvgPool2d((1,1))\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512, 1)  # No Sigmoid here!\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# ============================== GAN Training Function ==============================\n",
    "def train_on_video_sequence(video_path, encoder, generator, discriminator,\n",
    "                            optimizer_G, optimizer_D, device, buffer_size=BUFFER_SIZE):\n",
    "    \"\"\"\n",
    "    For each chunk of frames from the video:\n",
    "      - The encoder produces a latent feature map from real frames.\n",
    "      - The generator produces fake images from these latent features.\n",
    "      - The discriminator is updated to distinguish real from fake images.\n",
    "      - The encoder and generator are updated using both an adversarial loss and a reconstruction loss.\n",
    "    \"\"\"\n",
    "    data = prepare_video_sequence(video_path)\n",
    "    if data is None:\n",
    "        print(f\"[Warning] Video {video_path} has too few frames. Skipping.\")\n",
    "        return None, None\n",
    "    seq_len = len(data)\n",
    "    encoder.train()\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    scaler = GradScaler()\n",
    "    total_loss_G = 0.0\n",
    "    total_loss_D = 0.0\n",
    "    count = 0\n",
    "\n",
    "    for start in range(0, seq_len, buffer_size):\n",
    "        end = min(start + buffer_size, seq_len)\n",
    "        frames = [frame.to(device, non_blocking=True) for frame in data[start:end]]\n",
    "        x_real = torch.stack(frames, dim=0) / 255.0  # shape: (batch, 3, H, W)\n",
    "        batch_size = x_real.size(0)\n",
    "        count += batch_size\n",
    "\n",
    "        # ------------------------------\n",
    "        # Train the Discriminator\n",
    "        # ------------------------------\n",
    "        optimizer_D.zero_grad()\n",
    "        with autocast():\n",
    "            label_real = torch.ones(batch_size, 1, device=device)\n",
    "            output_real = discriminator(x_real)\n",
    "            loss_D_real = F.binary_cross_entropy_with_logits(output_real, label_real)\n",
    "            \n",
    "            latent = encoder(x_real)\n",
    "            x_fake = generator(latent)\n",
    "            label_fake = torch.zeros(batch_size, 1, device=device)\n",
    "            output_fake = discriminator(x_fake.detach())\n",
    "            loss_D_fake = F.binary_cross_entropy_with_logits(output_fake, label_fake)\n",
    "            \n",
    "            loss_D = (loss_D_real + loss_D_fake) * 0.5\n",
    "\n",
    "        scaler.scale(loss_D).backward()\n",
    "        scaler.step(optimizer_D)\n",
    "\n",
    "        # ------------------------------\n",
    "        # Train the Encoder and Generator (Generator Loss)\n",
    "        # ------------------------------\n",
    "        optimizer_G.zero_grad()\n",
    "        with autocast():\n",
    "            latent = encoder(x_real)\n",
    "            x_fake = generator(latent)\n",
    "            output_fake_for_G = discriminator(x_fake)\n",
    "            loss_G_adv = F.binary_cross_entropy_with_logits(output_fake_for_G, label_real)\n",
    "            loss_recon = F.l1_loss(x_fake, x_real)\n",
    "            loss_G = loss_G_adv + lambda_recon * loss_recon\n",
    "\n",
    "        scaler.scale(loss_G).backward()\n",
    "        scaler.step(optimizer_G)\n",
    "        scaler.update()\n",
    "\n",
    "        total_loss_D += loss_D.item() * batch_size\n",
    "        total_loss_G += loss_G.item() * batch_size\n",
    "\n",
    "    avg_loss_D = total_loss_D / count if count > 0 else float('inf')\n",
    "    avg_loss_G = total_loss_G / count if count > 0 else float('inf')\n",
    "    return avg_loss_G, avg_loss_D\n",
    "\n",
    "# ============================== (Optional) Validation Function ==============================\n",
    "def validate_on_video_sequence(video_path, encoder, generator, discriminator, device, buffer_size=BUFFER_SIZE):\n",
    "    data = prepare_video_sequence(video_path)\n",
    "    if data is None:\n",
    "        print(f\"[Warning] Video {video_path} has too few frames. Skipping.\")\n",
    "        return None, None\n",
    "    seq_len = len(data)\n",
    "    encoder.eval()\n",
    "    generator.eval()\n",
    "    discriminator.eval()\n",
    "    total_loss_D = 0.0\n",
    "    total_loss_G = 0.0\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for start in range(0, seq_len, buffer_size):\n",
    "            end = min(start + buffer_size, seq_len)\n",
    "            frames = [frame.to(device, non_blocking=True) for frame in data[start:end]]\n",
    "            x_real = torch.stack(frames, dim=0) / 255.0\n",
    "            batch_size = x_real.size(0)\n",
    "            count += batch_size\n",
    "\n",
    "            label_real = torch.ones(batch_size, 1, device=device)\n",
    "            output_real = discriminator(x_real)\n",
    "            loss_D_real = F.binary_cross_entropy_with_logits(output_real, label_real)\n",
    "\n",
    "            latent = encoder(x_real)\n",
    "            x_fake = generator(latent)\n",
    "            label_fake = torch.zeros(batch_size, 1, device=device)\n",
    "            output_fake = discriminator(x_fake)\n",
    "            loss_D_fake = F.binary_cross_entropy_with_logits(output_fake, label_fake)\n",
    "            loss_D = (loss_D_real + loss_D_fake) * 0.5\n",
    "\n",
    "            output_fake_for_G = discriminator(x_fake)\n",
    "            loss_G_adv = F.binary_cross_entropy_with_logits(output_fake_for_G, label_real)\n",
    "            loss_recon = F.l1_loss(x_fake, x_real)\n",
    "            loss_G = loss_G_adv + lambda_recon * loss_recon\n",
    "\n",
    "            total_loss_D += loss_D.item() * batch_size\n",
    "            total_loss_G += loss_G.item() * batch_size\n",
    "\n",
    "    avg_loss_D = total_loss_D / count if count > 0 else float('inf')\n",
    "    avg_loss_G = total_loss_G / count if count > 0 else float('inf')\n",
    "    return avg_loss_G, avg_loss_D\n",
    "\n",
    "# ============================== Main Training Loop ==============================\n",
    "def main_training():\n",
    "    # List video files for training and (optionally) validation\n",
    "    train_videos = [f for f in os.listdir(TRAIN_FOLDER) if f.lower().endswith(('.mov', '.mp4', '.avi'))]\n",
    "    train_videos.sort()\n",
    "    val_videos = [f for f in os.listdir(VAL_FOLDER) if f.lower().endswith(('.mov', '.mp4', '.avi'))]\n",
    "    val_videos.sort()\n",
    "    if MAX_TRAIN_VIDS is not None:\n",
    "        train_videos = train_videos[:MAX_TRAIN_VIDS]\n",
    "    if MAX_VAL_VIDS is not None:\n",
    "        val_videos = val_videos[:MAX_VAL_VIDS]\n",
    "    print(f\"\\n[Info] Number of training videos: {len(train_videos)}\")\n",
    "    if RUN_VALIDATION:\n",
    "        print(f\"[Info] Number of validation videos: {len(val_videos)}\")\n",
    "    \n",
    "    # Initialize networks with the new FFN encoder and deeper generator\n",
    "    encoder = FFNEncoder(latent_channels=latent_dim).to(device)\n",
    "    generator = FFNGenerator(latent_channels=latent_dim).to(device)\n",
    "    discriminator = Discriminator(input_channels=3).to(device)\n",
    "\n",
    "    # Create separate optimizers for (encoder + generator) and discriminator\n",
    "    optimizer_G = torch.optim.Adam(list(encoder.parameters()) + list(generator.parameters()),\n",
    "                                   lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
    "    optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
    "    \n",
    "    train_losses_G = []\n",
    "    train_losses_D = []\n",
    "    val_losses_G = []\n",
    "    val_losses_D = []\n",
    "\n",
    "    for epoch in range(1, NUM_EPOCHS + 1):\n",
    "        start_time = time.time()\n",
    "        print(f\"\\n[Epoch {epoch}/{NUM_EPOCHS}] - Training Phase\")\n",
    "        epoch_train_loss_G = []\n",
    "        epoch_train_loss_D = []\n",
    "        for video_file in tqdm(train_videos, desc=\"Training Videos\", unit=\"video\"):\n",
    "            video_path = os.path.join(TRAIN_FOLDER, video_file)\n",
    "            loss_G, loss_D = train_on_video_sequence(video_path, encoder, generator, discriminator,\n",
    "                                                     optimizer_G, optimizer_D, device, buffer_size=BUFFER_SIZE)\n",
    "            if loss_G is not None and loss_D is not None:\n",
    "                epoch_train_loss_G.append(loss_G)\n",
    "                epoch_train_loss_D.append(loss_D)\n",
    "        avg_train_loss_G = sum(epoch_train_loss_G) / len(epoch_train_loss_G) if epoch_train_loss_G else float('inf')\n",
    "        avg_train_loss_D = sum(epoch_train_loss_D) / len(epoch_train_loss_D) if epoch_train_loss_D else float('inf')\n",
    "        train_losses_G.append(avg_train_loss_G)\n",
    "        train_losses_D.append(avg_train_loss_D)\n",
    "        print(f\"[Epoch {epoch}] Average Generator Loss: {avg_train_loss_G:.6f} | Average Discriminator Loss: {avg_train_loss_D:.6f}\")\n",
    "\n",
    "        if RUN_VALIDATION:\n",
    "            print(f\"\\n[Epoch {epoch}/{NUM_EPOCHS}] - Validation Phase\")\n",
    "            epoch_val_loss_G = []\n",
    "            epoch_val_loss_D = []\n",
    "            for video_file in tqdm(val_videos, desc=\"Validation Videos\", unit=\"video\"):\n",
    "                video_path = os.path.join(VAL_FOLDER, video_file)\n",
    "                loss_G, loss_D = validate_on_video_sequence(video_path, encoder, generator, discriminator,\n",
    "                                                            device, buffer_size=BUFFER_SIZE)\n",
    "                if loss_G is not None and loss_D is not None:\n",
    "                    epoch_val_loss_G.append(loss_G)\n",
    "                    epoch_val_loss_D.append(loss_D)\n",
    "            avg_val_loss_G = sum(epoch_val_loss_G) / len(epoch_val_loss_G) if epoch_val_loss_G else float('inf')\n",
    "            avg_val_loss_D = sum(epoch_val_loss_D) / len(epoch_val_loss_D) if epoch_val_loss_D else float('inf')\n",
    "            val_losses_G.append(avg_val_loss_G)\n",
    "            val_losses_D.append(avg_val_loss_D)\n",
    "            print(f\"[Epoch {epoch}] Average Validation Generator Loss: {avg_val_loss_G:.6f} | Discriminator Loss: {avg_val_loss_D:.6f}\")\n",
    "\n",
    "        # Save checkpoint.\n",
    "        ckpt_path = os.path.join(CHECKPOINT_DIR, f\"gan_epoch_{epoch}.pth\")\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'encoder_state_dict': encoder.state_dict(),\n",
    "            'generator_state_dict': generator.state_dict(),\n",
    "            'discriminator_state_dict': discriminator.state_dict(),\n",
    "            'optimizer_G_state_dict': optimizer_G.state_dict(),\n",
    "            'optimizer_D_state_dict': optimizer_D.state_dict(),\n",
    "            'train_loss_G': avg_train_loss_G,\n",
    "            'train_loss_D': avg_train_loss_D\n",
    "        }, ckpt_path)\n",
    "        print(f\"[Info] Checkpoint saved at epoch {epoch}.\")\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"[Epoch {epoch}] Completed in {elapsed_time:.2f} seconds.\")\n",
    "\n",
    "    # Plot training (and validation) loss curves.\n",
    "    epochs = range(1, NUM_EPOCHS + 1)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, train_losses_G, label='Train Generator Loss', marker='o')\n",
    "    plt.plot(epochs, train_losses_D, label='Train Discriminator Loss', marker='o')\n",
    "    if RUN_VALIDATION:\n",
    "        plt.plot(epochs, val_losses_G, label='Val Generator Loss', marker='x')\n",
    "        plt.plot(epochs, val_losses_D, label='Val Discriminator Loss', marker='x')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('GAN Training Loss over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    print(\"[Info] Training process completed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_training()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA + GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python \n",
    "\"\"\"\n",
    "train_gan_from_embeddings_ffn.py\n",
    "\n",
    "This script trains a GAN that uses a PCA-based encoder to project video frames \n",
    "into a latent code. The latent code is fed into a Generator that upsamples it to \n",
    "reconstruct the image. A Discriminator (using logits loss) is used to distinguish \n",
    "real frames from generated ones.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ============================== Global Flags and Hyperparameters ==============================\n",
    "NUM_EPOCHS = 200\n",
    "LEARNING_RATE = 1e-4\n",
    "BUFFER_SIZE = 50                # Number of frames per chunk (batch size for training)\n",
    "latent_dim = 128                 # Dimension of the latent code produced by PCA\n",
    "lambda_recon = 1.0              # Weight for reconstruction (L1) loss\n",
    "RUN_VALIDATION = False          # Set to True if you want to run validation\n",
    "\n",
    "# ============================== Device Configuration ==============================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"[Info] Using device: {device}\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# ============================== Global Config & Paths ==============================\n",
    "TRAIN_FOLDER = \"bdd100k/videos/train\"  # Path to training videos\n",
    "VAL_FOLDER   = \"bdd100k/videos/val\"      # Path to validation videos\n",
    "CHECKPOINT_DIR = \"checkpoints_gan\"\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "MAX_TRAIN_VIDS = 1  # Limit number of training videos\n",
    "MAX_VAL_VIDS = 1    # Limit number of validation videos\n",
    "\n",
    "# ============================== Frame Size Configuration ==============================\n",
    "ORIG_HEIGHT, ORIG_WIDTH = 720, 1280\n",
    "FRAME_SCALE_INPUT = 0.10  \n",
    "FRAME_SCALE_OUTPUT = 0.10  \n",
    "SCALED_INPUT_HEIGHT = int(ORIG_HEIGHT * FRAME_SCALE_INPUT)\n",
    "SCALED_INPUT_WIDTH  = int(ORIG_WIDTH * FRAME_SCALE_INPUT)\n",
    "TARGET_SIZE = (SCALED_INPUT_HEIGHT, SCALED_INPUT_WIDTH)\n",
    "\n",
    "print(\"\\n===== FRAME INFO =====\")\n",
    "print(f\"Original Frame Size: {ORIG_HEIGHT} x {ORIG_WIDTH}\")\n",
    "print(f\"Target Frame Size: {TARGET_SIZE[0]} x {TARGET_SIZE[1]}\")\n",
    "\n",
    "# ============================== Data Preparation ==============================\n",
    "def process_frame(frame, target_size):\n",
    "    \"\"\"Resize the frame to the target size if necessary.\"\"\"\n",
    "    H, W = frame.shape[:2]\n",
    "    if (H, W) != target_size:\n",
    "        frame = cv2.resize(frame, (target_size[1], target_size[0]))\n",
    "    return frame\n",
    "\n",
    "def prepare_video_sequence(video_path):\n",
    "    \"\"\"\n",
    "    Loads all frames from the video and processes/resizes them.\n",
    "    Returns a list of torch tensors (with shape [3, H, W] and type float).\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        # Convert BGR to RGB\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frames.append(frame_rgb)\n",
    "    cap.release()\n",
    "    if len(frames) < 1:\n",
    "        return None\n",
    "    processed_frames = []\n",
    "    for frame in frames:\n",
    "        processed = process_frame(frame, TARGET_SIZE)\n",
    "        tensor_frame = (torch.from_numpy(np.ascontiguousarray(processed))\n",
    "                            .permute(2, 0, 1)\n",
    "                            .float()\n",
    "                            .pin_memory())\n",
    "        processed_frames.append(tensor_frame)\n",
    "    return processed_frames\n",
    "\n",
    "# ============================== PCA Encoder Definition ==============================\n",
    "# Instead of learning a convolutional encoder, we use a precomputed PCA projection.\n",
    "# This module flattens the input image, subtracts a mean, and multiplies by a PCA basis.\n",
    "# The PCA basis and mean should be computed over your training dataset.\n",
    "class PCAEncoder(nn.Module):\n",
    "    def __init__(self, latent_dim, pca_mean, pca_basis):\n",
    "        \"\"\"\n",
    "        pca_mean: Tensor of shape (D,) where D = 3*H*W\n",
    "        pca_basis: Tensor of shape (D, latent_dim), assumed to be orthonormal.\n",
    "        \"\"\"\n",
    "        super(PCAEncoder, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        # Register as buffers so they are part of the state (but not updated by gradients)\n",
    "        self.register_buffer('pca_mean', pca_mean)\n",
    "        self.register_buffer('pca_basis', pca_basis)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (B, 3, H, W)\n",
    "        B = x.shape[0]\n",
    "        x_flat = x.view(B, -1)  # shape: (B, D)\n",
    "        x_centered = x_flat - self.pca_mean  # subtract mean\n",
    "        latent = x_centered @ self.pca_basis   # shape: (B, latent_dim)\n",
    "        return latent\n",
    "\n",
    "# For demonstration purposes, we create a dummy PCA mean and basis.\n",
    "# In practice, you should compute these from your training data.\n",
    "D = 3 * SCALED_INPUT_HEIGHT * SCALED_INPUT_WIDTH  # dimensionality of flattened image\n",
    "dummy_mean = torch.zeros(D)  # For example, mean = 0 (you would use your computed mean)\n",
    "# Create a random basis and orthonormalize it.\n",
    "dummy_basis = torch.randn(D, latent_dim)\n",
    "q, r = torch.qr(dummy_basis)  # q is orthonormal of shape (D, latent_dim)\n",
    "pca_basis = q\n",
    "\n",
    "# ============================== Generator Definition ==============================\n",
    "# Now the Generator receives a latent vector (of size latent_dim) and upsamples it.\n",
    "# We first map the latent vector into a spatial feature map via a fully connected layer.\n",
    "# We choose a target feature map shape that, when upsampled, produces the image.\n",
    "# For example, if we set feature_shape = (256, H/8, W/8), then H/8 = SCALED_INPUT_HEIGHT//8.\n",
    "feature_shape = (256, SCALED_INPUT_HEIGHT // 8, SCALED_INPUT_WIDTH // 8)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, feature_shape):\n",
    "        super(Generator, self).__init__()\n",
    "        self.feature_shape = feature_shape\n",
    "        flatten_dim = feature_shape[0] * feature_shape[1] * feature_shape[2]\n",
    "        self.fc = nn.Linear(latent_dim, flatten_dim)\n",
    "        self.deconv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),  # Upsample\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),   # Upsample\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1),     # Upsample to image\n",
    "            nn.Sigmoid()  # Outputs in [0, 1]\n",
    "        )\n",
    "        \n",
    "    def forward(self, latent):\n",
    "        x = self.fc(latent)\n",
    "        x = x.view(latent.size(0), *self.feature_shape)\n",
    "        x = self.deconv(x)\n",
    "        return x\n",
    "\n",
    "# ============================== Discriminator Definition ==============================\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_channels=3):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.AdaptiveAvgPool2d((1,1))\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512, 1)  # No Sigmoid here!\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# ============================== GAN Training Function ==============================\n",
    "def train_on_video_sequence(video_path, pca_encoder, generator, discriminator,\n",
    "                            optimizer_G, optimizer_D, device, buffer_size=BUFFER_SIZE):\n",
    "    \"\"\"\n",
    "    For each chunk of frames from the video:\n",
    "      - The PCA encoder projects real frames into a latent code.\n",
    "      - The generator produces fake images from these latent codes.\n",
    "      - The discriminator is updated to distinguish real from fake images.\n",
    "      - The PCA encoder is fixed (non-trainable) so only the generator is updated,\n",
    "        together with the discriminator.\n",
    "    \"\"\"\n",
    "    data = prepare_video_sequence(video_path)\n",
    "    if data is None:\n",
    "        print(f\"[Warning] Video {video_path} has too few frames. Skipping.\")\n",
    "        return None, None\n",
    "    seq_len = len(data)\n",
    "    # Note: The PCA encoder is not set to train() (its parameters are fixed).\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    scaler = GradScaler()\n",
    "    total_loss_G = 0.0\n",
    "    total_loss_D = 0.0\n",
    "    count = 0\n",
    "\n",
    "    for start in range(0, seq_len, buffer_size):\n",
    "        end = min(start + buffer_size, seq_len)\n",
    "        frames = [frame.to(device, non_blocking=True) for frame in data[start:end]]\n",
    "        x_real = torch.stack(frames, dim=0) / 255.0  # shape: (batch, 3, H, W)\n",
    "        batch_size = x_real.size(0)\n",
    "        count += batch_size\n",
    "\n",
    "        # ------------------------------\n",
    "        # Train the Discriminator\n",
    "        # ------------------------------\n",
    "        optimizer_D.zero_grad()\n",
    "        with autocast():\n",
    "            label_real = torch.ones(batch_size, 1, device=device)\n",
    "            output_real = discriminator(x_real)\n",
    "            loss_D_real = F.binary_cross_entropy_with_logits(output_real, label_real)\n",
    "            \n",
    "            latent = pca_encoder(x_real)  # shape: (B, latent_dim)\n",
    "            x_fake = generator(latent)\n",
    "            label_fake = torch.zeros(batch_size, 1, device=device)\n",
    "            output_fake = discriminator(x_fake.detach())\n",
    "            loss_D_fake = F.binary_cross_entropy_with_logits(output_fake, label_fake)\n",
    "            \n",
    "            loss_D = (loss_D_real + loss_D_fake) * 0.5\n",
    "\n",
    "        scaler.scale(loss_D).backward()\n",
    "        scaler.step(optimizer_D)\n",
    "\n",
    "        # ------------------------------\n",
    "        # Train the Generator (Generator Loss)\n",
    "        # ------------------------------\n",
    "        optimizer_G.zero_grad()\n",
    "        with autocast():\n",
    "            latent = pca_encoder(x_real)\n",
    "            x_fake = generator(latent)\n",
    "            output_fake_for_G = discriminator(x_fake)\n",
    "            loss_G_adv = F.binary_cross_entropy_with_logits(output_fake_for_G, label_real)\n",
    "            loss_recon = F.l1_loss(x_fake, x_real)\n",
    "            loss_G = loss_G_adv + lambda_recon * loss_recon\n",
    "\n",
    "        scaler.scale(loss_G).backward()\n",
    "        scaler.step(optimizer_G)\n",
    "        scaler.update()\n",
    "\n",
    "        total_loss_D += loss_D.item() * batch_size\n",
    "        total_loss_G += loss_G.item() * batch_size\n",
    "\n",
    "    avg_loss_D = total_loss_D / count if count > 0 else float('inf')\n",
    "    avg_loss_G = total_loss_G / count if count > 0 else float('inf')\n",
    "    return avg_loss_G, avg_loss_D\n",
    "\n",
    "# ============================== (Optional) Validation Function ==============================\n",
    "def validate_on_video_sequence(video_path, pca_encoder, generator, discriminator, device, buffer_size=BUFFER_SIZE):\n",
    "    data = prepare_video_sequence(video_path)\n",
    "    if data is None:\n",
    "        print(f\"[Warning] Video {video_path} has too few frames. Skipping.\")\n",
    "        return None, None\n",
    "    seq_len = len(data)\n",
    "    generator.eval()\n",
    "    discriminator.eval()\n",
    "    total_loss_D = 0.0\n",
    "    total_loss_G = 0.0\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for start in range(0, seq_len, buffer_size):\n",
    "            end = min(start + buffer_size, seq_len)\n",
    "            frames = [frame.to(device, non_blocking=True) for frame in data[start:end]]\n",
    "            x_real = torch.stack(frames, dim=0) / 255.0\n",
    "            batch_size = x_real.size(0)\n",
    "            count += batch_size\n",
    "\n",
    "            label_real = torch.ones(batch_size, 1, device=device)\n",
    "            output_real = discriminator(x_real)\n",
    "            loss_D_real = F.binary_cross_entropy_with_logits(output_real, label_real)\n",
    "\n",
    "            latent = pca_encoder(x_real)\n",
    "            x_fake = generator(latent)\n",
    "            label_fake = torch.zeros(batch_size, 1, device=device)\n",
    "            output_fake = discriminator(x_fake)\n",
    "            loss_D_fake = F.binary_cross_entropy_with_logits(output_fake, label_fake)\n",
    "            loss_D = (loss_D_real + loss_D_fake) * 0.5\n",
    "\n",
    "            output_fake_for_G = discriminator(x_fake)\n",
    "            loss_G_adv = F.binary_cross_entropy_with_logits(output_fake_for_G, label_real)\n",
    "            loss_recon = F.l1_loss(x_fake, x_real)\n",
    "            loss_G = loss_G_adv + lambda_recon * loss_recon\n",
    "\n",
    "            total_loss_D += loss_D.item() * batch_size\n",
    "            total_loss_G += loss_G.item() * batch_size\n",
    "\n",
    "    avg_loss_D = total_loss_D / count if count > 0 else float('inf')\n",
    "    avg_loss_G = total_loss_G / count if count > 0 else float('inf')\n",
    "    return avg_loss_G, avg_loss_D\n",
    "\n",
    "# ============================== Main Training Loop ==============================\n",
    "def main_training():\n",
    "    # List video files for training and (optionally) validation\n",
    "    train_videos = [f for f in os.listdir(TRAIN_FOLDER) if f.lower().endswith(('.mov', '.mp4', '.avi'))]\n",
    "    train_videos.sort()\n",
    "    val_videos = [f for f in os.listdir(VAL_FOLDER) if f.lower().endswith(('.mov', '.mp4', '.avi'))]\n",
    "    val_videos.sort()\n",
    "    if MAX_TRAIN_VIDS is not None:\n",
    "        train_videos = train_videos[:MAX_TRAIN_VIDS]\n",
    "    if MAX_VAL_VIDS is not None:\n",
    "        val_videos = val_videos[:MAX_VAL_VIDS]\n",
    "    print(f\"\\n[Info] Number of training videos: {len(train_videos)}\")\n",
    "    if RUN_VALIDATION:\n",
    "        print(f\"[Info] Number of validation videos: {len(val_videos)}\")\n",
    "    \n",
    "    # Initialize the PCA encoder, generator, and discriminator.\n",
    "    # The PCA encoder is initialized with the dummy PCA mean and basis.\n",
    "    pca_encoder = PCAEncoder(latent_dim=latent_dim, pca_mean=dummy_mean, pca_basis=pca_basis).to(device)\n",
    "    generator = Generator(latent_dim=latent_dim, feature_shape=feature_shape).to(device)\n",
    "    discriminator = Discriminator(input_channels=3).to(device)\n",
    "\n",
    "    # Create separate optimizers for (generator) and discriminator.\n",
    "    # The PCA encoder is not trainable.\n",
    "    optimizer_G = torch.optim.Adam(generator.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
    "    optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
    "    \n",
    "    train_losses_G = []\n",
    "    train_losses_D = []\n",
    "    val_losses_G = []\n",
    "    val_losses_D = []\n",
    "\n",
    "    for epoch in range(1, NUM_EPOCHS + 1):\n",
    "        start_time = time.time()\n",
    "        print(f\"\\n[Epoch {epoch}/{NUM_EPOCHS}] - Training Phase\")\n",
    "        epoch_train_loss_G = []\n",
    "        epoch_train_loss_D = []\n",
    "        for video_file in tqdm(train_videos, desc=\"Training Videos\", unit=\"video\"):\n",
    "            video_path = os.path.join(TRAIN_FOLDER, video_file)\n",
    "            loss_G, loss_D = train_on_video_sequence(video_path, pca_encoder, generator, discriminator,\n",
    "                                                     optimizer_G, optimizer_D, device, buffer_size=BUFFER_SIZE)\n",
    "            if loss_G is not None and loss_D is not None:\n",
    "                epoch_train_loss_G.append(loss_G)\n",
    "                epoch_train_loss_D.append(loss_D)\n",
    "        avg_train_loss_G = sum(epoch_train_loss_G) / len(epoch_train_loss_G) if epoch_train_loss_G else float('inf')\n",
    "        avg_train_loss_D = sum(epoch_train_loss_D) / len(epoch_train_loss_D) if epoch_train_loss_D else float('inf')\n",
    "        train_losses_G.append(avg_train_loss_G)\n",
    "        train_losses_D.append(avg_train_loss_D)\n",
    "        print(f\"[Epoch {epoch}] Average Generator Loss: {avg_train_loss_G:.6f} | Average Discriminator Loss: {avg_train_loss_D:.6f}\")\n",
    "\n",
    "        if RUN_VALIDATION:\n",
    "            print(f\"\\n[Epoch {epoch}/{NUM_EPOCHS}] - Validation Phase\")\n",
    "            epoch_val_loss_G = []\n",
    "            epoch_val_loss_D = []\n",
    "            for video_file in tqdm(val_videos, desc=\"Validation Videos\", unit=\"video\"):\n",
    "                video_path = os.path.join(VAL_FOLDER, video_file)\n",
    "                loss_G, loss_D = validate_on_video_sequence(video_path, pca_encoder, generator, discriminator,\n",
    "                                                            device, buffer_size=BUFFER_SIZE)\n",
    "                if loss_G is not None and loss_D is not None:\n",
    "                    epoch_val_loss_G.append(loss_G)\n",
    "                    epoch_val_loss_D.append(loss_D)\n",
    "            avg_val_loss_G = sum(epoch_val_loss_G) / len(epoch_val_loss_G) if epoch_val_loss_G else float('inf')\n",
    "            avg_val_loss_D = sum(epoch_val_loss_D) / len(epoch_val_loss_D) if epoch_val_loss_D else float('inf')\n",
    "            val_losses_G.append(avg_val_loss_G)\n",
    "            val_losses_D.append(avg_val_loss_D)\n",
    "            print(f\"[Epoch {epoch}] Average Validation Generator Loss: {avg_val_loss_G:.6f} | Discriminator Loss: {avg_val_loss_D:.6f}\")\n",
    "\n",
    "        # Save checkpoint.\n",
    "        ckpt_path = os.path.join(CHECKPOINT_DIR, f\"gan_epoch_{epoch}.pth\")\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'generator_state_dict': generator.state_dict(),\n",
    "            'discriminator_state_dict': discriminator.state_dict(),\n",
    "            'optimizer_G_state_dict': optimizer_G.state_dict(),\n",
    "            'optimizer_D_state_dict': optimizer_D.state_dict(),\n",
    "            'train_loss_G': avg_train_loss_G,\n",
    "            'train_loss_D': avg_train_loss_D\n",
    "        }, ckpt_path)\n",
    "        print(f\"[Info] Checkpoint saved at epoch {epoch}.\")\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"[Epoch {epoch}] Completed in {elapsed_time:.2f} seconds.\")\n",
    "\n",
    "    # Plot training (and validation) loss curves.\n",
    "    epochs = range(1, NUM_EPOCHS + 1)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, train_losses_G, label='Train Generator Loss', marker='o')\n",
    "    plt.plot(epochs, train_losses_D, label='Train Discriminator Loss', marker='o')\n",
    "    if RUN_VALIDATION:\n",
    "        plt.plot(epochs, val_losses_G, label='Val Generator Loss', marker='x')\n",
    "        plt.plot(epochs, val_losses_D, label='Val Discriminator Loss', marker='x')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('GAN Training Loss over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    print(\"[Info] Training process completed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_training()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python \n",
    "\"\"\"\n",
    "train_gan_from_embeddings_ffn.py\n",
    "\n",
    "This script trains a GAN that uses a hash-based encoder to convert video frames \n",
    "into a latent code. The encoder takes the image, computes a hash, and then uses \n",
    "that hash as a seed for a random number generator to produce a noise vector. \n",
    "The latent code is then fed into a Generator that upsamples it to reconstruct the image.\n",
    "A Discriminator (using logits loss) is used to distinguish real frames from generated ones.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ============================== Global Flags and Hyperparameters ==============================\n",
    "NUM_EPOCHS = 200\n",
    "LEARNING_RATE = 1e-4\n",
    "BUFFER_SIZE = 50                # Number of frames per chunk (batch size for training)\n",
    "latent_dim = 128                # Dimension of the latent code from the hash-based encoder\n",
    "lambda_recon = 1.0              # Weight for reconstruction (L1) loss\n",
    "RUN_VALIDATION = False          # Set to True if you want to run validation\n",
    "\n",
    "# ============================== Device Configuration ==============================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"[Info] Using device: {device}\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# ============================== Global Config & Paths ==============================\n",
    "TRAIN_FOLDER = \"bdd100k/videos/train\"  # Path to training videos\n",
    "VAL_FOLDER   = \"bdd100k/videos/val\"      # Path to validation videos\n",
    "CHECKPOINT_DIR = \"checkpoints_gan\"\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "MAX_TRAIN_VIDS = 1  # Limit number of training videos\n",
    "MAX_VAL_VIDS = 1    # Limit number of validation videos\n",
    "\n",
    "# ============================== Frame Size Configuration ==============================\n",
    "ORIG_HEIGHT, ORIG_WIDTH = 720, 1280\n",
    "FRAME_SCALE_INPUT = 0.10  \n",
    "FRAME_SCALE_OUTPUT = 0.10  \n",
    "SCALED_INPUT_HEIGHT = int(ORIG_HEIGHT * FRAME_SCALE_INPUT)\n",
    "SCALED_INPUT_WIDTH  = int(ORIG_WIDTH * FRAME_SCALE_INPUT)\n",
    "TARGET_SIZE = (SCALED_INPUT_HEIGHT, SCALED_INPUT_WIDTH)\n",
    "\n",
    "print(\"\\n===== FRAME INFO =====\")\n",
    "print(f\"Original Frame Size: {ORIG_HEIGHT} x {ORIG_WIDTH}\")\n",
    "print(f\"Target Frame Size: {TARGET_SIZE[0]} x {TARGET_SIZE[1]}\")\n",
    "\n",
    "# ============================== Data Preparation ==============================\n",
    "def process_frame(frame, target_size):\n",
    "    \"\"\"Resize the frame to the target size if necessary.\"\"\"\n",
    "    H, W = frame.shape[:2]\n",
    "    if (H, W) != target_size:\n",
    "        frame = cv2.resize(frame, (target_size[1], target_size[0]))\n",
    "    return frame\n",
    "\n",
    "def prepare_video_sequence(video_path):\n",
    "    \"\"\"\n",
    "    Loads all frames from the video and processes/resizes them.\n",
    "    Returns a list of torch tensors (with shape [3, H, W] and type float).\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        # Convert BGR to RGB\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frames.append(frame_rgb)\n",
    "    cap.release()\n",
    "    if len(frames) < 1:\n",
    "        return None\n",
    "    processed_frames = []\n",
    "    for frame in frames:\n",
    "        processed = process_frame(frame, TARGET_SIZE)\n",
    "        tensor_frame = (torch.from_numpy(np.ascontiguousarray(processed))\n",
    "                            .permute(2, 0, 1)\n",
    "                            .float()\n",
    "                            .pin_memory())\n",
    "        processed_frames.append(tensor_frame)\n",
    "    return processed_frames\n",
    "\n",
    "# ============================== Hash Encoder Definition ==============================\n",
    "# This encoder does not learn anything. Instead, it takes the input image,\n",
    "# computes a simple hash, and uses that hash to seed a random number generator,\n",
    "# which produces a noise vector as the latent code.\n",
    "class HashEncoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(HashEncoder, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, 3, H, W)\n",
    "        B = x.shape[0]\n",
    "        latent_list = []\n",
    "        # Process each sample in the batch individually.\n",
    "        for i in range(B):\n",
    "            # Compute a simple hash value from the image.\n",
    "            # (For example, multiply by 1000 and take the sum.)\n",
    "            # You can modify this to a more sophisticated hash if desired.\n",
    "            val = torch.sum(x[i] * 1000).item()\n",
    "            # Convert the value to an integer seed.\n",
    "            seed = int(val) % (2**32 - 1)\n",
    "            # Use the seed to initialize a NumPy RandomState (or Generator).\n",
    "            rng = np.random.RandomState(seed)\n",
    "            noise = rng.randn(self.latent_dim).astype(np.float32)\n",
    "            latent_list.append(noise)\n",
    "        # Stack into a tensor of shape (B, latent_dim)\n",
    "        latent = torch.tensor(np.stack(latent_list), device=x.device)\n",
    "        return latent\n",
    "\n",
    "# ============================== Generator Definition ==============================\n",
    "# Now the Generator receives a latent vector (of size latent_dim) and upsamples it.\n",
    "# We first map the latent vector into a spatial feature map via a fully connected layer.\n",
    "# For example, if we set feature_shape = (256, H/8, W/8).\n",
    "feature_shape = (256, SCALED_INPUT_HEIGHT // 8, SCALED_INPUT_WIDTH // 8)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, feature_shape):\n",
    "        super(Generator, self).__init__()\n",
    "        self.feature_shape = feature_shape\n",
    "        flatten_dim = feature_shape[0] * feature_shape[1] * feature_shape[2]\n",
    "        self.fc = nn.Linear(latent_dim, flatten_dim)\n",
    "        self.deconv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),  # Upsample\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),   # Upsample\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1),     # Upsample to image\n",
    "            nn.Sigmoid()  # Outputs in [0, 1]\n",
    "        )\n",
    "        \n",
    "    def forward(self, latent):\n",
    "        x = self.fc(latent)\n",
    "        x = x.view(latent.size(0), *self.feature_shape)\n",
    "        x = self.deconv(x)\n",
    "        return x\n",
    "\n",
    "# ============================== Discriminator Definition ==============================\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_channels=3):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.AdaptiveAvgPool2d((1,1))\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512, 1)  # No Sigmoid here!\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# ============================== GAN Training Function ==============================\n",
    "def train_on_video_sequence(video_path, hash_encoder, generator, discriminator,\n",
    "                            optimizer_G, optimizer_D, device, buffer_size=BUFFER_SIZE):\n",
    "    \"\"\"\n",
    "    For each chunk of frames from the video:\n",
    "      - The hash encoder deterministically produces a latent code from real frames.\n",
    "      - The generator produces fake images from these latent codes.\n",
    "      - The discriminator is updated to distinguish real from fake images.\n",
    "      - Since the hash encoder is fixed (non-trainable), only the generator and discriminator are updated.\n",
    "    \"\"\"\n",
    "    data = prepare_video_sequence(video_path)\n",
    "    if data is None:\n",
    "        print(f\"[Warning] Video {video_path} has too few frames. Skipping.\")\n",
    "        return None, None\n",
    "    seq_len = len(data)\n",
    "    # The hash encoder is fixed.\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    scaler = GradScaler()\n",
    "    total_loss_G = 0.0\n",
    "    total_loss_D = 0.0\n",
    "    count = 0\n",
    "\n",
    "    for start in range(0, seq_len, buffer_size):\n",
    "        end = min(start + buffer_size, seq_len)\n",
    "        frames = [frame.to(device, non_blocking=True) for frame in data[start:end]]\n",
    "        x_real = torch.stack(frames, dim=0) / 255.0  # shape: (B, 3, H, W)\n",
    "        batch_size = x_real.size(0)\n",
    "        count += batch_size\n",
    "\n",
    "        # ------------------------------\n",
    "        # Train the Discriminator\n",
    "        # ------------------------------\n",
    "        optimizer_D.zero_grad()\n",
    "        with autocast():\n",
    "            label_real = torch.ones(batch_size, 1, device=device)\n",
    "            output_real = discriminator(x_real)\n",
    "            loss_D_real = F.binary_cross_entropy_with_logits(output_real, label_real)\n",
    "            \n",
    "            latent = hash_encoder(x_real)  # shape: (B, latent_dim)\n",
    "            x_fake = generator(latent)\n",
    "            label_fake = torch.zeros(batch_size, 1, device=device)\n",
    "            output_fake = discriminator(x_fake.detach())\n",
    "            loss_D_fake = F.binary_cross_entropy_with_logits(output_fake, label_fake)\n",
    "            \n",
    "            loss_D = (loss_D_real + loss_D_fake) * 0.5\n",
    "\n",
    "        scaler.scale(loss_D).backward()\n",
    "        scaler.step(optimizer_D)\n",
    "\n",
    "        # ------------------------------\n",
    "        # Train the Generator (Generator Loss)\n",
    "        # ------------------------------\n",
    "        optimizer_G.zero_grad()\n",
    "        with autocast():\n",
    "            latent = hash_encoder(x_real)\n",
    "            x_fake = generator(latent)\n",
    "            output_fake_for_G = discriminator(x_fake)\n",
    "            loss_G_adv = F.binary_cross_entropy_with_logits(output_fake_for_G, label_real)\n",
    "            loss_recon = F.l1_loss(x_fake, x_real)\n",
    "            loss_G = loss_G_adv + lambda_recon * loss_recon\n",
    "\n",
    "        scaler.scale(loss_G).backward()\n",
    "        scaler.step(optimizer_G)\n",
    "        scaler.update()\n",
    "\n",
    "        total_loss_D += loss_D.item() * batch_size\n",
    "        total_loss_G += loss_G.item() * batch_size\n",
    "\n",
    "    avg_loss_D = total_loss_D / count if count > 0 else float('inf')\n",
    "    avg_loss_G = total_loss_G / count if count > 0 else float('inf')\n",
    "    return avg_loss_G, avg_loss_D\n",
    "\n",
    "# ============================== (Optional) Validation Function ==============================\n",
    "def validate_on_video_sequence(video_path, hash_encoder, generator, discriminator, device, buffer_size=BUFFER_SIZE):\n",
    "    data = prepare_video_sequence(video_path)\n",
    "    if data is None:\n",
    "        print(f\"[Warning] Video {video_path} has too few frames. Skipping.\")\n",
    "        return None, None\n",
    "    seq_len = len(data)\n",
    "    generator.eval()\n",
    "    discriminator.eval()\n",
    "    total_loss_D = 0.0\n",
    "    total_loss_G = 0.0\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for start in range(0, seq_len, buffer_size):\n",
    "            end = min(start + buffer_size, seq_len)\n",
    "            frames = [frame.to(device, non_blocking=True) for frame in data[start:end]]\n",
    "            x_real = torch.stack(frames, dim=0) / 255.0\n",
    "            batch_size = x_real.size(0)\n",
    "            count += batch_size\n",
    "\n",
    "            label_real = torch.ones(batch_size, 1, device=device)\n",
    "            output_real = discriminator(x_real)\n",
    "            loss_D_real = F.binary_cross_entropy_with_logits(output_real, label_real)\n",
    "\n",
    "            latent = hash_encoder(x_real)\n",
    "            x_fake = generator(latent)\n",
    "            label_fake = torch.zeros(batch_size, 1, device=device)\n",
    "            output_fake = discriminator(x_fake)\n",
    "            loss_D_fake = F.binary_cross_entropy_with_logits(output_fake, label_fake)\n",
    "            loss_D = (loss_D_real + loss_D_fake) * 0.5\n",
    "\n",
    "            output_fake_for_G = discriminator(x_fake)\n",
    "            loss_G_adv = F.binary_cross_entropy_with_logits(output_fake_for_G, label_real)\n",
    "            loss_recon = F.l1_loss(x_fake, x_real)\n",
    "            loss_G = loss_G_adv + lambda_recon * loss_recon\n",
    "\n",
    "            total_loss_D += loss_D.item() * batch_size\n",
    "            total_loss_G += loss_G.item() * batch_size\n",
    "\n",
    "    avg_loss_D = total_loss_D / count if count > 0 else float('inf')\n",
    "    avg_loss_G = total_loss_G / count if count > 0 else float('inf')\n",
    "    return avg_loss_G, avg_loss_D\n",
    "\n",
    "# ============================== Main Training Loop ==============================\n",
    "def main_training():\n",
    "    # List video files for training and (optionally) validation\n",
    "    train_videos = [f for f in os.listdir(TRAIN_FOLDER) if f.lower().endswith(('.mov', '.mp4', '.avi'))]\n",
    "    train_videos.sort()\n",
    "    val_videos = [f for f in os.listdir(VAL_FOLDER) if f.lower().endswith(('.mov', '.mp4', '.avi'))]\n",
    "    val_videos.sort()\n",
    "    if MAX_TRAIN_VIDS is not None:\n",
    "        train_videos = train_videos[:MAX_TRAIN_VIDS]\n",
    "    if MAX_VAL_VIDS is not None:\n",
    "        val_videos = val_videos[:MAX_VAL_VIDS]\n",
    "    print(f\"\\n[Info] Number of training videos: {len(train_videos)}\")\n",
    "    if RUN_VALIDATION:\n",
    "        print(f\"[Info] Number of validation videos: {len(val_videos)}\")\n",
    "    \n",
    "    # Initialize the hash encoder, generator, and discriminator.\n",
    "    hash_encoder = HashEncoder(latent_dim=latent_dim).to(device)\n",
    "    generator = Generator(latent_dim=latent_dim, feature_shape=feature_shape).to(device)\n",
    "    discriminator = Discriminator(input_channels=3).to(device)\n",
    "\n",
    "    # Create separate optimizers for generator and discriminator.\n",
    "    # The hash encoder is non-trainable.\n",
    "    optimizer_G = torch.optim.Adam(generator.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
    "    optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
    "    \n",
    "    train_losses_G = []\n",
    "    train_losses_D = []\n",
    "    val_losses_G = []\n",
    "    val_losses_D = []\n",
    "\n",
    "    for epoch in range(1, NUM_EPOCHS + 1):\n",
    "        start_time = time.time()\n",
    "        print(f\"\\n[Epoch {epoch}/{NUM_EPOCHS}] - Training Phase\")\n",
    "        epoch_train_loss_G = []\n",
    "        epoch_train_loss_D = []\n",
    "        for video_file in tqdm(train_videos, desc=\"Training Videos\", unit=\"video\"):\n",
    "            video_path = os.path.join(TRAIN_FOLDER, video_file)\n",
    "            loss_G, loss_D = train_on_video_sequence(video_path, hash_encoder, generator, discriminator,\n",
    "                                                     optimizer_G, optimizer_D, device, buffer_size=BUFFER_SIZE)\n",
    "            if loss_G is not None and loss_D is not None:\n",
    "                epoch_train_loss_G.append(loss_G)\n",
    "                epoch_train_loss_D.append(loss_D)\n",
    "        avg_train_loss_G = sum(epoch_train_loss_G) / len(epoch_train_loss_G) if epoch_train_loss_G else float('inf')\n",
    "        avg_train_loss_D = sum(epoch_train_loss_D) / len(epoch_train_loss_D) if epoch_train_loss_D else float('inf')\n",
    "        train_losses_G.append(avg_train_loss_G)\n",
    "        train_losses_D.append(avg_train_loss_D)\n",
    "        print(f\"[Epoch {epoch}] Average Generator Loss: {avg_train_loss_G:.6f} | Average Discriminator Loss: {avg_train_loss_D:.6f}\")\n",
    "\n",
    "        if RUN_VALIDATION:\n",
    "            print(f\"\\n[Epoch {epoch}/{NUM_EPOCHS}] - Validation Phase\")\n",
    "            epoch_val_loss_G = []\n",
    "            epoch_val_loss_D = []\n",
    "            for video_file in tqdm(val_videos, desc=\"Validation Videos\", unit=\"video\"):\n",
    "                video_path = os.path.join(VAL_FOLDER, video_file)\n",
    "                loss_G, loss_D = validate_on_video_sequence(video_path, hash_encoder, generator, discriminator,\n",
    "                                                            device, buffer_size=BUFFER_SIZE)\n",
    "                if loss_G is not None and loss_D is not None:\n",
    "                    epoch_val_loss_G.append(loss_G)\n",
    "                    epoch_val_loss_D.append(loss_D)\n",
    "            avg_val_loss_G = sum(epoch_val_loss_G) / len(epoch_val_loss_G) if epoch_val_loss_G else float('inf')\n",
    "            avg_val_loss_D = sum(epoch_val_loss_D) / len(epoch_val_loss_D) if epoch_val_loss_D else float('inf')\n",
    "            val_losses_G.append(avg_val_loss_G)\n",
    "            val_losses_D.append(avg_val_loss_D)\n",
    "            print(f\"[Epoch {epoch}] Average Validation Generator Loss: {avg_val_loss_G:.6f} | Discriminator Loss: {avg_val_loss_D:.6f}\")\n",
    "\n",
    "        # Save checkpoint.\n",
    "        ckpt_path = os.path.join(CHECKPOINT_DIR, f\"gan_epoch_{epoch}.pth\")\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'generator_state_dict': generator.state_dict(),\n",
    "            'discriminator_state_dict': discriminator.state_dict(),\n",
    "            'optimizer_G_state_dict': optimizer_G.state_dict(),\n",
    "            'optimizer_D_state_dict': optimizer_D.state_dict(),\n",
    "            'train_loss_G': avg_train_loss_G,\n",
    "            'train_loss_D': avg_train_loss_D\n",
    "        }, ckpt_path)\n",
    "        print(f\"[Info] Checkpoint saved at epoch {epoch}.\")\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"[Epoch {epoch}] Completed in {elapsed_time:.2f} seconds.\")\n",
    "\n",
    "    # Plot training (and validation) loss curves.\n",
    "    epochs = range(1, NUM_EPOCHS + 1)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, train_losses_G, label='Train Generator Loss', marker='o')\n",
    "    plt.plot(epochs, train_losses_D, label='Train Discriminator Loss', marker='o')\n",
    "    if RUN_VALIDATION:\n",
    "        plt.plot(epochs, val_losses_G, label='Val Generator Loss', marker='x')\n",
    "        plt.plot(epochs, val_losses_D, label='Val Discriminator Loss', marker='x')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('GAN Training Loss over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    print(\"[Info] Training process completed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_training()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Image Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "train_transformer_from_embeddings.py\n",
    "\n",
    "This script uses a PCA-based encoder to project video frames into a latent code.\n",
    "A Transformer-based generator upsamples the latent code into an image.\n",
    "Training is driven by a reconstruction loss (L1) between the generated and original images.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ============================== Global Flags and Hyperparameters ==============================\n",
    "NUM_EPOCHS = 200\n",
    "LEARNING_RATE = 1e-4\n",
    "BUFFER_SIZE = 50                # Number of frames per chunk (batch size for training)\n",
    "latent_dim = 128                # Dimension of the latent code produced by PCA\n",
    "lambda_recon = 1.0              # Weight for reconstruction (L1) loss\n",
    "\n",
    "# Transformer-specific hyperparameters\n",
    "PATCH_SIZE = 4                  # Size of each image patch (assumes square patches)\n",
    "d_model = 256                   # Transformer embedding dimension\n",
    "nhead = 8                       # Number of attention heads\n",
    "num_layers = 6                  # Number of transformer encoder layers\n",
    "dropout = 0.1                   # Dropout probability\n",
    "\n",
    "# ============================== Device Configuration ==============================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"[Info] Using device: {device}\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# ============================== Global Config & Paths ==============================\n",
    "TRAIN_FOLDER = \"bdd100k/videos/train\"  # Path to training videos\n",
    "VAL_FOLDER   = \"bdd100k/videos/val\"      # Path to validation videos\n",
    "CHECKPOINT_DIR = \"checkpoints_transformer\"\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "MAX_TRAIN_VIDS = 2  # Limit number of training videos\n",
    "MAX_VAL_VIDS = 2    # Limit number of validation videos\n",
    "\n",
    "# ============================== Frame Size Configuration ==============================\n",
    "ORIG_HEIGHT, ORIG_WIDTH = 720, 1280\n",
    "FRAME_SCALE_INPUT = 0.50  \n",
    "FRAME_SCALE_OUTPUT = 1  \n",
    "SCALED_INPUT_HEIGHT = int(ORIG_HEIGHT * FRAME_SCALE_INPUT)\n",
    "SCALED_INPUT_WIDTH  = int(ORIG_WIDTH * FRAME_SCALE_INPUT)\n",
    "TARGET_SIZE = (SCALED_INPUT_HEIGHT, SCALED_INPUT_WIDTH)\n",
    "\n",
    "print(\"\\n===== FRAME INFO =====\")\n",
    "print(f\"Original Frame Size: {ORIG_HEIGHT} x {ORIG_WIDTH}\")\n",
    "print(f\"Target Frame Size: {TARGET_SIZE[0]} x {TARGET_SIZE[1]}\")\n",
    "\n",
    "# ============================== Data Preparation ==============================\n",
    "def process_frame(frame, target_size):\n",
    "    \"\"\"Resize the frame to the target size if necessary.\"\"\"\n",
    "    H, W = frame.shape[:2]\n",
    "    if (H, W) != target_size:\n",
    "        frame = cv2.resize(frame, (target_size[1], target_size[0]))\n",
    "    return frame\n",
    "\n",
    "def prepare_video_sequence(video_path):\n",
    "    \"\"\"\n",
    "    Loads all frames from the video and processes/resizes them.\n",
    "    Returns a list of torch tensors (with shape [3, H, W] and type float).\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        # Convert BGR to RGB\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frames.append(frame_rgb)\n",
    "    cap.release()\n",
    "    if len(frames) < 1:\n",
    "        return None\n",
    "    processed_frames = []\n",
    "    for frame in frames:\n",
    "        processed = process_frame(frame, TARGET_SIZE)\n",
    "        tensor_frame = (torch.from_numpy(np.ascontiguousarray(processed))\n",
    "                            .permute(2, 0, 1)\n",
    "                            .float()\n",
    "                            .pin_memory())\n",
    "        processed_frames.append(tensor_frame)\n",
    "    return processed_frames\n",
    "\n",
    "# ============================== PCA Encoder Definition ==============================\n",
    "# This encoder projects the image into a lower-dimensional latent code via PCA.\n",
    "class PCAEncoder(nn.Module):\n",
    "    def __init__(self, latent_dim, pca_mean, pca_basis):\n",
    "        \"\"\"\n",
    "        pca_mean: Tensor of shape (D,) where D = 3*H*W\n",
    "        pca_basis: Tensor of shape (D, latent_dim), assumed to be orthonormal.\n",
    "        \"\"\"\n",
    "        super(PCAEncoder, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.register_buffer('pca_mean', pca_mean)\n",
    "        self.register_buffer('pca_basis', pca_basis)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (B, 3, H, W)\n",
    "        B = x.shape[0]\n",
    "        x_flat = x.view(B, -1)  # shape: (B, D)\n",
    "        x_centered = x_flat - self.pca_mean  # subtract mean\n",
    "        latent = x_centered @ self.pca_basis   # shape: (B, latent_dim)\n",
    "        return latent\n",
    "\n",
    "# For demonstration purposes, create a dummy PCA mean and basis.\n",
    "D = 3 * SCALED_INPUT_HEIGHT * SCALED_INPUT_WIDTH  # dimensionality of flattened image\n",
    "dummy_mean = torch.zeros(D)  # In practice, use your computed mean.\n",
    "dummy_basis = torch.randn(D, latent_dim)\n",
    "q, _ = torch.qr(dummy_basis)  # Orthonormalize\n",
    "pca_basis = q\n",
    "\n",
    "# ============================== Transformer Generator Definition ==============================\n",
    "# This module takes the latent code and maps it into a sequence of patch embeddings.\n",
    "# The transformer then processes the token sequence and the tokens are reassembled into an image.\n",
    "class TransformerGenerator(nn.Module):\n",
    "    def __init__(self, latent_dim, image_size, patch_size, d_model=256, nhead=8, num_layers=6, dropout=0.1):\n",
    "        \"\"\"\n",
    "        image_size: Tuple (H, W) for the output image.\n",
    "        patch_size: Size of square patches to divide the image into.\n",
    "        \"\"\"\n",
    "        super(TransformerGenerator, self).__init__()\n",
    "        self.image_size = image_size  # (H, W)\n",
    "        self.patch_size = patch_size\n",
    "        self.grid_size = (image_size[0] // patch_size, image_size[1] // patch_size)\n",
    "        self.num_tokens = self.grid_size[0] * self.grid_size[1]\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # Map the latent vector to a sequence of tokens\n",
    "        self.linear_proj = nn.Linear(latent_dim, self.num_tokens * d_model)\n",
    "        # Learned positional embeddings for the tokens\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, self.num_tokens, d_model))\n",
    "        \n",
    "        # Transformer encoder layers\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dropout=dropout)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Map each token to a patch (flattened patch pixels)\n",
    "        self.token_to_patch = nn.Linear(d_model, patch_size * patch_size * 3)\n",
    "        \n",
    "    def forward(self, latent):\n",
    "        # latent shape: (B, latent_dim)\n",
    "        B = latent.size(0)\n",
    "        # Map latent to token embeddings: shape (B, num_tokens, d_model)\n",
    "        tokens = self.linear_proj(latent).view(B, self.num_tokens, self.d_model)\n",
    "        tokens = tokens + self.pos_embedding  # Add positional information\n",
    "        \n",
    "        # Transformer expects shape (sequence_length, batch, d_model)\n",
    "        tokens = tokens.transpose(0, 1)\n",
    "        tokens = self.transformer(tokens)\n",
    "        tokens = tokens.transpose(0, 1)  # Back to (B, num_tokens, d_model)\n",
    "        \n",
    "        # Map tokens to patches\n",
    "        patches = self.token_to_patch(tokens)  # (B, num_tokens, patch_size*patch_size*3)\n",
    "        \n",
    "        # Reassemble patches into full image:\n",
    "        grid_h, grid_w = self.grid_size  # e.g., (H/patch_size, W/patch_size)\n",
    "        patches = patches.view(B, grid_h, grid_w, self.patch_size, self.patch_size, 3)\n",
    "        # Rearrange axes to form image: (B, 3, H, W)\n",
    "        image = patches.permute(0, 5, 1, 3, 2, 4).contiguous()\n",
    "        image = image.view(B, 3, grid_h * self.patch_size, grid_w * self.patch_size)\n",
    "        # Use sigmoid to produce outputs in [0, 1]\n",
    "        return torch.sigmoid(image)\n",
    "\n",
    "# ============================== Training Function ==============================\n",
    "def train_on_video_sequence(video_path, pca_encoder, transformer_gen,\n",
    "                            optimizer, device, buffer_size=BUFFER_SIZE):\n",
    "    \"\"\"\n",
    "    For each chunk of frames from the video:\n",
    "      - Use the PCA encoder to get latent representations.\n",
    "      - The transformer upsamples the latent code to reconstruct the image.\n",
    "      - Update the transformer based solely on a reconstruction (L1) loss.\n",
    "    \"\"\"\n",
    "    data = prepare_video_sequence(video_path)\n",
    "    if data is None:\n",
    "        print(f\"[Warning] Video {video_path} has too few frames. Skipping.\")\n",
    "        return None\n",
    "    seq_len = len(data)\n",
    "    transformer_gen.train()\n",
    "    scaler = GradScaler()\n",
    "    total_loss = 0.0\n",
    "    count = 0\n",
    "\n",
    "    for start in range(0, seq_len, buffer_size):\n",
    "        end = min(start + buffer_size, seq_len)\n",
    "        frames = [frame.to(device, non_blocking=True) for frame in data[start:end]]\n",
    "        x_real = torch.stack(frames, dim=0) / 255.0  # (B, 3, H, W)\n",
    "        batch_size = x_real.size(0)\n",
    "        count += batch_size\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with autocast():\n",
    "            # Get latent representation via PCA encoder (fixed)\n",
    "            latent = pca_encoder(x_real)\n",
    "            # Generate (reconstructed) image via the transformer generator\n",
    "            x_rec = transformer_gen(latent)\n",
    "            # Reconstruction loss (L1)\n",
    "            loss = F.l1_loss(x_rec, x_real)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_loss += loss.item() * batch_size\n",
    "\n",
    "    avg_loss = total_loss / count if count > 0 else float('inf')\n",
    "    return avg_loss\n",
    "\n",
    "# ============================== (Optional) Validation Function ==============================\n",
    "def validate_on_video_sequence(video_path, pca_encoder, transformer_gen, device, buffer_size=BUFFER_SIZE):\n",
    "    data = prepare_video_sequence(video_path)\n",
    "    if data is None:\n",
    "        print(f\"[Warning] Video {video_path} has too few frames. Skipping.\")\n",
    "        return None\n",
    "    seq_len = len(data)\n",
    "    transformer_gen.eval()\n",
    "    total_loss = 0.0\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for start in range(0, seq_len, buffer_size):\n",
    "            end = min(start + buffer_size, seq_len)\n",
    "            frames = [frame.to(device, non_blocking=True) for frame in data[start:end]]\n",
    "            x_real = torch.stack(frames, dim=0) / 255.0\n",
    "            batch_size = x_real.size(0)\n",
    "            count += batch_size\n",
    "\n",
    "            latent = pca_encoder(x_real)\n",
    "            x_rec = transformer_gen(latent)\n",
    "            loss = F.l1_loss(x_rec, x_real)\n",
    "            total_loss += loss.item() * batch_size\n",
    "\n",
    "    avg_loss = total_loss / count if count > 0 else float('inf')\n",
    "    return avg_loss\n",
    "\n",
    "# ============================== Main Training Loop ==============================\n",
    "def main_training():\n",
    "    # List video files for training and (optionally) validation\n",
    "    train_videos = [f for f in os.listdir(TRAIN_FOLDER) if f.lower().endswith(('.mov', '.mp4', '.avi'))]\n",
    "    train_videos.sort()\n",
    "    val_videos = [f for f in os.listdir(VAL_FOLDER) if f.lower().endswith(('.mov', '.mp4', '.avi'))]\n",
    "    val_videos.sort()\n",
    "    if MAX_TRAIN_VIDS is not None:\n",
    "        train_videos = train_videos[:MAX_TRAIN_VIDS]\n",
    "    if MAX_VAL_VIDS is not None:\n",
    "        val_videos = val_videos[:MAX_VAL_VIDS]\n",
    "    print(f\"\\n[Info] Number of training videos: {len(train_videos)}\")\n",
    "    print(f\"[Info] Number of validation videos: {len(val_videos)}\")\n",
    "    \n",
    "    # Initialize the PCA encoder (fixed) and the Transformer-based generator.\n",
    "    pca_encoder = PCAEncoder(latent_dim=latent_dim, pca_mean=dummy_mean, pca_basis=pca_basis).to(device)\n",
    "    transformer_gen = TransformerGenerator(\n",
    "        latent_dim=latent_dim,\n",
    "        image_size=TARGET_SIZE,\n",
    "        patch_size=PATCH_SIZE,\n",
    "        d_model=d_model,\n",
    "        nhead=nhead,\n",
    "        num_layers=num_layers,\n",
    "        dropout=dropout\n",
    "    ).to(device)\n",
    "\n",
    "    # Only the transformer generator is trainable.\n",
    "    optimizer = torch.optim.Adam(transformer_gen.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(1, NUM_EPOCHS + 1):\n",
    "        start_time = time.time()\n",
    "        print(f\"\\n[Epoch {epoch}/{NUM_EPOCHS}] - Training Phase\")\n",
    "        epoch_train_losses = []\n",
    "        for video_file in tqdm(train_videos, desc=\"Training Videos\", unit=\"video\"):\n",
    "            video_path = os.path.join(TRAIN_FOLDER, video_file)\n",
    "            loss = train_on_video_sequence(video_path, pca_encoder, transformer_gen,\n",
    "                                           optimizer, device, buffer_size=BUFFER_SIZE)\n",
    "            if loss is not None:\n",
    "                epoch_train_losses.append(loss)\n",
    "        avg_train_loss = sum(epoch_train_losses) / len(epoch_train_losses) if epoch_train_losses else float('inf')\n",
    "        train_losses.append(avg_train_loss)\n",
    "        print(f\"[Epoch {epoch}] Average Train Loss: {avg_train_loss:.6f}\")\n",
    "\n",
    "        # Validation phase\n",
    "        epoch_val_losses = []\n",
    "        for video_file in tqdm(val_videos, desc=\"Validation Videos\", unit=\"video\"):\n",
    "            video_path = os.path.join(VAL_FOLDER, video_file)\n",
    "            loss = validate_on_video_sequence(video_path, pca_encoder, transformer_gen, device, buffer_size=BUFFER_SIZE)\n",
    "            if loss is not None:\n",
    "                epoch_val_losses.append(loss)\n",
    "        avg_val_loss = sum(epoch_val_losses) / len(epoch_val_losses) if epoch_val_losses else float('inf')\n",
    "        val_losses.append(avg_val_loss)\n",
    "        print(f\"[Epoch {epoch}] Average Validation Loss: {avg_val_loss:.6f}\")\n",
    "\n",
    "        # Save checkpoint.\n",
    "        ckpt_path = os.path.join(CHECKPOINT_DIR, f\"transformer_epoch_{epoch}.pth\")\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'transformer_gen_state_dict': transformer_gen.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_loss': avg_train_loss,\n",
    "            'val_loss': avg_val_loss\n",
    "        }, ckpt_path)\n",
    "        print(f\"[Info] Checkpoint saved at epoch {epoch}.\")\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"[Epoch {epoch}] Completed in {elapsed_time:.2f} seconds.\")\n",
    "\n",
    "    # Plot training and validation loss curves.\n",
    "    epochs = range(1, NUM_EPOCHS + 1)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, train_losses, label='Train Loss', marker='o')\n",
    "    plt.plot(epochs, val_losses, label='Val Loss', marker='x')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('L1 Reconstruction Loss')\n",
    "    plt.title('Transformer Training Loss over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    print(\"[Info] Training process completed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_training()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Informer + Frame Gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Using device: cuda\n",
      "\n",
      "===== FRAME INFO =====\n",
      "Original Frame Size: 720 x 1280\n",
      "Target Frame Size: 72 x 128\n",
      "\n",
      "[Info] Number of training videos: 2\n",
      "[Info] Number of validation videos: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\keplarV4\\AppData\\Local\\Temp\\ipykernel_21992\\2232168893.py:121: UserWarning: torch.qr is deprecated in favor of torch.linalg.qr and will be removed in a future PyTorch release.\n",
      "The boolean parameter 'some' has been replaced with a string parameter 'mode'.\n",
      "Q, R = torch.qr(A, some)\n",
      "should be replaced with\n",
      "Q, R = torch.linalg.qr(A, 'reduced' if some else 'complete') (Triggered internally at ..\\aten\\src\\ATen\\native\\BatchLinearAlgebra.cpp:2422.)\n",
      "  q, _ = torch.qr(dummy_basis)  # Orthonormalize\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 1/200] - Training Phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Videos: 100%|██████████| 2/2 [00:27<00:00, 13.61s/video]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Average Train Loss: 0.089938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Videos: 100%|██████████| 2/2 [00:10<00:00,  5.47s/video]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Average Validation Loss: 0.200497\n",
      "[Info] Checkpoint saved at epoch 1.\n",
      "[Epoch 1] Completed in 38.72 seconds.\n",
      "\n",
      "[Epoch 2/200] - Training Phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Videos: 100%|██████████| 2/2 [00:25<00:00, 12.80s/video]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Average Train Loss: 0.077753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Videos: 100%|██████████| 2/2 [00:11<00:00,  5.55s/video]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Average Validation Loss: 0.192978\n",
      "[Info] Checkpoint saved at epoch 2.\n",
      "[Epoch 2] Completed in 37.23 seconds.\n",
      "\n",
      "[Epoch 3/200] - Training Phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Videos: 100%|██████████| 2/2 [00:25<00:00, 12.85s/video]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Average Train Loss: 0.071903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Videos: 100%|██████████| 2/2 [00:11<00:00,  5.58s/video]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Average Validation Loss: 0.191970\n",
      "[Info] Checkpoint saved at epoch 3.\n",
      "[Epoch 3] Completed in 37.38 seconds.\n",
      "\n",
      "[Epoch 4/200] - Training Phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Videos: 100%|██████████| 2/2 [00:25<00:00, 12.53s/video]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Average Train Loss: 0.067114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Videos: 100%|██████████| 2/2 [00:10<00:00,  5.49s/video]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Average Validation Loss: 0.170891\n",
      "[Info] Checkpoint saved at epoch 4.\n",
      "[Epoch 4] Completed in 36.57 seconds.\n",
      "\n",
      "[Epoch 5/200] - Training Phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Videos: 100%|██████████| 2/2 [00:25<00:00, 12.64s/video]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Average Train Loss: 0.063435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Videos: 100%|██████████| 2/2 [00:11<00:00,  5.51s/video]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Average Validation Loss: 0.156714\n",
      "[Info] Checkpoint saved at epoch 5.\n",
      "[Epoch 5] Completed in 36.81 seconds.\n",
      "\n",
      "[Epoch 6/200] - Training Phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Videos: 100%|██████████| 2/2 [00:25<00:00, 12.66s/video]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] Average Train Loss: 0.060605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Videos: 100%|██████████| 2/2 [00:11<00:00,  5.56s/video]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] Average Validation Loss: 0.142262\n",
      "[Info] Checkpoint saved at epoch 6.\n",
      "[Epoch 6] Completed in 36.96 seconds.\n",
      "\n",
      "[Epoch 7/200] - Training Phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Videos: 100%|██████████| 2/2 [00:25<00:00, 12.74s/video]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7] Average Train Loss: 0.058274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Videos: 100%|██████████| 2/2 [00:11<00:00,  5.78s/video]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7] Average Validation Loss: 0.141806\n",
      "[Info] Checkpoint saved at epoch 7.\n",
      "[Epoch 7] Completed in 37.58 seconds.\n",
      "\n",
      "[Epoch 8/200] - Training Phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Videos: 100%|██████████| 2/2 [00:25<00:00, 12.95s/video]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8] Average Train Loss: 0.056407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Videos: 100%|██████████| 2/2 [00:11<00:00,  5.80s/video]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8] Average Validation Loss: 0.142115\n",
      "[Info] Checkpoint saved at epoch 8.\n",
      "[Epoch 8] Completed in 38.01 seconds.\n",
      "\n",
      "[Epoch 9/200] - Training Phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Videos:   0%|          | 0/2 [00:00<?, ?video/s]\n",
      "Training Videos:   0%|          | 0/2 [00:08<?, ?video/s]133.58frame/s, time/img (s)=0.0078]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 359\u001b[0m\n\u001b[0;32m    356\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Info] Training process completed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 359\u001b[0m     \u001b[43mmain_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 313\u001b[0m, in \u001b[0;36mmain_training\u001b[1;34m()\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m video_file \u001b[38;5;129;01min\u001b[39;00m tqdm(train_videos, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Videos\u001b[39m\u001b[38;5;124m\"\u001b[39m, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvideo\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    312\u001b[0m     video_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(TRAIN_FOLDER, video_file)\n\u001b[1;32m--> 313\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_on_video_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpca_encoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minformer_gen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBUFFER_SIZE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    315\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    316\u001b[0m         epoch_train_losses\u001b[38;5;241m.\u001b[39mappend(loss)\n",
      "Cell \u001b[1;32mIn[1], line 243\u001b[0m, in \u001b[0;36mtrain_on_video_sequence\u001b[1;34m(video_path, pca_encoder, informer_gen, optimizer, device, buffer_size)\u001b[0m\n\u001b[0;32m    241\u001b[0m         batch_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m batch_start_time\n\u001b[0;32m    242\u001b[0m         time_per_image \u001b[38;5;241m=\u001b[39m batch_time \u001b[38;5;241m/\u001b[39m batch_size\n\u001b[1;32m--> 243\u001b[0m         \u001b[43mframe_pbar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_postfix\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtime/img (s)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtime_per_image\u001b[49m\u001b[38;5;132;43;01m:\u001b[39;49;00m\u001b[38;5;124;43m.4f\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    244\u001b[0m         frame_pbar\u001b[38;5;241m.\u001b[39mupdate(batch_size)\n\u001b[0;32m    245\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m total_loss \u001b[38;5;241m/\u001b[39m count \u001b[38;5;28;01mif\u001b[39;00m count \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\keplarV4\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\std.py:1431\u001b[0m, in \u001b[0;36mtqdm.set_postfix\u001b[1;34m(self, ordered_dict, refresh, **kwargs)\u001b[0m\n\u001b[0;32m   1428\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostfix \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(key \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m postfix[key]\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m   1429\u001b[0m                          \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m postfix\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m   1430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m refresh:\n\u001b[1;32m-> 1431\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrefresh\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\keplarV4\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\std.py:1347\u001b[0m, in \u001b[0;36mtqdm.refresh\u001b[1;34m(self, nolock, lock_args)\u001b[0m\n\u001b[0;32m   1345\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1346\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39macquire()\n\u001b[1;32m-> 1347\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m nolock:\n\u001b[0;32m   1349\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\keplarV4\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\std.py:1495\u001b[0m, in \u001b[0;36mtqdm.display\u001b[1;34m(self, msg, pos)\u001b[0m\n\u001b[0;32m   1493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pos:\n\u001b[0;32m   1494\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmoveto(pos)\n\u001b[1;32m-> 1495\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__str__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pos:\n\u001b[0;32m   1497\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmoveto(\u001b[38;5;241m-\u001b[39mpos)\n",
      "File \u001b[1;32mc:\\Users\\keplarV4\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\std.py:459\u001b[0m, in \u001b[0;36mtqdm.status_printer.<locals>.print_status\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprint_status\u001b[39m(s):\n\u001b[0;32m    458\u001b[0m     len_s \u001b[38;5;241m=\u001b[39m disp_len(s)\n\u001b[1;32m--> 459\u001b[0m     \u001b[43mfp_write\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\r\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlast_len\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlen_s\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    460\u001b[0m     last_len[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m len_s\n",
      "File \u001b[1;32mc:\\Users\\keplarV4\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\std.py:453\u001b[0m, in \u001b[0;36mtqdm.status_printer.<locals>.fp_write\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfp_write\u001b[39m(s):\n\u001b[0;32m    452\u001b[0m     fp\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;28mstr\u001b[39m(s))\n\u001b[1;32m--> 453\u001b[0m     \u001b[43mfp_flush\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\keplarV4\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\utils.py:196\u001b[0m, in \u001b[0;36mDisableOnWriteError.disable_on_exception.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 196\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    198\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39merrno \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m5\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\iostream.py:497\u001b[0m, in \u001b[0;36mOutStream.flush\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    495\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpub_thread\u001b[38;5;241m.\u001b[39mschedule(evt\u001b[38;5;241m.\u001b[39mset)\n\u001b[0;32m    496\u001b[0m     \u001b[38;5;66;03m# and give a timeout to avoid\u001b[39;00m\n\u001b[1;32m--> 497\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mevt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflush_timeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    498\u001b[0m         \u001b[38;5;66;03m# write directly to __stderr__ instead of warning because\u001b[39;00m\n\u001b[0;32m    499\u001b[0m         \u001b[38;5;66;03m# if this is happening sys.stderr may be the problem.\u001b[39;00m\n\u001b[0;32m    500\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIOStream.flush timed out\u001b[39m\u001b[38;5;124m\"\u001b[39m, file\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39m__stderr__)\n\u001b[0;32m    501\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\keplarV4\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\threading.py:622\u001b[0m, in \u001b[0;36mEvent.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    620\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[0;32m    621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[1;32m--> 622\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    623\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[1;32mc:\\Users\\keplarV4\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 324\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    326\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "train_informer_from_embeddings.py\n",
    "\n",
    "This script uses a PCA-based encoder to project video frames into a latent code.\n",
    "An Informer-based generator upsamples the latent code into an image.\n",
    "Training is driven by a reconstruction loss (L1) between the generated and original images.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ============================== Global Flags and Hyperparameters ==============================\n",
    "NUM_EPOCHS = 200\n",
    "LEARNING_RATE = 1e-4\n",
    "BUFFER_SIZE = 4                # Number of frames per chunk (batch size for training)\n",
    "latent_dim = 256               # Dimension of the latent code produced by PCA\n",
    "lambda_recon = 1.0             # Weight for reconstruction (L1) loss\n",
    "\n",
    "# Informer-specific hyperparameters\n",
    "PATCH_SIZE = 4                 # Size of each image patch (assumes square patches)\n",
    "d_model = 256                  # Embedding dimension\n",
    "nhead = 8                      # Number of attention heads\n",
    "num_layers = 6                 # Number of Informer encoder layers\n",
    "dropout = 0.1                  # Dropout probability\n",
    "\n",
    "# ============================== Device Configuration ==============================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"[Info] Using device: {device}\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# ============================== Global Config & Paths ==============================\n",
    "TRAIN_FOLDER = \"bdd100k/videos/train\"  # Path to training videos\n",
    "VAL_FOLDER   = \"bdd100k/videos/val\"      # Path to validation videos\n",
    "CHECKPOINT_DIR = \"checkpoints_informer\"\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "MAX_TRAIN_VIDS = 2  # Limit number of training videos\n",
    "MAX_VAL_VIDS = 2    # Limit number of validation videos\n",
    "\n",
    "# ============================== Frame Size Configuration ==============================\n",
    "ORIG_HEIGHT, ORIG_WIDTH = 720, 1280\n",
    "FRAME_SCALE_INPUT = 0.10  \n",
    "FRAME_SCALE_OUTPUT = 0.25  \n",
    "SCALED_INPUT_HEIGHT = int(ORIG_HEIGHT * FRAME_SCALE_INPUT)\n",
    "SCALED_INPUT_WIDTH  = int(ORIG_WIDTH * FRAME_SCALE_INPUT)\n",
    "TARGET_SIZE = (SCALED_INPUT_HEIGHT, SCALED_INPUT_WIDTH)\n",
    "\n",
    "print(\"\\n===== FRAME INFO =====\")\n",
    "print(f\"Original Frame Size: {ORIG_HEIGHT} x {ORIG_WIDTH}\")\n",
    "print(f\"Target Frame Size: {TARGET_SIZE[0]} x {TARGET_SIZE[1]}\")\n",
    "\n",
    "# ============================== Data Preparation ==============================\n",
    "def process_frame(frame, target_size):\n",
    "    \"\"\"Resize the frame to the target size if necessary.\"\"\"\n",
    "    H, W = frame.shape[:2]\n",
    "    if (H, W) != target_size:\n",
    "        frame = cv2.resize(frame, (target_size[1], target_size[0]))\n",
    "    return frame\n",
    "\n",
    "def prepare_video_sequence(video_path):\n",
    "    \"\"\"\n",
    "    Loads all frames from the video and processes/resizes them.\n",
    "    Returns a list of torch tensors (with shape [3, H, W] and type float).\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        # Convert BGR to RGB\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frames.append(frame_rgb)\n",
    "    cap.release()\n",
    "    if len(frames) < 1:\n",
    "        return None\n",
    "    processed_frames = []\n",
    "    for frame in frames:\n",
    "        processed = process_frame(frame, TARGET_SIZE)\n",
    "        tensor_frame = (torch.from_numpy(np.ascontiguousarray(processed))\n",
    "                            .permute(2, 0, 1)\n",
    "                            .float()\n",
    "                            .pin_memory())\n",
    "        processed_frames.append(tensor_frame)\n",
    "    return processed_frames\n",
    "\n",
    "# ============================== PCA Encoder Definition ==============================\n",
    "class PCAEncoder(nn.Module):\n",
    "    def __init__(self, latent_dim, pca_mean, pca_basis):\n",
    "        \"\"\"\n",
    "        pca_mean: Tensor of shape (D,) where D = 3*H*W\n",
    "        pca_basis: Tensor of shape (D, latent_dim), assumed to be orthonormal.\n",
    "        \"\"\"\n",
    "        super(PCAEncoder, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.register_buffer('pca_mean', pca_mean)\n",
    "        self.register_buffer('pca_basis', pca_basis)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (B, 3, H, W)\n",
    "        B = x.shape[0]\n",
    "        x_flat = x.reshape(B, -1)  # shape: (B, D)\n",
    "        x_centered = x_flat - self.pca_mean  # subtract mean\n",
    "        latent = x_centered @ self.pca_basis   # shape: (B, latent_dim)\n",
    "        return latent\n",
    "\n",
    "# Create a dummy PCA mean and basis.\n",
    "D = 3 * SCALED_INPUT_HEIGHT * SCALED_INPUT_WIDTH  # dimensionality of flattened image\n",
    "dummy_mean = torch.zeros(D)  # In practice, use your computed mean.\n",
    "dummy_basis = torch.randn(D, latent_dim)\n",
    "q, _ = torch.qr(dummy_basis)  # Orthonormalize\n",
    "pca_basis = q\n",
    "\n",
    "# ============================== Informer Generator Definition ==============================\n",
    "class InformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dropout=0.1):\n",
    "        super(InformerEncoderLayer, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        self.linear1 = nn.Linear(d_model, d_model * 4)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(d_model * 4, d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, src):\n",
    "        # src shape: (seq_len, B, d_model)\n",
    "        attn_output, _ = self.self_attn(src, src, src)\n",
    "        src = src + self.dropout1(attn_output)\n",
    "        src = self.norm1(src)\n",
    "        ff_output = self.linear2(self.dropout(F.relu(self.linear1(src))))\n",
    "        src = src + self.dropout2(ff_output)\n",
    "        src = self.norm2(src)\n",
    "        return src\n",
    "\n",
    "class InformerGenerator(nn.Module):\n",
    "    def __init__(self, latent_dim, image_size, patch_size, d_model=256, nhead=8, num_layers=6, dropout=0.1):\n",
    "        \"\"\"\n",
    "        image_size: Tuple (H, W) for the output image.\n",
    "        patch_size: Size of square patches to divide the image into.\n",
    "        \"\"\"\n",
    "        super(InformerGenerator, self).__init__()\n",
    "        self.image_size = image_size  # (H, W)\n",
    "        self.patch_size = patch_size\n",
    "        self.grid_size = (image_size[0] // patch_size, image_size[1] // patch_size)\n",
    "        self.num_tokens = self.grid_size[0] * self.grid_size[1]\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # Map the latent vector to a sequence of tokens.\n",
    "        self.linear_proj = nn.Linear(latent_dim, self.num_tokens * d_model)\n",
    "        # Learned positional embeddings for the tokens.\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, self.num_tokens, d_model))\n",
    "        \n",
    "        # Stack of Informer encoder layers.\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            InformerEncoderLayer(d_model, nhead, dropout=dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Map each token to a patch (flattened patch pixels).\n",
    "        self.token_to_patch = nn.Linear(d_model, patch_size * patch_size * 3)\n",
    "        \n",
    "    def forward(self, latent):\n",
    "        # latent shape: (B, latent_dim)\n",
    "        B = latent.size(0)\n",
    "        # Map latent to token embeddings: shape (B, num_tokens, d_model)\n",
    "        tokens = self.linear_proj(latent).view(B, self.num_tokens, self.d_model)\n",
    "        tokens = tokens + self.pos_embedding  # Add positional information\n",
    "        \n",
    "        # Informer encoder layers expect input shape (seq_len, B, d_model)\n",
    "        tokens = tokens.transpose(0, 1)\n",
    "        for layer in self.encoder_layers:\n",
    "            tokens = layer(tokens)\n",
    "        tokens = tokens.transpose(0, 1)  # Back to (B, num_tokens, d_model)\n",
    "        \n",
    "        # Map tokens to patches.\n",
    "        patches = self.token_to_patch(tokens)  # (B, num_tokens, patch_size*patch_size*3)\n",
    "        \n",
    "        # Reassemble patches into full image.\n",
    "        grid_h, grid_w = self.grid_size  # e.g., (H/patch_size, W/patch_size)\n",
    "        patches = patches.view(B, grid_h, grid_w, self.patch_size, self.patch_size, 3)\n",
    "        # Rearrange axes to form image: (B, 3, H, W)\n",
    "        image = patches.permute(0, 5, 1, 3, 2, 4).contiguous()\n",
    "        image = image.view(B, 3, grid_h * self.patch_size, grid_w * self.patch_size)\n",
    "        return torch.sigmoid(image)\n",
    "\n",
    "# ============================== Training Function ==============================\n",
    "def train_on_video_sequence(video_path, pca_encoder, informer_gen,\n",
    "                            optimizer, device, buffer_size=BUFFER_SIZE):\n",
    "    \"\"\"\n",
    "    Processes a video sequence in chunks (batches).\n",
    "    For each chunk:\n",
    "      - Get latent representations from the PCA encoder.\n",
    "      - Reconstruct the image with the Informer generator.\n",
    "      - Compute and backpropagate the reconstruction (L1) loss.\n",
    "    Additionally, a progress bar reports the number of frames processed and\n",
    "    the average time per image in each batch.\n",
    "    \"\"\"\n",
    "    data = prepare_video_sequence(video_path)\n",
    "    if data is None:\n",
    "        print(f\"[Warning] Video {video_path} has too few frames. Skipping.\")\n",
    "        return None\n",
    "    seq_len = len(data)\n",
    "    informer_gen.train()\n",
    "    scaler = GradScaler()\n",
    "    total_loss = 0.0\n",
    "    count = 0\n",
    "\n",
    "    # Create a progress bar for the frames in the video.\n",
    "    with tqdm(total=seq_len, desc=\"Training Frames\", unit=\"frame\", leave=False) as frame_pbar:\n",
    "        for start in range(0, seq_len, buffer_size):\n",
    "            batch_start_time = time.time()\n",
    "            end = min(start + buffer_size, seq_len)\n",
    "            frames = [frame.to(device, non_blocking=True) for frame in data[start:end]]\n",
    "            x_real = torch.stack(frames, dim=0) / 255.0  # (B, 3, H, W)\n",
    "            batch_size = x_real.size(0)\n",
    "            count += batch_size\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            with autocast():\n",
    "                latent = pca_encoder(x_real)\n",
    "                x_rec = informer_gen(latent)\n",
    "                loss = F.l1_loss(x_rec, x_real)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            total_loss += loss.item() * batch_size\n",
    "\n",
    "            # Compute average time per image for this batch.\n",
    "            batch_time = time.time() - batch_start_time\n",
    "            time_per_image = batch_time / batch_size\n",
    "            frame_pbar.set_postfix({\"time/img (s)\": f\"{time_per_image:.4f}\"})\n",
    "            frame_pbar.update(batch_size)\n",
    "    avg_loss = total_loss / count if count > 0 else float('inf')\n",
    "    return avg_loss\n",
    "\n",
    "# ============================== (Optional) Validation Function ==============================\n",
    "def validate_on_video_sequence(video_path, pca_encoder, informer_gen, device, buffer_size=BUFFER_SIZE):\n",
    "    data = prepare_video_sequence(video_path)\n",
    "    if data is None:\n",
    "        print(f\"[Warning] Video {video_path} has too few frames. Skipping.\")\n",
    "        return None\n",
    "    seq_len = len(data)\n",
    "    informer_gen.eval()\n",
    "    total_loss = 0.0\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for start in range(0, seq_len, buffer_size):\n",
    "            end = min(start + buffer_size, seq_len)\n",
    "            frames = [frame.to(device, non_blocking=True) for frame in data[start:end]]\n",
    "            x_real = torch.stack(frames, dim=0) / 255.0\n",
    "            batch_size = x_real.size(0)\n",
    "            count += batch_size\n",
    "\n",
    "            latent = pca_encoder(x_real)\n",
    "            x_rec = informer_gen(latent)\n",
    "            loss = F.l1_loss(x_rec, x_real)\n",
    "            total_loss += loss.item() * batch_size\n",
    "\n",
    "    avg_loss = total_loss / count if count > 0 else float('inf')\n",
    "    return avg_loss\n",
    "\n",
    "# ============================== Main Training Loop ==============================\n",
    "def main_training():\n",
    "    # List video files for training and (optionally) validation.\n",
    "    train_videos = [f for f in os.listdir(TRAIN_FOLDER) if f.lower().endswith(('.mov', '.mp4', '.avi'))]\n",
    "    train_videos.sort()\n",
    "    val_videos = [f for f in os.listdir(VAL_FOLDER) if f.lower().endswith(('.mov', '.mp4', '.avi'))]\n",
    "    val_videos.sort()\n",
    "    if MAX_TRAIN_VIDS is not None:\n",
    "        train_videos = train_videos[:MAX_TRAIN_VIDS]\n",
    "    if MAX_VAL_VIDS is not None:\n",
    "        val_videos = val_videos[:MAX_VAL_VIDS]\n",
    "    print(f\"\\n[Info] Number of training videos: {len(train_videos)}\")\n",
    "    print(f\"[Info] Number of validation videos: {len(val_videos)}\")\n",
    "    \n",
    "    # Initialize the PCA encoder (fixed) and the Informer-based generator.\n",
    "    pca_encoder = PCAEncoder(latent_dim=latent_dim, pca_mean=dummy_mean, pca_basis=pca_basis).to(device)\n",
    "    informer_gen = InformerGenerator(\n",
    "        latent_dim=latent_dim,\n",
    "        image_size=TARGET_SIZE,\n",
    "        patch_size=PATCH_SIZE,\n",
    "        d_model=d_model,\n",
    "        nhead=nhead,\n",
    "        num_layers=num_layers,\n",
    "        dropout=dropout\n",
    "    ).to(device)\n",
    "\n",
    "    # Only the Informer generator is trainable.\n",
    "    optimizer = torch.optim.Adam(informer_gen.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(1, NUM_EPOCHS + 1):\n",
    "        start_time = time.time()\n",
    "        print(f\"\\n[Epoch {epoch}/{NUM_EPOCHS}] - Training Phase\")\n",
    "        epoch_train_losses = []\n",
    "        for video_file in tqdm(train_videos, desc=\"Training Videos\", unit=\"video\"):\n",
    "            video_path = os.path.join(TRAIN_FOLDER, video_file)\n",
    "            loss = train_on_video_sequence(video_path, pca_encoder, informer_gen,\n",
    "                                           optimizer, device, buffer_size=BUFFER_SIZE)\n",
    "            if loss is not None:\n",
    "                epoch_train_losses.append(loss)\n",
    "        avg_train_loss = sum(epoch_train_losses) / len(epoch_train_losses) if epoch_train_losses else float('inf')\n",
    "        train_losses.append(avg_train_loss)\n",
    "        print(f\"[Epoch {epoch}] Average Train Loss: {avg_train_loss:.6f}\")\n",
    "\n",
    "        # Validation phase.\n",
    "        epoch_val_losses = []\n",
    "        for video_file in tqdm(val_videos, desc=\"Validation Videos\", unit=\"video\"):\n",
    "            video_path = os.path.join(VAL_FOLDER, video_file)\n",
    "            loss = validate_on_video_sequence(video_path, pca_encoder, informer_gen, device, buffer_size=BUFFER_SIZE)\n",
    "            if loss is not None:\n",
    "                epoch_val_losses.append(loss)\n",
    "        avg_val_loss = sum(epoch_val_losses) / len(epoch_val_losses) if epoch_val_losses else float('inf')\n",
    "        val_losses.append(avg_val_loss)\n",
    "        print(f\"[Epoch {epoch}] Average Validation Loss: {avg_val_loss:.6f}\")\n",
    "\n",
    "        # Save checkpoint.\n",
    "        ckpt_path = os.path.join(CHECKPOINT_DIR, f\"informer_epoch_{epoch}.pth\")\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'informer_gen_state_dict': informer_gen.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_loss': avg_train_loss,\n",
    "            'val_loss': avg_val_loss\n",
    "        }, ckpt_path)\n",
    "        print(f\"[Info] Checkpoint saved at epoch {epoch}.\")\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"[Epoch {epoch}] Completed in {elapsed_time:.2f} seconds.\")\n",
    "\n",
    "    # Plot training and validation loss curves.\n",
    "    epochs = range(1, NUM_EPOCHS + 1)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, train_losses, label='Train Loss', marker='o')\n",
    "    plt.plot(epochs, val_losses, label='Val Loss', marker='x')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('L1 Reconstruction Loss')\n",
    "    plt.title('Informer Training Loss over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    print(\"[Info] Training process completed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_training()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN + Informer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Using device: cuda\n",
      "\n",
      "===== FRAME INFO =====\n",
      "Original Frame Size: 720 x 1280\n",
      "Target Frame Size: 72 x 128\n",
      "\n",
      "[Info] Number of training videos: 1\n",
      "[Info] Number of validation videos: 1\n",
      "\n",
      "[Epoch 1/200] - Training Phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Videos: 100%|██████████| 1/1 [00:15<00:00, 15.97s/video]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Average Train Loss (1-SSIM): 0.712585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Videos: 100%|██████████| 1/1 [00:06<00:00,  6.72s/video]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Average Validation Loss (1-SSIM): 0.793712\n",
      "[Info] Checkpoint saved at epoch 1.\n",
      "[Epoch 1] Completed in 23.28 seconds.\n",
      "\n",
      "[Epoch 2/200] - Training Phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Videos: 100%|██████████| 1/1 [00:14<00:00, 14.65s/video]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Average Train Loss (1-SSIM): 0.743346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Videos: 100%|██████████| 1/1 [00:06<00:00,  6.37s/video]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Average Validation Loss (1-SSIM): 0.800513\n",
      "[Info] Checkpoint saved at epoch 2.\n",
      "[Epoch 2] Completed in 21.61 seconds.\n",
      "\n",
      "[Epoch 3/200] - Training Phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Videos: 100%|██████████| 1/1 [00:14<00:00, 14.48s/video]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Average Train Loss (1-SSIM): 0.718202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Videos: 100%|██████████| 1/1 [00:06<00:00,  6.32s/video]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Average Validation Loss (1-SSIM): 0.779145\n",
      "[Info] Checkpoint saved at epoch 3.\n",
      "[Epoch 3] Completed in 21.39 seconds.\n",
      "\n",
      "[Epoch 4/200] - Training Phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Videos: 100%|██████████| 1/1 [00:14<00:00, 14.63s/video]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Average Train Loss (1-SSIM): 0.716426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Videos: 100%|██████████| 1/1 [00:06<00:00,  6.35s/video]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Average Validation Loss (1-SSIM): 0.809348\n",
      "[Info] Checkpoint saved at epoch 4.\n",
      "[Epoch 4] Completed in 21.55 seconds.\n",
      "\n",
      "[Epoch 5/200] - Training Phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Videos: 100%|██████████| 1/1 [00:14<00:00, 14.89s/video]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Average Train Loss (1-SSIM): 0.726387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Videos: 100%|██████████| 1/1 [00:06<00:00,  6.36s/video]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Average Validation Loss (1-SSIM): 0.818580\n",
      "[Info] Checkpoint saved at epoch 5.\n",
      "[Epoch 5] Completed in 21.82 seconds.\n",
      "\n",
      "[Epoch 6/200] - Training Phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Videos: 100%|██████████| 1/1 [00:14<00:00, 14.96s/video]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] Average Train Loss (1-SSIM): 0.711299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Videos: 100%|██████████| 1/1 [00:06<00:00,  6.39s/video]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] Average Validation Loss (1-SSIM): 0.812109\n",
      "[Info] Checkpoint saved at epoch 6.\n",
      "[Epoch 6] Completed in 21.93 seconds.\n",
      "\n",
      "[Epoch 7/200] - Training Phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Videos: 100%|██████████| 1/1 [00:14<00:00, 14.79s/video]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7] Average Train Loss (1-SSIM): 0.816731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Videos: 100%|██████████| 1/1 [00:06<00:00,  6.38s/video]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7] Average Validation Loss (1-SSIM): 0.833303\n",
      "[Info] Checkpoint saved at epoch 7.\n",
      "[Epoch 7] Completed in 21.74 seconds.\n",
      "\n",
      "[Epoch 8/200] - Training Phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Videos: 100%|██████████| 1/1 [00:14<00:00, 14.91s/video]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8] Average Train Loss (1-SSIM): 0.703722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Videos: 100%|██████████| 1/1 [00:06<00:00,  6.50s/video]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8] Average Validation Loss (1-SSIM): 0.773848\n",
      "[Info] Checkpoint saved at epoch 8.\n",
      "[Epoch 8] Completed in 22.03 seconds.\n",
      "\n",
      "[Epoch 9/200] - Training Phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Videos: 100%|██████████| 1/1 [00:14<00:00, 14.91s/video]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9] Average Train Loss (1-SSIM): 0.674037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Videos: 100%|██████████| 1/1 [00:06<00:00,  6.75s/video]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9] Average Validation Loss (1-SSIM): 0.768590\n",
      "[Info] Checkpoint saved at epoch 9.\n",
      "[Epoch 9] Completed in 22.25 seconds.\n",
      "\n",
      "[Epoch 10/200] - Training Phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Videos: 100%|██████████| 1/1 [00:14<00:00, 14.57s/video]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10] Average Train Loss (1-SSIM): 0.707453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Videos: 100%|██████████| 1/1 [00:06<00:00,  6.40s/video]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10] Average Validation Loss (1-SSIM): 0.802462\n",
      "[Info] Checkpoint saved at epoch 10.\n",
      "[Epoch 10] Completed in 21.54 seconds.\n",
      "\n",
      "[Epoch 11/200] - Training Phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Videos: 100%|██████████| 1/1 [00:14<00:00, 14.42s/video]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 11] Average Train Loss (1-SSIM): 0.701173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Videos: 100%|██████████| 1/1 [00:06<00:00,  6.28s/video]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 11] Average Validation Loss (1-SSIM): 0.802664\n",
      "[Info] Checkpoint saved at epoch 11.\n",
      "[Epoch 11] Completed in 21.27 seconds.\n",
      "\n",
      "[Epoch 12/200] - Training Phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Videos: 100%|██████████| 1/1 [00:14<00:00, 14.72s/video]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 12] Average Train Loss (1-SSIM): 0.727849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Videos: 100%|██████████| 1/1 [00:06<00:00,  6.56s/video]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 12] Average Validation Loss (1-SSIM): 0.784975\n",
      "[Info] Checkpoint saved at epoch 12.\n",
      "[Epoch 12] Completed in 21.86 seconds.\n",
      "\n",
      "[Epoch 13/200] - Training Phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Videos: 100%|██████████| 1/1 [00:14<00:00, 14.89s/video]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 13] Average Train Loss (1-SSIM): 0.697297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Videos: 100%|██████████| 1/1 [00:06<00:00,  6.54s/video]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 13] Average Validation Loss (1-SSIM): 0.781157\n",
      "[Info] Checkpoint saved at epoch 13.\n",
      "[Epoch 13] Completed in 22.02 seconds.\n",
      "\n",
      "[Epoch 14/200] - Training Phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Videos: 100%|██████████| 1/1 [00:16<00:00, 16.12s/video]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 14] Average Train Loss (1-SSIM): 0.770155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Videos: 100%|██████████| 1/1 [00:07<00:00,  7.45s/video]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 14] Average Validation Loss (1-SSIM): 0.810032\n",
      "[Info] Checkpoint saved at epoch 14.\n",
      "[Epoch 14] Completed in 24.23 seconds.\n",
      "\n",
      "[Epoch 15/200] - Training Phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Videos: 100%|██████████| 1/1 [00:15<00:00, 15.97s/video]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 15] Average Train Loss (1-SSIM): 0.723636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Videos: 100%|██████████| 1/1 [00:07<00:00,  7.67s/video]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 15] Average Validation Loss (1-SSIM): 0.820516\n",
      "[Info] Checkpoint saved at epoch 15.\n",
      "[Epoch 15] Completed in 24.23 seconds.\n",
      "\n",
      "[Epoch 16/200] - Training Phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Videos: 100%|██████████| 1/1 [00:16<00:00, 16.36s/video]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 16] Average Train Loss (1-SSIM): 0.777573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Videos: 100%|██████████| 1/1 [00:07<00:00,  7.87s/video]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 16] Average Validation Loss (1-SSIM): 0.828812\n",
      "[Info] Checkpoint saved at epoch 16.\n",
      "[Epoch 16] Completed in 24.83 seconds.\n",
      "\n",
      "[Epoch 17/200] - Training Phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Videos: 100%|██████████| 1/1 [00:16<00:00, 16.59s/video]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 17] Average Train Loss (1-SSIM): 0.872060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Videos: 100%|██████████| 1/1 [00:07<00:00,  7.33s/video]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 17] Average Validation Loss (1-SSIM): 0.884081\n",
      "[Info] Checkpoint saved at epoch 17.\n",
      "[Epoch 17] Completed in 24.54 seconds.\n",
      "\n",
      "[Epoch 18/200] - Training Phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Videos:   0%|          | 0/1 [00:12<?, ?video/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 372\u001b[0m\n\u001b[0;32m    369\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Info] Training process completed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 372\u001b[0m     \u001b[43mmain_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 325\u001b[0m, in \u001b[0;36mmain_training\u001b[1;34m()\u001b[0m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m video_file \u001b[38;5;129;01min\u001b[39;00m tqdm(train_videos, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Videos\u001b[39m\u001b[38;5;124m\"\u001b[39m, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvideo\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    324\u001b[0m     video_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(TRAIN_FOLDER, video_file)\n\u001b[1;32m--> 325\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_on_video_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minformer_gen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBUFFER_SIZE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    327\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    328\u001b[0m         epoch_train_losses\u001b[38;5;241m.\u001b[39mappend(loss)\n",
      "Cell \u001b[1;32mIn[5], line 249\u001b[0m, in \u001b[0;36mtrain_on_video_sequence\u001b[1;34m(video_path, encoder, informer_gen, optimizer, device, buffer_size)\u001b[0m\n\u001b[0;32m    246\u001b[0m scaler\u001b[38;5;241m.\u001b[39mstep(optimizer)\n\u001b[0;32m    247\u001b[0m scaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m--> 249\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m batch_size\n\u001b[0;32m    251\u001b[0m batch_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m batch_start_time\n\u001b[0;32m    252\u001b[0m time_per_image \u001b[38;5;241m=\u001b[39m batch_time \u001b[38;5;241m/\u001b[39m batch_size\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "train_informer_from_pretrained_embeddings_ssim.py\n",
    "\n",
    "This script uses a pretrained image embedding model (ResNet‑18) to project video frames \n",
    "into a latent embedding space. An Informer‑based generator then upsamples the latent code \n",
    "into an image. Training is driven by a reconstruction loss based on SSIM between the \n",
    "generated and original images.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pytorch_msssim import ssim\n",
    "\n",
    "# Import torchvision to load a pretrained model.\n",
    "import torchvision.models as models\n",
    "\n",
    "# ============================== Global Flags and Hyperparameters ==============================\n",
    "NUM_EPOCHS = 200\n",
    "LEARNING_RATE = 1e-4\n",
    "BUFFER_SIZE = 4                # Number of frames per chunk (batch size for training)\n",
    "latent_dim = 256               # Dimension of the latent embedding produced by the pretrained encoder\n",
    "lambda_recon = 1.0             # Weight for reconstruction loss\n",
    "\n",
    "# Informer-specific hyperparameters\n",
    "PATCH_SIZE = 4                 # Size of each image patch (smaller patch increases number of patches)\n",
    "d_model = 256                  # Embedding dimension (should match latent_dim for the linear mapping)\n",
    "nhead = 8                      # Number of attention heads\n",
    "num_layers = 6                 # Number of Informer encoder layers\n",
    "dropout = 0.1                  # Dropout probability\n",
    "\n",
    "# ============================== Device Configuration ==============================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"[Info] Using device: {device}\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# ============================== Global Config & Paths ==============================\n",
    "TRAIN_FOLDER = \"bdd100k/videos/train\"  # Path to training videos\n",
    "VAL_FOLDER   = \"bdd100k/videos/val\"      # Path to validation videos\n",
    "CHECKPOINT_DIR = \"checkpoints_informer\"\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "MAX_TRAIN_VIDS = 1  # Limit number of training videos\n",
    "MAX_VAL_VIDS = 1    # Limit number of validation videos\n",
    "\n",
    "# ============================== Frame Size Configuration ==============================\n",
    "ORIG_HEIGHT, ORIG_WIDTH = 720, 1280\n",
    "FRAME_SCALE_INPUT = 0.10  \n",
    "FRAME_SCALE_OUTPUT = 0.25  \n",
    "SCALED_INPUT_HEIGHT = int(ORIG_HEIGHT * FRAME_SCALE_INPUT)\n",
    "SCALED_INPUT_WIDTH  = int(ORIG_WIDTH * FRAME_SCALE_INPUT)\n",
    "TARGET_SIZE = (SCALED_INPUT_HEIGHT, SCALED_INPUT_WIDTH)\n",
    "\n",
    "print(\"\\n===== FRAME INFO =====\")\n",
    "print(f\"Original Frame Size: {ORIG_HEIGHT} x {ORIG_WIDTH}\")\n",
    "print(f\"Target Frame Size: {TARGET_SIZE[0]} x {TARGET_SIZE[1]}\")\n",
    "\n",
    "# ============================== Data Preparation ==============================\n",
    "def process_frame(frame, target_size):\n",
    "    \"\"\"Resize the frame to the target size if necessary.\"\"\"\n",
    "    H, W = frame.shape[:2]\n",
    "    if (H, W) != target_size:\n",
    "        frame = cv2.resize(frame, (target_size[1], target_size[0]))\n",
    "    return frame\n",
    "\n",
    "def prepare_video_sequence(video_path):\n",
    "    \"\"\"\n",
    "    Loads all frames from the video and processes/resizes them.\n",
    "    Returns a list of torch tensors (with shape [3, H, W] and type float).\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        # Convert BGR to RGB\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frames.append(frame_rgb)\n",
    "    cap.release()\n",
    "    if len(frames) < 1:\n",
    "        return None\n",
    "    processed_frames = []\n",
    "    for frame in frames:\n",
    "        processed = process_frame(frame, TARGET_SIZE)\n",
    "        # Convert to tensor; scaling to [0,1] will be handled in the encoder if needed.\n",
    "        tensor_frame = (torch.from_numpy(np.ascontiguousarray(processed))\n",
    "                            .permute(2, 0, 1)\n",
    "                            .float()\n",
    "                            .pin_memory())\n",
    "        processed_frames.append(tensor_frame)\n",
    "    return processed_frames\n",
    "\n",
    "# ============================== Pretrained Image Encoder Definition ==============================\n",
    "class PretrainedResNetEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Uses a pretrained ResNet-18 to extract image features, then projects them\n",
    "    to a latent embedding of dimension `latent_dim`.\n",
    "    \n",
    "    The model expects input images to be in [0, 1] and normalizes them using \n",
    "    ImageNet mean and std.\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim):\n",
    "        super(PretrainedResNetEncoder, self).__init__()\n",
    "        # Load a pretrained ResNet-18.\n",
    "        resnet = models.resnet18(pretrained=True)\n",
    "        # Remove the final fully-connected layer.\n",
    "        self.feature_extractor = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        # Project the extracted features to the desired latent dimension.\n",
    "        self.proj = nn.Linear(resnet.fc.in_features, latent_dim)\n",
    "        # ImageNet normalization parameters.\n",
    "        self.register_buffer(\"mean\", torch.tensor([0.485, 0.456, 0.406]).view(1,3,1,1))\n",
    "        self.register_buffer(\"std\", torch.tensor([0.229, 0.224, 0.225]).view(1,3,1,1))\n",
    "        # Optionally freeze the feature extractor if you do not wish to finetune it:\n",
    "        for param in self.feature_extractor.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, 3, H, W) in [0, 1]. Normalize using ImageNet statistics.\n",
    "        x = (x - self.mean) / self.std\n",
    "        features = self.feature_extractor(x)  # (B, C, 1, 1)\n",
    "        features = features.view(features.size(0), -1)\n",
    "        latent = self.proj(features)\n",
    "        return latent\n",
    "\n",
    "# ============================== Informer Generator Definition ==============================\n",
    "class InformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dropout=0.1):\n",
    "        super(InformerEncoderLayer, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        self.linear1 = nn.Linear(d_model, d_model * 4)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(d_model * 4, d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, src):\n",
    "        # src shape: (seq_len, B, d_model)\n",
    "        attn_output, _ = self.self_attn(src, src, src)\n",
    "        src = src + self.dropout1(attn_output)\n",
    "        src = self.norm1(src)\n",
    "        ff_output = self.linear2(self.dropout(F.relu(self.linear1(src))))\n",
    "        src = src + self.dropout2(ff_output)\n",
    "        src = self.norm2(src)\n",
    "        return src\n",
    "\n",
    "class InformerGenerator(nn.Module):\n",
    "    def __init__(self, latent_dim, image_size, patch_size, d_model=256, nhead=8, num_layers=6, dropout=0.1):\n",
    "        \"\"\"\n",
    "        image_size: Tuple (H, W) for the output image.\n",
    "        patch_size: Size of square patches to divide the image into.\n",
    "        \"\"\"\n",
    "        super(InformerGenerator, self).__init__()\n",
    "        self.image_size = image_size  # (H, W)\n",
    "        self.patch_size = patch_size\n",
    "        self.grid_size = (image_size[0] // patch_size, image_size[1] // patch_size)\n",
    "        self.num_tokens = self.grid_size[0] * self.grid_size[1]\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # Map the latent vector to a sequence of tokens.\n",
    "        self.linear_proj = nn.Linear(latent_dim, self.num_tokens * d_model)\n",
    "        # Learned positional embeddings for the tokens.\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, self.num_tokens, d_model))\n",
    "        \n",
    "        # Stack of Informer encoder layers.\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            InformerEncoderLayer(d_model, nhead, dropout=dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Map each token to a patch (flattened patch pixels).\n",
    "        self.token_to_patch = nn.Linear(d_model, patch_size * patch_size * 3)\n",
    "        \n",
    "    def forward(self, latent):\n",
    "        # latent shape: (B, latent_dim)\n",
    "        B = latent.size(0)\n",
    "        # Map latent to token embeddings: shape (B, num_tokens, d_model)\n",
    "        tokens = self.linear_proj(latent).view(B, self.num_tokens, self.d_model)\n",
    "        tokens = tokens + self.pos_embedding  # Add positional information\n",
    "        \n",
    "        # Informer encoder layers expect input shape (seq_len, B, d_model)\n",
    "        tokens = tokens.transpose(0, 1)\n",
    "        for layer in self.encoder_layers:\n",
    "            tokens = layer(tokens)\n",
    "        tokens = tokens.transpose(0, 1)  # Back to (B, num_tokens, d_model)\n",
    "        \n",
    "        # Map tokens to patches.\n",
    "        patches = self.token_to_patch(tokens)  # (B, num_tokens, patch_size*patch_size*3)\n",
    "        \n",
    "        # Reassemble patches into full image.\n",
    "        grid_h, grid_w = self.grid_size  # e.g., (H/patch_size, W/patch_size)\n",
    "        patches = patches.view(B, grid_h, grid_w, self.patch_size, self.patch_size, 3)\n",
    "        # Rearrange axes to form image: (B, 3, H, W)\n",
    "        image = patches.permute(0, 5, 1, 3, 2, 4).contiguous()\n",
    "        image = image.view(B, 3, grid_h * self.patch_size, grid_w * self.patch_size)\n",
    "        return torch.sigmoid(image)\n",
    "\n",
    "# ============================== Training Function ==============================\n",
    "def train_on_video_sequence(video_path, encoder, informer_gen,\n",
    "                            optimizer, device, buffer_size=BUFFER_SIZE):\n",
    "    \"\"\"\n",
    "    Processes a video sequence in chunks (batches).\n",
    "    For each chunk:\n",
    "      - Get latent representations from the pretrained encoder.\n",
    "      - Reconstruct the image with the Informer generator.\n",
    "      - Compute and backpropagate the reconstruction loss based on SSIM.\n",
    "    \"\"\"\n",
    "    data = prepare_video_sequence(video_path)\n",
    "    if data is None:\n",
    "        print(f\"[Warning] Video {video_path} has too few frames. Skipping.\")\n",
    "        return None\n",
    "    seq_len = len(data)\n",
    "    informer_gen.train()\n",
    "    encoder.train()\n",
    "    scaler = GradScaler()\n",
    "    total_loss = 0.0\n",
    "    count = 0\n",
    "\n",
    "    with tqdm(total=seq_len, desc=\"Training Frames\", unit=\"frame\", leave=False) as frame_pbar:\n",
    "        for start in range(0, seq_len, buffer_size):\n",
    "            batch_start_time = time.time()\n",
    "            end = min(start + buffer_size, seq_len)\n",
    "            frames = [frame.to(device, non_blocking=True) for frame in data[start:end]]\n",
    "            # Scale to [0,1]\n",
    "            x_real = torch.stack(frames, dim=0) / 255.0  # (B, 3, H, W)\n",
    "            batch_size = x_real.size(0)\n",
    "            count += batch_size\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            with autocast():\n",
    "                # Get latent embeddings from the pretrained encoder.\n",
    "                latent = encoder(x_real)\n",
    "                x_rec = informer_gen(latent)\n",
    "                # Use SSIM as the loss (minimizing 1 - SSIM)\n",
    "                loss = 1 - ssim(x_rec, x_real, data_range=1.0, size_average=True)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            total_loss += loss.item() * batch_size\n",
    "\n",
    "            batch_time = time.time() - batch_start_time\n",
    "            time_per_image = batch_time / batch_size\n",
    "            frame_pbar.set_postfix({\"time/img (s)\": f\"{time_per_image:.4f}\"})\n",
    "            frame_pbar.update(batch_size)\n",
    "    avg_loss = total_loss / count if count > 0 else float('inf')\n",
    "    return avg_loss\n",
    "\n",
    "# ============================== Validation Function ==============================\n",
    "def validate_on_video_sequence(video_path, encoder, informer_gen, device, buffer_size=BUFFER_SIZE):\n",
    "    data = prepare_video_sequence(video_path)\n",
    "    if data is None:\n",
    "        print(f\"[Warning] Video {video_path} has too few frames. Skipping.\")\n",
    "        return None\n",
    "    seq_len = len(data)\n",
    "    informer_gen.eval()\n",
    "    encoder.eval()\n",
    "    total_loss = 0.0\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for start in range(0, seq_len, buffer_size):\n",
    "            end = min(start + buffer_size, seq_len)\n",
    "            frames = [frame.to(device, non_blocking=True) for frame in data[start:end]]\n",
    "            x_real = torch.stack(frames, dim=0) / 255.0\n",
    "            batch_size = x_real.size(0)\n",
    "            count += batch_size\n",
    "\n",
    "            latent = encoder(x_real)\n",
    "            x_rec = informer_gen(latent)\n",
    "            loss = 1 - ssim(x_rec, x_real, data_range=1.0, size_average=True)\n",
    "            total_loss += loss.item() * batch_size\n",
    "\n",
    "    avg_loss = total_loss / count if count > 0 else float('inf')\n",
    "    return avg_loss\n",
    "\n",
    "# ============================== Main Training Loop ==============================\n",
    "def main_training():\n",
    "    # List video files for training and (optionally) validation.\n",
    "    train_videos = [f for f in os.listdir(TRAIN_FOLDER) if f.lower().endswith(('.mov', '.mp4', '.avi'))]\n",
    "    train_videos.sort()\n",
    "    val_videos = [f for f in os.listdir(VAL_FOLDER) if f.lower().endswith(('.mov', '.mp4', '.avi'))]\n",
    "    val_videos.sort()\n",
    "    if MAX_TRAIN_VIDS is not None:\n",
    "        train_videos = train_videos[:MAX_TRAIN_VIDS]\n",
    "    if MAX_VAL_VIDS is not None:\n",
    "        val_videos = val_videos[:MAX_VAL_VIDS]\n",
    "    print(f\"\\n[Info] Number of training videos: {len(train_videos)}\")\n",
    "    print(f\"[Info] Number of validation videos: {len(val_videos)}\")\n",
    "    \n",
    "    # Initialize the pretrained encoder and the Informer-based generator.\n",
    "    encoder = PretrainedResNetEncoder(latent_dim=latent_dim).to(device)\n",
    "    informer_gen = InformerGenerator(\n",
    "        latent_dim=latent_dim,\n",
    "        image_size=TARGET_SIZE,\n",
    "        patch_size=PATCH_SIZE,\n",
    "        d_model=d_model,\n",
    "        nhead=nhead,\n",
    "        num_layers=num_layers,\n",
    "        dropout=dropout\n",
    "    ).to(device)\n",
    "\n",
    "    # Both networks are trainable (though you might freeze the encoder if desired).\n",
    "    optimizer = torch.optim.Adam(list(encoder.parameters()) + list(informer_gen.parameters()),\n",
    "                                 lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(1, NUM_EPOCHS + 1):\n",
    "        start_time = time.time()\n",
    "        print(f\"\\n[Epoch {epoch}/{NUM_EPOCHS}] - Training Phase\")\n",
    "        epoch_train_losses = []\n",
    "        for video_file in tqdm(train_videos, desc=\"Training Videos\", unit=\"video\"):\n",
    "            video_path = os.path.join(TRAIN_FOLDER, video_file)\n",
    "            loss = train_on_video_sequence(video_path, encoder, informer_gen,\n",
    "                                           optimizer, device, buffer_size=BUFFER_SIZE)\n",
    "            if loss is not None:\n",
    "                epoch_train_losses.append(loss)\n",
    "        avg_train_loss = sum(epoch_train_losses) / len(epoch_train_losses) if epoch_train_losses else float('inf')\n",
    "        train_losses.append(avg_train_loss)\n",
    "        print(f\"[Epoch {epoch}] Average Train Loss (1-SSIM): {avg_train_loss:.6f}\")\n",
    "\n",
    "        # Validation phase.\n",
    "        epoch_val_losses = []\n",
    "        for video_file in tqdm(val_videos, desc=\"Validation Videos\", unit=\"video\"):\n",
    "            video_path = os.path.join(VAL_FOLDER, video_file)\n",
    "            loss = validate_on_video_sequence(video_path, encoder, informer_gen, device, buffer_size=BUFFER_SIZE)\n",
    "            if loss is not None:\n",
    "                epoch_val_losses.append(loss)\n",
    "        avg_val_loss = sum(epoch_val_losses) / len(epoch_val_losses) if epoch_val_losses else float('inf')\n",
    "        val_losses.append(avg_val_loss)\n",
    "        print(f\"[Epoch {epoch}] Average Validation Loss (1-SSIM): {avg_val_loss:.6f}\")\n",
    "\n",
    "        # Save checkpoint.\n",
    "        ckpt_path = os.path.join(CHECKPOINT_DIR, f\"informer_epoch_{epoch}.pth\")\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'encoder_state_dict': encoder.state_dict(),\n",
    "            'informer_gen_state_dict': informer_gen.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_loss': avg_train_loss,\n",
    "            'val_loss': avg_val_loss\n",
    "        }, ckpt_path)\n",
    "        print(f\"[Info] Checkpoint saved at epoch {epoch}.\")\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"[Epoch {epoch}] Completed in {elapsed_time:.2f} seconds.\")\n",
    "\n",
    "    # Plot training and validation loss curves.\n",
    "    epochs = range(1, NUM_EPOCHS + 1)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, train_losses, label='Train Loss (1-SSIM)', marker='o')\n",
    "    plt.plot(epochs, val_losses, label='Val Loss (1-SSIM)', marker='x')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss (1 - SSIM)')\n",
    "    plt.title('Informer Training Loss over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    print(\"[Info] Training process completed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_training()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Starting the Informer training process...\n",
      "Total videos: 700\n",
      "Training videos: 560, Testing videos: 140\n",
      "User-specified checkpoint 'c:\\Users\\keplarV4\\Downloads\\Github\\Graduate School\\Semantic-Communication-Robot\\checkpoints\\informer_epoch_10.pth' not found. Starting training from scratch.\n",
      "Total videos to train on: 700\n",
      "Training videos: 560, Testing videos: 140\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 42116, 45452, 37396, 32116) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\keplarV4\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1133\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1132\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1133\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "File \u001b[1;32mc:\\Users\\keplarV4\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\queue.py:179\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m--> 179\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_empty\u001b[38;5;241m.\u001b[39mwait(remaining)\n",
      "\u001b[1;31mEmpty\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 608\u001b[0m\n\u001b[0;32m    605\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining videos: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_filenames)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Testing videos: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(test_filenames)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    607\u001b[0m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[1;32m--> 608\u001b[0m \u001b[43mtrain_informer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    610\u001b[0m \u001b[38;5;66;03m# After training, prepare list of test samples for GUI\u001b[39;00m\n\u001b[0;32m    611\u001b[0m \u001b[38;5;66;03m# For GUI, it's more efficient to randomly select videos and frames as needed\u001b[39;00m\n\u001b[0;32m    612\u001b[0m \u001b[38;5;66;03m# Therefore, no need to preload all test samples\u001b[39;00m\n\u001b[0;32m    613\u001b[0m \n\u001b[0;32m    614\u001b[0m \u001b[38;5;66;03m# Start the GUI\u001b[39;00m\n\u001b[0;32m    615\u001b[0m root \u001b[38;5;241m=\u001b[39m tk\u001b[38;5;241m.\u001b[39mTk()\n",
      "Cell \u001b[1;32mIn[6], line 327\u001b[0m, in \u001b[0;36mtrain_informer\u001b[1;34m(num_epochs, checkpoint_interval)\u001b[0m\n\u001b[0;32m    324\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    326\u001b[0m \u001b[38;5;66;03m# Iterate over the DataLoader\u001b[39;00m\n\u001b[1;32m--> 327\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (input_seq, output_seq) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[0;32m    328\u001b[0m     input_seq \u001b[38;5;241m=\u001b[39m input_seq\u001b[38;5;241m.\u001b[39mto(device)   \u001b[38;5;66;03m# [B, input_length, C, H, W]\u001b[39;00m\n\u001b[0;32m    329\u001b[0m     output_seq \u001b[38;5;241m=\u001b[39m output_seq\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;66;03m# [B, output_length, C, H, W]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\keplarV4\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\keplarV4\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1329\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[0;32m   1328\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1329\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1330\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[0;32m   1332\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\keplarV4\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1285\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1283\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m   1284\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[1;32m-> 1285\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1286\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[0;32m   1287\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\keplarV4\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1146\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1145\u001b[0m     pids_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(w\u001b[38;5;241m.\u001b[39mpid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[1;32m-> 1146\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpids_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) exited unexpectedly\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue\u001b[38;5;241m.\u001b[39mEmpty):\n\u001b[0;32m   1148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 42116, 45452, 37396, 32116) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "from PIL import Image\n",
    "import time\n",
    "import warnings\n",
    "import tkinter as tk\n",
    "from tkinter import messagebox\n",
    "from PIL import ImageTk\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "##############################################\n",
    "# 1) Configuration & Paths\n",
    "##############################################\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Paths\n",
    "current_dir = os.getcwd()\n",
    "csv_train = os.path.join(current_dir, \"train_data.csv\")  # CSV containing .mov filenames\n",
    "video_dir = os.path.join(current_dir, \"bdd100k\", \"videos\")  # Directory where videos are stored\n",
    "\n",
    "# Image and model configurations\n",
    "IMAGE_SIZE = 256  # Increased for better visibility\n",
    "EMBED_DIM = 256    # Embedding size for Transformer\n",
    "NUM_HEADS = 8      # Number of attention heads\n",
    "NUM_LAYERS = 3     # Number of layers in encoder and decoder\n",
    "NUM_EPOCHS = 100   # Number of training epochs\n",
    "BATCH_SIZE = 16    # Batch size for training\n",
    "LEARNING_RATE = 5e-4  # Learning rate for optimizer\n",
    "CHECKPOINT_DIR = os.path.join(current_dir, \"checkpoints\")  # Directory to save checkpoints\n",
    "TRAIN_TEST_SPLIT = 0.8  # Ratio of data to be used for training\n",
    "\n",
    "# Create checkpoint directory if it doesn't exist\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Starting the Informer training process...\")\n",
    "\n",
    "# Define image transformations\n",
    "transform_model = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "transform_display = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "])\n",
    "\n",
    "##############################################\n",
    "# 2) Iterable Dataset Class for Streaming Frame Pairs\n",
    "##############################################\n",
    "\n",
    "class StreamingVideoFrameDataset(IterableDataset):\n",
    "    def __init__(self, video_filenames, video_root, transform=None, input_length=1, output_length=1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            video_filenames (list): List of video filenames.\n",
    "            video_root (str): Directory with all the videos.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "            input_length (int): Number of input frames.\n",
    "            output_length (int): Number of output frames to predict.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.video_filenames = video_filenames\n",
    "        self.video_root = video_root\n",
    "        self.transform = transform\n",
    "        self.input_length = input_length\n",
    "        self.output_length = output_length\n",
    "\n",
    "    def __iter__(self):\n",
    "        for fname in self.video_filenames:\n",
    "            video_path = os.path.join(self.video_root, fname)\n",
    "            if not os.path.exists(video_path):\n",
    "                print(f\"[Warning] Video file {video_path} does not exist. Skipping.\")\n",
    "                continue  # Skip non-existent videos\n",
    "\n",
    "            cap = cv2.VideoCapture(video_path)\n",
    "            frames = []\n",
    "            while True:\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                frames.append(frame)\n",
    "            cap.release()\n",
    "\n",
    "            # Generate (input_frame, future_frame) pairs\n",
    "            for i in range(len(frames) - self.output_length):\n",
    "                input_frames = frames[i:i + self.input_length]\n",
    "                output_frames = frames[i + self.input_length:i + self.input_length + self.output_length]\n",
    "\n",
    "                # Apply transformations\n",
    "                input_seq = []\n",
    "                output_seq = []\n",
    "                for input_frame in input_frames:\n",
    "                    frame_rgb = cv2.cvtColor(input_frame, cv2.COLOR_BGR2RGB)\n",
    "                    pil_image = Image.fromarray(frame_rgb)\n",
    "                    if self.transform:\n",
    "                        pil_image = self.transform(pil_image)\n",
    "                    input_seq.append(pil_image)\n",
    "                input_seq = torch.stack(input_seq)  # [input_length, C, H, W]\n",
    "\n",
    "                for output_frame in output_frames:\n",
    "                    frame_rgb = cv2.cvtColor(output_frame, cv2.COLOR_BGR2RGB)\n",
    "                    pil_image = Image.fromarray(frame_rgb)\n",
    "                    if self.transform:\n",
    "                        pil_image = self.transform(pil_image)\n",
    "                    output_seq.append(pil_image)\n",
    "                output_seq = torch.stack(output_seq)  # [output_length, C, H, W]\n",
    "\n",
    "                yield input_seq, output_seq\n",
    "\n",
    "##############################################\n",
    "# 3) Informer Model with Proper Decoder\n",
    "##############################################\n",
    "\n",
    "# Implementing a proper Informer Decoder with Cross-Attention\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, src, src_mask=None):\n",
    "        src2, _ = self.self_attn(src, src, src, attn_mask=src_mask)\n",
    "        src = src + self.dropout1(src2)\n",
    "        src = self.norm1(src)\n",
    "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
    "        src = src + self.dropout2(src2)\n",
    "        src = self.norm2(src)\n",
    "        return src\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)\n",
    "        self.cross_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None):\n",
    "        # Self-attention\n",
    "        tgt2, _ = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask)\n",
    "        tgt = tgt + self.dropout1(tgt2)\n",
    "        tgt = self.norm1(tgt)\n",
    "        \n",
    "        # Cross-attention\n",
    "        tgt2, _ = self.cross_attn(tgt, memory, memory, attn_mask=memory_mask)\n",
    "        tgt = tgt + self.dropout2(tgt2)\n",
    "        tgt = self.norm2(tgt)\n",
    "        \n",
    "        # Feed forward\n",
    "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n",
    "        tgt = tgt + self.dropout3(tgt2)\n",
    "        tgt = self.norm3(tgt)\n",
    "        return tgt\n",
    "\n",
    "class InformerEncoder(nn.Module):\n",
    "    def __init__(self, input_size, d_model=256, n_heads=8, d_ff=512, num_layers=3, dropout=0.1):\n",
    "        super(InformerEncoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.input_projection = nn.Linear(input_size, d_model)\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, src_mask=None):\n",
    "        src = self.input_projection(src)  # [seq_len, batch_size, d_model]\n",
    "        src = self.dropout(src)\n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask)\n",
    "        return src  # [seq_len, batch_size, d_model]\n",
    "\n",
    "class InformerDecoder(nn.Module):\n",
    "    def __init__(self, d_model=256, n_heads=8, d_ff=512, num_layers=3, dropout=0.1, output_size=3*IMAGE_SIZE*IMAGE_SIZE):\n",
    "        super(InformerDecoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.target_projection = nn.Linear(output_size, d_model)  # Added projection\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.output_layer = nn.Linear(d_model, output_size)\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None):\n",
    "        tgt = self.target_projection(tgt)  # [seq_len, batch_size, d_model]\n",
    "        for layer in self.layers:\n",
    "            tgt = layer(tgt, memory, tgt_mask, memory_mask)\n",
    "        output = self.output_layer(tgt)  # [seq_len, batch_size, output_size]\n",
    "        return output\n",
    "\n",
    "class Informer(nn.Module):\n",
    "    def __init__(self, input_size=3*IMAGE_SIZE*IMAGE_SIZE, d_model=256, n_heads=8, d_ff=512, num_layers=3, dropout=0.1, output_size=3*IMAGE_SIZE*IMAGE_SIZE):\n",
    "        super(Informer, self).__init__()\n",
    "        self.encoder = InformerEncoder(input_size, d_model, n_heads, d_ff, num_layers, dropout)\n",
    "        self.decoder = InformerDecoder(d_model, n_heads, d_ff, num_layers, dropout, output_size)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None, memory_mask=None):\n",
    "        memory = self.encoder(src, src_mask)  # [seq_len, batch_size, d_model]\n",
    "        decoder_output = self.decoder(tgt, memory, tgt_mask, memory_mask)  # [seq_len, batch_size, output_size]\n",
    "        return decoder_output\n",
    "\n",
    "# Initialize Informer Model\n",
    "model = Informer().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "##############################################\n",
    "# 4) Checkpointing Functions\n",
    "##############################################\n",
    "\n",
    "def save_checkpoint(state, filename):\n",
    "    \"\"\"\n",
    "    Save the training state to a checkpoint file.\n",
    "\n",
    "    Args:\n",
    "        state (dict): State dictionary containing model, optimizer states, etc.\n",
    "        filename (str): Path to the checkpoint file.\n",
    "    \"\"\"\n",
    "    torch.save(state, filename)\n",
    "    print(f\"Checkpoint saved at {filename}\")\n",
    "\n",
    "def load_checkpoint(filename):\n",
    "    \"\"\"\n",
    "    Load the training state from a checkpoint file.\n",
    "\n",
    "    Args:\n",
    "        filename (str): Path to the checkpoint file.\n",
    "\n",
    "    Returns:\n",
    "        dict or None: Loaded state dictionary or None if file doesn't exist or is invalid.\n",
    "    \"\"\"\n",
    "    if os.path.isfile(filename):\n",
    "        print(f\"Loading checkpoint '{filename}'\")\n",
    "        checkpoint = torch.load(filename, map_location=device)\n",
    "        required_keys = ['model_state_dict', 'optimizer_state_dict', 'epoch']\n",
    "        if all(key in checkpoint for key in required_keys):\n",
    "            return checkpoint\n",
    "        else:\n",
    "            print(f\"Checkpoint '{filename}' is missing required keys. Skipping.\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"No checkpoint found at '{filename}'. Starting from scratch.\")\n",
    "        return None\n",
    "\n",
    "##############################################\n",
    "# 5) Training Loop with Optimizations\n",
    "##############################################\n",
    "\n",
    "def train_informer(num_epochs=NUM_EPOCHS, checkpoint_interval=10):\n",
    "    \"\"\"\n",
    "    Training loop for the Informer model.\n",
    "\n",
    "    Args:\n",
    "        num_epochs (int): Number of training epochs.\n",
    "        checkpoint_interval (int): Interval (in epochs) at which to save checkpoints.\n",
    "    \"\"\"\n",
    "    start_epoch = 0\n",
    "    \n",
    "    # User-specified checkpoint path (relative)\n",
    "    user_checkpoint_path = os.path.join(CHECKPOINT_DIR, \"informer_epoch_10.pth\")  # UPDATE: Set your desired checkpoint here\n",
    "    \n",
    "    if os.path.exists(user_checkpoint_path):\n",
    "        checkpoint = load_checkpoint(user_checkpoint_path)\n",
    "        if checkpoint:\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            start_epoch = checkpoint['epoch']\n",
    "            print(f\"Resuming training from epoch {start_epoch}\")\n",
    "        else:\n",
    "            print(\"Failed to load the specified checkpoint. Starting from scratch.\")\n",
    "    else:\n",
    "        print(f\"User-specified checkpoint '{user_checkpoint_path}' not found. Starting training from scratch.\")\n",
    "\n",
    "    # Read the CSV and get all training video filenames\n",
    "    data = pd.read_csv(csv_train)\n",
    "    all_filenames = data['filename'].tolist()\n",
    "    print(f\"Total videos to train on: {len(all_filenames)}\")\n",
    "    \n",
    "    # Split into training and testing video filenames\n",
    "    train_size = int(TRAIN_TEST_SPLIT * len(all_filenames))\n",
    "    train_filenames = all_filenames[:train_size]\n",
    "    test_filenames = all_filenames[train_size:]\n",
    "    print(f\"Training videos: {len(train_filenames)}, Testing videos: {len(test_filenames)}\")\n",
    "\n",
    "    # Initialize Streaming Dataset and DataLoader\n",
    "    streaming_dataset = StreamingVideoFrameDataset(train_filenames, video_dir, transform=transform_model, input_length=1, output_length=1)\n",
    "    dataloader = DataLoader(\n",
    "        streaming_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=False,  # Shuffle is False for IterableDataset\n",
    "        num_workers=4,  # Adjust based on your CPU cores\n",
    "        pin_memory=True if device.type == 'cuda' else False,\n",
    "        prefetch_factor=2,  # Number of batches loaded in advance by each worker\n",
    "        persistent_workers=True  # Keeps workers alive between epochs\n",
    "    )\n",
    "\n",
    "    # Initialize Mixed Precision scaler\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Iterate over the DataLoader\n",
    "        for batch_idx, (input_seq, output_seq) in enumerate(dataloader):\n",
    "            input_seq = input_seq.to(device)   # [B, input_length, C, H, W]\n",
    "            output_seq = output_seq.to(device) # [B, output_length, C, H, W]\n",
    "            \n",
    "            # Flatten the input frames\n",
    "            input_seq = input_seq.view(input_seq.size(0), input_seq.size(1), -1)  # [B, input_length, C*H*W]\n",
    "            output_seq = output_seq.view(output_seq.size(0), output_seq.size(1), -1)  # [B, output_length, C*H*W]\n",
    "            \n",
    "            # Prepare source and target sequences\n",
    "            src = input_seq.permute(1, 0, 2)  # [input_length, B, C*H*W]\n",
    "            tgt = output_seq.permute(1, 0, 2) # [output_length, B, C*H*W]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass with mixed precision\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(src, tgt)\n",
    "                loss = criterion(outputs, tgt)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            # Logging at intervals (e.g., every 100 batches)\n",
    "            if (batch_idx + 1) % 100 == 0:\n",
    "                print(f\"[Epoch {epoch+1}/{num_epochs}] [Batch {batch_idx+1}] Loss: {loss.item():.6f}\")\n",
    "\n",
    "        end_time = time.time()\n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.6f}, Time: {end_time - start_time:.2f}s\")\n",
    "        \n",
    "        # Save periodic checkpoints every 'checkpoint_interval' epochs\n",
    "        if (epoch + 1) % checkpoint_interval == 0:\n",
    "            checkpoint_path = os.path.join(CHECKPOINT_DIR, f\"informer_epoch_{epoch+1}.pth\")\n",
    "            save_checkpoint({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': avg_loss,\n",
    "            }, checkpoint_path)\n",
    "            print(f\"Saved checkpoint: {checkpoint_path}\")\n",
    "        \n",
    "        # Evaluate after each epoch\n",
    "        val_loss = evaluate_informer(model, test_filenames, criterion)\n",
    "        \n",
    "    print(\"Training completed.\")\n",
    "\n",
    "##############################################\n",
    "# 6) Evaluation Function\n",
    "##############################################\n",
    "\n",
    "def evaluate_informer(model, test_filenames, criterion):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the test set.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The Informer model.\n",
    "        test_filenames (list): List of test video filenames.\n",
    "        criterion (nn.Module): Loss function.\n",
    "\n",
    "    Returns:\n",
    "        float: Average loss on the test set.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_batches = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Initialize Evaluation DataLoader\n",
    "    eval_dataset = StreamingVideoFrameDataset(test_filenames, video_dir, transform=transform_model, input_length=1, output_length=1)\n",
    "    eval_dataloader = DataLoader(\n",
    "        eval_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=False,  # Shuffle is False for evaluation\n",
    "        num_workers=4,  # Adjust based on your CPU cores\n",
    "        pin_memory=True if device.type == 'cuda' else False,\n",
    "        prefetch_factor=2,\n",
    "        persistent_workers=True\n",
    "    )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (input_seq, output_seq) in enumerate(eval_dataloader):\n",
    "            input_seq = input_seq.to(device)\n",
    "            output_seq = output_seq.to(device)\n",
    "            \n",
    "            # Flatten the input frames\n",
    "            input_seq = input_seq.view(input_seq.size(0), input_seq.size(1), -1)  # [B, input_length, C*H*W]\n",
    "            output_seq = output_seq.view(output_seq.size(0), output_seq.size(1), -1)  # [B, output_length, C*H*W]\n",
    "            \n",
    "            # Prepare source and target sequences\n",
    "            src = input_seq.permute(1, 0, 2)  # [input_length, B, C*H*W]\n",
    "            tgt = output_seq.permute(1, 0, 2) # [output_length, B, C*H*W]\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(src, tgt)\n",
    "            loss = criterion(outputs, tgt)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_batches += 1\n",
    "\n",
    "    avg_loss = total_loss / total_batches if total_batches > 0 else 0\n",
    "    end_time = time.time()\n",
    "    print(f\"Validation Loss: {avg_loss:.6f}, Time: {end_time - start_time:.2f}s\")\n",
    "    model.train()\n",
    "    \n",
    "    return avg_loss\n",
    "\n",
    "##############################################\n",
    "# 7) GUI for Frame Prediction\n",
    "##############################################\n",
    "\n",
    "class FramePredictorGUI:\n",
    "    def __init__(self, master, model, test_filenames, transform_model, transform_display):\n",
    "        self.master = master\n",
    "        self.master.title(\"Next Frame Predictor\")\n",
    "        self.master.geometry(\"600x550\")  # Increased window size for better visibility\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        self.test_filenames = test_filenames\n",
    "        self.transform_model = transform_model\n",
    "        self.transform_display = transform_display\n",
    "        self.current_frame = None  # Current input frame\n",
    "        self.predicted_frame = None  # Current predicted frame\n",
    "\n",
    "        # GUI Elements\n",
    "        self.input_label = tk.Label(master, text=\"Input Frame\")\n",
    "        self.input_label.pack(pady=5)\n",
    "        self.canvas_input = tk.Canvas(master, width=IMAGE_SIZE, height=IMAGE_SIZE)\n",
    "        self.canvas_input.pack()\n",
    "\n",
    "        self.predicted_label = tk.Label(master, text=\"Predicted Future Frame\")\n",
    "        self.predicted_label.pack(pady=5)\n",
    "        self.canvas_predicted = tk.Canvas(master, width=IMAGE_SIZE, height=IMAGE_SIZE)\n",
    "        self.canvas_predicted.pack()\n",
    "\n",
    "        self.next_button = tk.Button(master, text=\"Next\", command=self.next_frame)\n",
    "        self.next_button.pack(pady=10)\n",
    "\n",
    "        # Initialize with a random sample\n",
    "        self.load_random_sample()\n",
    "\n",
    "    def load_random_sample(self):\n",
    "        # Randomly select a test video\n",
    "        if not self.test_filenames:\n",
    "            messagebox.showerror(\"Error\", \"No test videos available for GUI.\")\n",
    "            return\n",
    "\n",
    "        fname = np.random.choice(self.test_filenames)\n",
    "        video_path = os.path.join(video_dir, fname)\n",
    "        dataset = VideoFrameDataset(video_path, transform=self.transform_model, input_length=1, output_length=1)\n",
    "        if len(dataset) == 0:\n",
    "            messagebox.showerror(\"Error\", f\"No frame pairs found in {fname}.\")\n",
    "            return\n",
    "\n",
    "        # Randomly select a frame pair\n",
    "        idx = np.random.randint(0, len(dataset))\n",
    "        input_seq, _ = dataset[idx]  # We don't need the actual output sequence for the GUI\n",
    "\n",
    "        # Prepare input tensor\n",
    "        input_tensor = input_seq.unsqueeze(0).to(device)   # [1, input_length, C, H, W]\n",
    "\n",
    "        # Flatten the input frames\n",
    "        input_flat = input_tensor.view(input_tensor.size(0), -1)  # [1, C*H*W]\n",
    "        # Prepare source and target sequences\n",
    "        src = input_flat.unsqueeze(0)  # [1,1, C*H*W]\n",
    "        tgt = torch.zeros(1, 1, 3*IMAGE_SIZE*IMAGE_SIZE).to(device)  # [1,1, C*H*W]\n",
    "\n",
    "        # Debugging: Print shapes\n",
    "        print(f\"Loading Random Sample from {fname} - src shape: {src.shape}, tgt shape: {tgt.shape}\")\n",
    "\n",
    "        # Forward pass to get prediction with mixed precision\n",
    "        with torch.cuda.amp.autocast():\n",
    "            prediction = self.model(src, tgt)  # [1,1, C*H*W]\n",
    "\n",
    "        # Reshape outputs\n",
    "        predicted_output = prediction.permute(1,0,2).view(1, -1, 3, IMAGE_SIZE, IMAGE_SIZE)  # [1,1, C, H, W]\n",
    "\n",
    "        # Convert tensors to NumPy arrays for visualization\n",
    "        self.current_frame = input_tensor.cpu().squeeze(0).squeeze(0).permute(1, 2, 0).numpy()\n",
    "        self.predicted_frame = predicted_output.cpu().squeeze(0).squeeze(0).permute(1, 2, 0).numpy()\n",
    "\n",
    "        # Clip values to [0,1] range for display\n",
    "        self.current_frame = np.clip(self.current_frame, 0, 1)\n",
    "        self.predicted_frame = np.clip(self.predicted_frame, 0, 1)\n",
    "\n",
    "        # Convert to PIL Images\n",
    "        input_pil = Image.fromarray((self.current_frame * 255).astype(np.uint8))\n",
    "        predicted_pil = Image.fromarray((self.predicted_frame * 255).astype(np.uint8))\n",
    "\n",
    "        # Apply display transformations\n",
    "        input_pil = self.transform_display(input_pil)\n",
    "        predicted_pil = self.transform_display(predicted_pil)\n",
    "\n",
    "        # Convert to Tkinter-compatible images\n",
    "        self.tk_input = ImageTk.PhotoImage(input_pil)\n",
    "        self.tk_predicted = ImageTk.PhotoImage(predicted_pil)\n",
    "\n",
    "        # Create image items and store their IDs\n",
    "        self.image_input = self.canvas_input.create_image(0, 0, anchor=tk.NW, image=self.tk_input)\n",
    "        self.image_predicted = self.canvas_predicted.create_image(0, 0, anchor=tk.NW, image=self.tk_predicted)\n",
    "\n",
    "    def next_frame(self):\n",
    "        if self.predicted_frame is None:\n",
    "            messagebox.showwarning(\"Warning\", \"No predicted frame available.\")\n",
    "            return\n",
    "\n",
    "        # Prepare input tensor from the predicted frame\n",
    "        input_pil = Image.fromarray((self.predicted_frame * 255).astype(np.uint8))\n",
    "        input_tensor = self.transform_model(input_pil).unsqueeze(0).to(device)  # [1, C, H, W]\n",
    "\n",
    "        # Flatten the input frames\n",
    "        input_flat = input_tensor.view(input_tensor.size(0), -1)  # [1, C*H*W]\n",
    "        # Prepare source and target sequences\n",
    "        src = input_flat.unsqueeze(0)  # [1,1, C*H*W]\n",
    "        tgt = torch.zeros(1, 1, 3*IMAGE_SIZE*IMAGE_SIZE).to(device)  # [1,1, C*H*W]\n",
    "\n",
    "        # Debugging: Print shapes\n",
    "        print(f\"Next Frame - src shape: {src.shape}, tgt shape: {tgt.shape}\")\n",
    "\n",
    "        # Forward pass to get prediction with mixed precision\n",
    "        try:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                prediction = self.model(src, tgt)  # [1,1, C*H*W]\n",
    "        except RuntimeError as e:\n",
    "            print(f\"RuntimeError during model inference: {e}\")\n",
    "            messagebox.showerror(\"Error\", f\"RuntimeError during model inference:\\n{e}\")\n",
    "            return\n",
    "\n",
    "        # Reshape outputs\n",
    "        predicted_output = prediction.permute(1,0,2).view(1, -1, 3, IMAGE_SIZE, IMAGE_SIZE)  # [1,1, C, H, W]\n",
    "\n",
    "        # Convert tensors to NumPy arrays for visualization\n",
    "        self.current_frame = self.predicted_frame  # Set previous prediction as new input\n",
    "        self.predicted_frame = predicted_output.cpu().squeeze(0).squeeze(0).permute(1, 2, 0).numpy()\n",
    "\n",
    "        # Clip values to [0,1] range for display\n",
    "        self.predicted_frame = np.clip(self.predicted_frame, 0, 1)\n",
    "\n",
    "        # Convert to PIL Image\n",
    "        predicted_pil = Image.fromarray((self.predicted_frame * 255).astype(np.uint8))\n",
    "\n",
    "        # Apply display transformations\n",
    "        predicted_pil = self.transform_display(predicted_pil)\n",
    "\n",
    "        # Convert to Tkinter-compatible image\n",
    "        self.tk_predicted = ImageTk.PhotoImage(predicted_pil)\n",
    "\n",
    "        # Update the predicted frame on the canvas\n",
    "        self.canvas_predicted.itemconfig(self.image_predicted, image=self.tk_predicted)\n",
    "\n",
    "        # Update the input frame on the canvas with the previous predicted frame\n",
    "        input_pil = Image.fromarray((self.current_frame * 255).astype(np.uint8))\n",
    "        input_pil = self.transform_display(input_pil)\n",
    "        self.tk_input = ImageTk.PhotoImage(input_pil)\n",
    "        self.canvas_input.itemconfig(self.image_input, image=self.tk_input)\n",
    "\n",
    "        # Debugging: Print new predicted frame shape\n",
    "        print(f\"Predicted Frame Shape: {self.predicted_frame.shape}\")\n",
    "\n",
    "##############################################\n",
    "# 8) Main Execution\n",
    "##############################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Read the CSV and get all video filenames\n",
    "    data = pd.read_csv(csv_train)\n",
    "    all_filenames = data['filename'].tolist()\n",
    "    print(f\"Total videos: {len(all_filenames)}\")\n",
    "    \n",
    "    # Split into training and testing video filenames\n",
    "    train_size = int(TRAIN_TEST_SPLIT * len(all_filenames))\n",
    "    train_filenames = all_filenames[:train_size]\n",
    "    test_filenames = all_filenames[train_size:]\n",
    "    print(f\"Training videos: {len(train_filenames)}, Testing videos: {len(test_filenames)}\")\n",
    "\n",
    "    # Start training\n",
    "    train_informer(num_epochs=NUM_EPOCHS, checkpoint_interval=10)\n",
    "    \n",
    "    # After training, prepare list of test samples for GUI\n",
    "    # For GUI, it's more efficient to randomly select videos and frames as needed\n",
    "    # Therefore, no need to preload all test samples\n",
    "    \n",
    "    # Start the GUI\n",
    "    root = tk.Tk()\n",
    "    gui = FramePredictorGUI(root, model, test_filenames, transform_model, transform_display)\n",
    "    root.mainloop()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"Total execution time: {end_time - start_time:.2f}s\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Starting the Informer training process...\n",
      "Total videos found: 488\n",
      "Dataset initialized with 488 samples.\n",
      "Training samples: 390, Testing samples: 98\n",
      "DataLoaders initialized.\n",
      "Best checkpoint not found. Starting training.\n",
      "Loading checkpoint 'c:\\Users\\keplarV4\\Downloads\\Github\\Graduate School\\Semantic-Communication-Robot\\checkpoints\\informer_epoch_15.pth'\n",
      "Resuming training from epoch 15\n",
      "[Epoch 16/100] [Batch 1/25]\n",
      "[Epoch 16/100] [Batch 11/25]\n",
      "[Epoch 16/100] [Batch 21/25]\n",
      "Epoch [16/100], Loss: 0.042142, Time: 14.28s\n",
      "Checkpoint saved at c:\\Users\\keplarV4\\Downloads\\Github\\Graduate School\\Semantic-Communication-Robot\\checkpoints\\informer_epoch_16.pth\n",
      "Saved checkpoint: c:\\Users\\keplarV4\\Downloads\\Github\\Graduate School\\Semantic-Communication-Robot\\checkpoints\\informer_epoch_16.pth\n",
      "Removed old checkpoint: informer_epoch_11.pth\n",
      "Validation Loss: 0.046772\n",
      "[Epoch 17/100] [Batch 1/25]\n",
      "[Epoch 17/100] [Batch 11/25]\n",
      "[Epoch 17/100] [Batch 21/25]\n",
      "Epoch [17/100], Loss: 0.041086, Time: 15.17s\n",
      "Checkpoint saved at c:\\Users\\keplarV4\\Downloads\\Github\\Graduate School\\Semantic-Communication-Robot\\checkpoints\\informer_epoch_17.pth\n",
      "Saved checkpoint: c:\\Users\\keplarV4\\Downloads\\Github\\Graduate School\\Semantic-Communication-Robot\\checkpoints\\informer_epoch_17.pth\n",
      "Removed old checkpoint: informer_epoch_12.pth\n",
      "Validation Loss: 0.044457\n",
      "[Epoch 18/100] [Batch 1/25]\n",
      "[Epoch 18/100] [Batch 11/25]\n",
      "[Epoch 18/100] [Batch 21/25]\n",
      "Epoch [18/100], Loss: 0.039438, Time: 15.48s\n",
      "Checkpoint saved at c:\\Users\\keplarV4\\Downloads\\Github\\Graduate School\\Semantic-Communication-Robot\\checkpoints\\informer_epoch_18.pth\n",
      "Saved checkpoint: c:\\Users\\keplarV4\\Downloads\\Github\\Graduate School\\Semantic-Communication-Robot\\checkpoints\\informer_epoch_18.pth\n",
      "Removed old checkpoint: informer_epoch_13.pth\n",
      "Validation Loss: 0.043293\n",
      "[Epoch 19/100] [Batch 1/25]\n",
      "[Epoch 19/100] [Batch 11/25]\n",
      "[Epoch 19/100] [Batch 21/25]\n",
      "Epoch [19/100], Loss: 0.039377, Time: 15.38s\n",
      "Checkpoint saved at c:\\Users\\keplarV4\\Downloads\\Github\\Graduate School\\Semantic-Communication-Robot\\checkpoints\\informer_epoch_19.pth\n",
      "Saved checkpoint: c:\\Users\\keplarV4\\Downloads\\Github\\Graduate School\\Semantic-Communication-Robot\\checkpoints\\informer_epoch_19.pth\n",
      "Removed old checkpoint: informer_epoch_14.pth\n",
      "Validation Loss: 0.042310\n",
      "[Epoch 20/100] [Batch 1/25]\n",
      "[Epoch 20/100] [Batch 11/25]\n",
      "[Epoch 20/100] [Batch 21/25]\n",
      "Epoch [20/100], Loss: 0.038023, Time: 15.48s\n",
      "Checkpoint saved at c:\\Users\\keplarV4\\Downloads\\Github\\Graduate School\\Semantic-Communication-Robot\\checkpoints\\informer_epoch_20.pth\n",
      "Saved checkpoint: c:\\Users\\keplarV4\\Downloads\\Github\\Graduate School\\Semantic-Communication-Robot\\checkpoints\\informer_epoch_20.pth\n",
      "Removed old checkpoint: informer_epoch_15.pth\n",
      "Validation Loss: 0.043522\n",
      "[Epoch 21/100] [Batch 1/25]\n",
      "[Epoch 21/100] [Batch 11/25]\n",
      "[Epoch 21/100] [Batch 21/25]\n",
      "Epoch [21/100], Loss: 0.037866, Time: 15.49s\n",
      "Checkpoint saved at c:\\Users\\keplarV4\\Downloads\\Github\\Graduate School\\Semantic-Communication-Robot\\checkpoints\\informer_epoch_21.pth\n",
      "Saved checkpoint: c:\\Users\\keplarV4\\Downloads\\Github\\Graduate School\\Semantic-Communication-Robot\\checkpoints\\informer_epoch_21.pth\n",
      "Removed old checkpoint: informer_epoch_16.pth\n",
      "Validation Loss: 0.044268\n",
      "[Epoch 22/100] [Batch 1/25]\n",
      "[Epoch 22/100] [Batch 11/25]\n",
      "[Epoch 22/100] [Batch 21/25]\n",
      "Epoch [22/100], Loss: 0.037615, Time: 16.18s\n",
      "Checkpoint saved at c:\\Users\\keplarV4\\Downloads\\Github\\Graduate School\\Semantic-Communication-Robot\\checkpoints\\informer_epoch_22.pth\n",
      "Saved checkpoint: c:\\Users\\keplarV4\\Downloads\\Github\\Graduate School\\Semantic-Communication-Robot\\checkpoints\\informer_epoch_22.pth\n",
      "Removed old checkpoint: informer_epoch_17.pth\n",
      "Validation Loss: 0.045527\n",
      "[Epoch 23/100] [Batch 1/25]\n",
      "[Epoch 23/100] [Batch 11/25]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 539\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest checkpoint not found. Starting training.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 539\u001b[0m     \u001b[43mtrain_informer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    541\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(best_checkpoint_path):\n\u001b[0;32m    542\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading best model from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_checkpoint_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[26], line 335\u001b[0m, in \u001b[0;36mtrain_informer\u001b[1;34m(num_epochs, max_keep)\u001b[0m\n\u001b[0;32m    333\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m    334\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 335\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (input_seq, output_seq) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader_train):\n\u001b[0;32m    336\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m batch_idx \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    337\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] [Batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_idx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(dataloader_train)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\keplarV4\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\keplarV4\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\keplarV4\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\keplarV4\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:419\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[1;34m(self, indices)\u001b[0m\n\u001b[0;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 419\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\keplarV4\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:419\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 419\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "Cell \u001b[1;32mIn[26], line 100\u001b[0m, in \u001b[0;36mVideoFrameDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     98\u001b[0m frames \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_length \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_length):\n\u001b[1;32m--> 100\u001b[0m     ret, frame \u001b[38;5;241m=\u001b[39m \u001b[43mcap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ret:\n\u001b[0;32m    102\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2 \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import warnings\n",
    "import tkinter as tk\n",
    "from tkinter import messagebox\n",
    "from PIL import ImageTk\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "##############################################\n",
    "# 1) Configuration & Paths\n",
    "##############################################\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Paths\n",
    "current_dir = os.getcwd()\n",
    "# Update video_dir to point to the \"train\" folder containing the .mov files.\n",
    "video_dir = os.path.join(current_dir, \"bdd100k\", \"videos\", \"train\")\n",
    "\n",
    "# Image and model configurations\n",
    "IMAGE_SIZE = 256  # Increased for better visibility\n",
    "EMBED_DIM = 256    # Embedding size for Transformer\n",
    "NUM_HEADS = 8      # Number of attention heads\n",
    "NUM_LAYERS = 3     # Number of layers in encoder and decoder\n",
    "NUM_EPOCHS = 100   # Number of epochs (adjust as needed)\n",
    "BATCH_SIZE = 16    # Batch size for training\n",
    "LEARNING_RATE = 5e-4  # Learning rate for optimizer\n",
    "CHECKPOINT_DIR = os.path.join(current_dir, \"checkpoints\")  # Directory to save checkpoints\n",
    "TRAIN_TEST_SPLIT = 0.8  # Ratio of data to be used for training\n",
    "\n",
    "# Create checkpoint directory if it doesn't exist\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Starting the Informer training process...\")\n",
    "\n",
    "# Define image transformations\n",
    "transform_model = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "transform_display = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "])\n",
    "\n",
    "##############################################\n",
    "# 2) Dataset Class for Video Frames from MOV Files\n",
    "##############################################\n",
    "\n",
    "class VideoFrameDataset(Dataset):\n",
    "    def __init__(self, video_root, transform=None, input_length=1, output_length=1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            video_root (str): Directory containing .mov video files.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "            input_length (int): Number of input frames.\n",
    "            output_length (int): Number of output frames to predict.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.video_root = video_root\n",
    "        self.transform = transform\n",
    "        # List all .mov files in the given directory\n",
    "        self.filenames = [f for f in os.listdir(video_root) if f.lower().endswith('.mov')]\n",
    "        self.input_length = input_length\n",
    "        self.output_length = output_length\n",
    "        print(f\"Total videos found: {len(self.filenames)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            fname = self.filenames[idx]\n",
    "            video_path = os.path.join(self.video_root, fname)\n",
    "            \n",
    "            if not os.path.exists(video_path):\n",
    "                print(f\"[Warning] Video file {video_path} does not exist. Returning black images.\")\n",
    "                # Return black images for input and output sequences\n",
    "                input_seq = torch.zeros(self.input_length, 3, IMAGE_SIZE, IMAGE_SIZE)\n",
    "                output_seq = torch.zeros(self.output_length, 3, IMAGE_SIZE, IMAGE_SIZE)\n",
    "                return input_seq, output_seq\n",
    "            \n",
    "            cap = cv2.VideoCapture(video_path)\n",
    "            frames = []\n",
    "            for _ in range(self.input_length + self.output_length):\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                frames.append(frame)\n",
    "            cap.release()\n",
    "            \n",
    "            if len(frames) < self.input_length + self.output_length:\n",
    "                print(f\"[Warning] Not enough frames in {video_path}. Returning black images.\")\n",
    "                input_seq = torch.zeros(self.input_length, 3, IMAGE_SIZE, IMAGE_SIZE)\n",
    "                output_seq = torch.zeros(self.output_length, 3, IMAGE_SIZE, IMAGE_SIZE)\n",
    "                return input_seq, output_seq\n",
    "            \n",
    "            # Process input frames\n",
    "            input_seq = []\n",
    "            for i in range(self.input_length):\n",
    "                frame_rgb = cv2.cvtColor(frames[i], cv2.COLOR_BGR2RGB)\n",
    "                pil_image = Image.fromarray(frame_rgb)\n",
    "                if self.transform:\n",
    "                    pil_image = self.transform(pil_image)\n",
    "                input_seq.append(pil_image)\n",
    "            input_seq = torch.stack(input_seq)  # [input_length, C, H, W]\n",
    "            \n",
    "            # Process output frames\n",
    "            output_seq = []\n",
    "            for i in range(self.input_length, self.input_length + self.output_length):\n",
    "                frame_rgb = cv2.cvtColor(frames[i], cv2.COLOR_BGR2RGB)\n",
    "                pil_image = Image.fromarray(frame_rgb)\n",
    "                if self.transform:\n",
    "                    pil_image = self.transform(pil_image)\n",
    "                output_seq.append(pil_image)\n",
    "            output_seq = torch.stack(output_seq)  # [output_length, C, H, W]\n",
    "            \n",
    "            return input_seq, output_seq  # (input_sequence, output_sequence)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading data at index {idx}: {e}\")\n",
    "            input_seq = torch.zeros(self.input_length, 3, IMAGE_SIZE, IMAGE_SIZE)\n",
    "            output_seq = torch.zeros(self.output_length, 3, IMAGE_SIZE, IMAGE_SIZE)\n",
    "            return input_seq, output_seq\n",
    "\n",
    "# Initialize dataset using the train folder of .mov files\n",
    "full_dataset = VideoFrameDataset(video_dir, transform=transform_model, input_length=1, output_length=1)\n",
    "print(f\"Dataset initialized with {len(full_dataset)} samples.\")\n",
    "\n",
    "# Split dataset into training and testing\n",
    "train_size = int(TRAIN_TEST_SPLIT * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n",
    "print(f\"Training samples: {len(train_dataset)}, Testing samples: {len(test_dataset)}\")\n",
    "\n",
    "# Initialize DataLoaders with num_workers=0 to avoid multiprocessing issues\n",
    "dataloader_train = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    num_workers=0,\n",
    "    pin_memory=True if device.type == 'cuda' else False\n",
    ")\n",
    "dataloader_test = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    num_workers=0,\n",
    "    pin_memory=True if device.type == 'cuda' else False\n",
    ")\n",
    "print(\"DataLoaders initialized.\")\n",
    "\n",
    "##############################################\n",
    "# 3) Informer Model with Proper Decoder\n",
    "##############################################\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, src, src_mask=None):\n",
    "        src2, _ = self.self_attn(src, src, src, attn_mask=src_mask)\n",
    "        src = src + self.dropout1(src2)\n",
    "        src = self.norm1(src)\n",
    "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
    "        src = src + self.dropout2(src2)\n",
    "        src = self.norm2(src)\n",
    "        return src\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)\n",
    "        self.cross_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None):\n",
    "        tgt2, _ = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask)\n",
    "        tgt = tgt + self.dropout1(tgt2)\n",
    "        tgt = self.norm1(tgt)\n",
    "        \n",
    "        tgt2, _ = self.cross_attn(tgt, memory, memory, attn_mask=memory_mask)\n",
    "        tgt = tgt + self.dropout2(tgt2)\n",
    "        tgt = self.norm2(tgt)\n",
    "        \n",
    "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n",
    "        tgt = tgt + self.dropout3(tgt2)\n",
    "        tgt = self.norm3(tgt)\n",
    "        return tgt\n",
    "\n",
    "class InformerEncoder(nn.Module):\n",
    "    def __init__(self, input_size, d_model=256, n_heads=8, d_ff=512, num_layers=3, dropout=0.1):\n",
    "        super(InformerEncoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.input_projection = nn.Linear(input_size, d_model)\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, src_mask=None):\n",
    "        src = self.input_projection(src)\n",
    "        src = self.dropout(src)\n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask)\n",
    "        return src\n",
    "\n",
    "class InformerDecoder(nn.Module):\n",
    "    def __init__(self, d_model=256, n_heads=8, d_ff=512, num_layers=3, dropout=0.1, output_size=3*IMAGE_SIZE*IMAGE_SIZE):\n",
    "        super(InformerDecoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.target_projection = nn.Linear(output_size, d_model)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.output_layer = nn.Linear(d_model, output_size)\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None):\n",
    "        tgt = self.target_projection(tgt)\n",
    "        for layer in self.layers:\n",
    "            tgt = layer(tgt, memory, tgt_mask, memory_mask)\n",
    "        output = self.output_layer(tgt)\n",
    "        return output\n",
    "\n",
    "class Informer(nn.Module):\n",
    "    def __init__(self, input_size=3*IMAGE_SIZE*IMAGE_SIZE, d_model=256, n_heads=8, d_ff=512, num_layers=3, dropout=0.1, output_size=3*IMAGE_SIZE*IMAGE_SIZE):\n",
    "        super(Informer, self).__init__()\n",
    "        self.encoder = InformerEncoder(input_size, d_model, n_heads, d_ff, num_layers, dropout)\n",
    "        self.decoder = InformerDecoder(d_model, n_heads, d_ff, num_layers, dropout, output_size)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None, memory_mask=None):\n",
    "        memory = self.encoder(src, src_mask)\n",
    "        decoder_output = self.decoder(tgt, memory, tgt_mask, memory_mask)\n",
    "        return decoder_output\n",
    "\n",
    "# Initialize the Informer model\n",
    "model = Informer().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "##############################################\n",
    "# 4) Checkpointing Functions\n",
    "##############################################\n",
    "\n",
    "def save_checkpoint(state, filename):\n",
    "    torch.save(state, filename)\n",
    "    print(f\"Checkpoint saved at {filename}\")\n",
    "\n",
    "def load_checkpoint(filename):\n",
    "    if os.path.isfile(filename):\n",
    "        print(f\"Loading checkpoint '{filename}'\")\n",
    "        checkpoint = torch.load(filename, map_location=device)\n",
    "        required_keys = ['model_state_dict', 'optimizer_state_dict']\n",
    "        if all(key in checkpoint for key in required_keys):\n",
    "            return checkpoint\n",
    "        else:\n",
    "            print(f\"Checkpoint '{filename}' is missing required keys. Skipping.\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"No checkpoint found at '{filename}'. Starting from scratch.\")\n",
    "        return None\n",
    "\n",
    "def cleanup_checkpoints(checkpoint_dir, max_keep=5):\n",
    "    checkpoint_files = [f for f in os.listdir(checkpoint_dir) if f.endswith('.pth') and \"best_informer\" not in f]\n",
    "    if len(checkpoint_files) <= max_keep:\n",
    "        return\n",
    "    checkpoint_files.sort(key=lambda x: int(x.split('_')[-1].split('.pth')[0]))\n",
    "    files_to_remove = checkpoint_files[:-max_keep]\n",
    "    for f in files_to_remove:\n",
    "        os.remove(os.path.join(checkpoint_dir, f))\n",
    "        print(f\"Removed old checkpoint: {f}\")\n",
    "\n",
    "##############################################\n",
    "# 5) Training Loop (Saving Every Epoch)\n",
    "##############################################\n",
    "\n",
    "def train_informer(num_epochs=NUM_EPOCHS, max_keep=5):\n",
    "    start_epoch = 0\n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    checkpoint_files = [f for f in os.listdir(CHECKPOINT_DIR) if f.endswith('.pth') and \"best_informer\" not in f]\n",
    "    if checkpoint_files:\n",
    "        try:\n",
    "            latest_checkpoint = max(\n",
    "                checkpoint_files, \n",
    "                key=lambda x: int(x.split('_')[-1].split('.pth')[0])\n",
    "            )\n",
    "            checkpoint_path = os.path.join(CHECKPOINT_DIR, latest_checkpoint)\n",
    "            checkpoint = load_checkpoint(checkpoint_path)\n",
    "            if checkpoint:\n",
    "                model.load_state_dict(checkpoint['model_state_dict'])\n",
    "                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "                start_epoch = checkpoint['epoch']\n",
    "                best_loss = checkpoint.get('best_loss', best_loss)\n",
    "                print(f\"Resuming training from epoch {start_epoch}\")\n",
    "            else:\n",
    "                print(\"Failed to load checkpoint. Starting from scratch.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing checkpoint filenames: {e}. Starting from scratch.\")\n",
    "    else:\n",
    "        print(\"No existing checkpoints found. Starting training from scratch.\")\n",
    "    \n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        start_time = time.time()\n",
    "        for batch_idx, (input_seq, output_seq) in enumerate(dataloader_train):\n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"[Epoch {epoch+1}/{num_epochs}] [Batch {batch_idx+1}/{len(dataloader_train)}]\")\n",
    "            \n",
    "            input_seq = input_seq.to(device)\n",
    "            output_seq = output_seq.to(device)\n",
    "            \n",
    "            input_seq = input_seq.view(input_seq.size(0), input_seq.size(1), -1)\n",
    "            output_seq = output_seq.view(output_seq.size(0), output_seq.size(1), -1)\n",
    "            \n",
    "            src = input_seq.permute(1, 0, 2)\n",
    "            tgt = output_seq.permute(1, 0, 2)\n",
    "            \n",
    "            outputs = model(src, tgt)\n",
    "            loss = criterion(outputs, tgt)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        end_time = time.time()\n",
    "        avg_loss = epoch_loss / len(dataloader_train)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.6f}, Time: {end_time - start_time:.2f}s\")\n",
    "        \n",
    "        checkpoint_path = os.path.join(CHECKPOINT_DIR, f\"informer_epoch_{epoch+1}.pth\")\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'best_loss': best_loss,\n",
    "            'loss': avg_loss,\n",
    "        }, checkpoint_path)\n",
    "        print(f\"Saved checkpoint: {checkpoint_path}\")\n",
    "        \n",
    "        cleanup_checkpoints(CHECKPOINT_DIR, max_keep=max_keep)\n",
    "        \n",
    "        val_loss = evaluate_informer(model, dataloader_test, criterion)\n",
    "        \n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_checkpoint_path = os.path.join(CHECKPOINT_DIR, \"best_informer.pth\")\n",
    "            save_checkpoint({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_loss': best_loss,\n",
    "                'loss': avg_loss,\n",
    "            }, best_checkpoint_path)\n",
    "            print(f\"New best model saved at epoch {epoch+1}\")\n",
    "\n",
    "##############################################\n",
    "# 6) Evaluation Function\n",
    "##############################################\n",
    "\n",
    "def evaluate_informer(model, dataloader_test, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (input_seq, output_seq) in enumerate(dataloader_test):\n",
    "            input_seq = input_seq.to(device)\n",
    "            output_seq = output_seq.to(device)\n",
    "            \n",
    "            input_seq = input_seq.view(input_seq.size(0), input_seq.size(1), -1)\n",
    "            output_seq = output_seq.view(output_seq.size(0), output_seq.size(1), -1)\n",
    "            \n",
    "            src = input_seq.permute(1, 0, 2)\n",
    "            tgt = output_seq.permute(1, 0, 2)\n",
    "            \n",
    "            outputs = model(src, tgt)\n",
    "            loss = criterion(outputs, tgt)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader_test)\n",
    "    print(f\"Validation Loss: {avg_loss:.6f}\")\n",
    "    model.train()\n",
    "    \n",
    "    return avg_loss\n",
    "\n",
    "##############################################\n",
    "# 7) GUI for Frame Prediction\n",
    "##############################################\n",
    "\n",
    "class FramePredictorGUI:\n",
    "    def __init__(self, master, model, test_samples, transform_model, transform_display):\n",
    "        self.master = master\n",
    "        self.master.title(\"Next Frame Predictor\")\n",
    "        self.master.geometry(\"600x550\")\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        self.test_samples = test_samples\n",
    "        self.transform_model = transform_model\n",
    "        self.transform_display = transform_display\n",
    "        self.current_frame = None\n",
    "        self.predicted_frame = None\n",
    "\n",
    "        self.input_label = tk.Label(master, text=\"Input Frame\")\n",
    "        self.input_label.pack(pady=5)\n",
    "        self.canvas_input = tk.Canvas(master, width=IMAGE_SIZE, height=IMAGE_SIZE)\n",
    "        self.canvas_input.pack()\n",
    "\n",
    "        self.predicted_label = tk.Label(master, text=\"Predicted Future Frame\")\n",
    "        self.predicted_label.pack(pady=5)\n",
    "        self.canvas_predicted = tk.Canvas(master, width=IMAGE_SIZE, height=IMAGE_SIZE)\n",
    "        self.canvas_predicted.pack()\n",
    "\n",
    "        self.next_button = tk.Button(master, text=\"Next\", command=self.next_frame)\n",
    "        self.next_button.pack(pady=10)\n",
    "\n",
    "        self.load_random_sample()\n",
    "\n",
    "    def load_random_sample(self):\n",
    "        idx = np.random.randint(0, len(self.test_samples))\n",
    "        input_seq, _ = self.test_samples[idx]\n",
    "\n",
    "        input_tensor = input_seq.unsqueeze(0).to(device)\n",
    "\n",
    "        input_flat = input_tensor.view(input_tensor.size(0), -1)\n",
    "        src = input_flat.unsqueeze(0)\n",
    "        tgt = torch.zeros(1, 1, 3*IMAGE_SIZE*IMAGE_SIZE).to(device)\n",
    "\n",
    "        print(f\"Loading Random Sample - src shape: {src.shape}, tgt shape: {tgt.shape}\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            prediction = self.model(src, tgt)\n",
    "\n",
    "        predicted_output = prediction.permute(1,0,2).view(1, -1, 3, IMAGE_SIZE, IMAGE_SIZE)\n",
    "\n",
    "        self.current_frame = input_tensor.cpu().squeeze(0).squeeze(0).permute(1, 2, 0).numpy()\n",
    "        self.predicted_frame = predicted_output.cpu().squeeze(0).squeeze(0).permute(1, 2, 0).numpy()\n",
    "\n",
    "        self.current_frame = np.clip(self.current_frame, 0, 1)\n",
    "        self.predicted_frame = np.clip(self.predicted_frame, 0, 1)\n",
    "\n",
    "        input_pil = Image.fromarray((self.current_frame * 255).astype(np.uint8))\n",
    "        predicted_pil = Image.fromarray((self.predicted_frame * 255).astype(np.uint8))\n",
    "\n",
    "        input_pil = self.transform_display(input_pil)\n",
    "        predicted_pil = self.transform_display(predicted_pil)\n",
    "\n",
    "        self.tk_input = ImageTk.PhotoImage(input_pil)\n",
    "        self.tk_predicted = ImageTk.PhotoImage(predicted_pil)\n",
    "\n",
    "        self.image_input = self.canvas_input.create_image(0, 0, anchor=tk.NW, image=self.tk_input)\n",
    "        self.image_predicted = self.canvas_predicted.create_image(0, 0, anchor=tk.NW, image=self.tk_predicted)\n",
    "\n",
    "    def next_frame(self):\n",
    "        if self.predicted_frame is None:\n",
    "            messagebox.showwarning(\"Warning\", \"No predicted frame available.\")\n",
    "            return\n",
    "\n",
    "        input_pil = Image.fromarray((self.predicted_frame * 255).astype(np.uint8))\n",
    "        input_tensor = self.transform_model(input_pil).unsqueeze(0).to(device)\n",
    "\n",
    "        input_flat = input_tensor.view(input_tensor.size(0), -1)\n",
    "        src = input_flat.unsqueeze(0)\n",
    "        tgt = torch.zeros(1, 1, 3*IMAGE_SIZE*IMAGE_SIZE).to(device)\n",
    "\n",
    "        print(f\"Next Frame - src shape: {src.shape}, tgt shape: {tgt.shape}\")\n",
    "\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                prediction = self.model(src, tgt)\n",
    "        except RuntimeError as e:\n",
    "            print(f\"RuntimeError during model inference: {e}\")\n",
    "            messagebox.showerror(\"Error\", f\"RuntimeError during model inference:\\n{e}\")\n",
    "            return\n",
    "\n",
    "        predicted_output = prediction.permute(1,0,2).view(1, -1, 3, IMAGE_SIZE, IMAGE_SIZE)\n",
    "\n",
    "        self.current_frame = self.predicted_frame\n",
    "        self.predicted_frame = predicted_output.cpu().squeeze(0).squeeze(0).permute(1, 2, 0).numpy()\n",
    "\n",
    "        self.predicted_frame = np.clip(self.predicted_frame, 0, 1)\n",
    "\n",
    "        predicted_pil = Image.fromarray((self.predicted_frame * 255).astype(np.uint8))\n",
    "        predicted_pil = self.transform_display(predicted_pil)\n",
    "        self.tk_predicted = ImageTk.PhotoImage(predicted_pil)\n",
    "        self.canvas_predicted.itemconfig(self.image_predicted, image=self.tk_predicted)\n",
    "\n",
    "        input_pil = Image.fromarray((self.current_frame * 255).astype(np.uint8))\n",
    "        input_pil = self.transform_display(input_pil)\n",
    "        self.tk_input = ImageTk.PhotoImage(input_pil)\n",
    "        self.canvas_input.itemconfig(self.image_input, image=self.tk_input)\n",
    "\n",
    "        print(f\"Predicted Frame Shape: {self.predicted_frame.shape}\")\n",
    "\n",
    "##############################################\n",
    "# 8) Main Execution\n",
    "##############################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()\n",
    "    \n",
    "    best_checkpoint_path = os.path.join(CHECKPOINT_DIR, \"best_informer.pth\")\n",
    "    \n",
    "    if os.path.exists(best_checkpoint_path):\n",
    "        print(f\"Loading best model from {best_checkpoint_path}\")\n",
    "        checkpoint = torch.load(best_checkpoint_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    else:\n",
    "        print(\"Best checkpoint not found. Starting training.\")\n",
    "        train_informer(num_epochs=NUM_EPOCHS)\n",
    "        \n",
    "        if os.path.exists(best_checkpoint_path):\n",
    "            print(f\"Loading best model from {best_checkpoint_path}\")\n",
    "            checkpoint = torch.load(best_checkpoint_path, map_location=device)\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        else:\n",
    "            print(\"Training completed but best checkpoint not found.\")\n",
    "            print(\"Proceeding to GUI with the current model state.\")\n",
    "    \n",
    "    test_samples = list(test_dataset)\n",
    "    \n",
    "    root = tk.Tk()\n",
    "    gui = FramePredictorGUI(root, model, test_samples, transform_model, transform_display)\n",
    "    root.mainloop()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"Total execution time: {end_time - start_time:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLD Code that worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageTk\n",
    "import tkinter as tk\n",
    "from tkinter import ttk, filedialog, messagebox\n",
    "import random\n",
    "import threading\n",
    "\n",
    "# ----------------------------\n",
    "# Configuration & Paths\n",
    "# ----------------------------\n",
    "\n",
    "# Define default paths (Update these paths as necessary)\n",
    "DEFAULT_H5_PATH = os.path.join(os.getcwd(), \"frame_pairs.h5\")  # Path to the H5 file\n",
    "DEFAULT_CHECKPOINT_PATH = os.path.join(os.getcwd(), \"checkpoints\", \"informer_epoch_50.pth\")  # Path to the checkpoint\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Image dimensions (based on your dataset)\n",
    "INPUT_HEIGHT = 90\n",
    "INPUT_WIDTH = 160\n",
    "OUTPUT_HEIGHT = 180\n",
    "OUTPUT_WIDTH = 320\n",
    "INPUT_SIZE = 3 * INPUT_HEIGHT * INPUT_WIDTH      # 3 * 90 * 160 = 43,200\n",
    "OUTPUT_SIZE = 3 * OUTPUT_HEIGHT * OUTPUT_WIDTH   # 3 * 180 * 320 = 172,800\n",
    "\n",
    "# ----------------------------\n",
    "# Informer Model Definition\n",
    "# ----------------------------\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, src, src_mask=None):\n",
    "        src2, _ = self.self_attn(src, src, src, attn_mask=src_mask)\n",
    "        src = src + self.dropout1(src2)\n",
    "        src = self.norm1(src)\n",
    "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
    "        src = src + self.dropout2(src2)\n",
    "        src = self.norm2(src)\n",
    "        return src\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)\n",
    "        self.cross_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None):\n",
    "        # Self-attention\n",
    "        tgt2, _ = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask)\n",
    "        tgt = tgt + self.dropout1(tgt2)\n",
    "        tgt = self.norm1(tgt)\n",
    "        \n",
    "        # Cross-attention\n",
    "        tgt2, _ = self.cross_attn(tgt, memory, memory, attn_mask=memory_mask)\n",
    "        tgt = tgt + self.dropout2(tgt2)\n",
    "        tgt = self.norm2(tgt)\n",
    "        \n",
    "        # Feed forward\n",
    "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n",
    "        tgt = tgt + self.dropout3(tgt2)\n",
    "        tgt = self.norm3(tgt)\n",
    "        return tgt\n",
    "\n",
    "class InformerEncoder(nn.Module):\n",
    "    def __init__(self, input_size, d_model=256, n_heads=8, d_ff=512, num_layers=3, dropout=0.1):\n",
    "        super(InformerEncoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.input_projection = nn.Linear(input_size, d_model)\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, src_mask=None):\n",
    "        src = self.input_projection(src)  # [seq_len, batch_size, d_model]\n",
    "        src = self.dropout(src)\n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask)\n",
    "        return src  # [seq_len, batch_size, d_model]\n",
    "\n",
    "class InformerDecoder(nn.Module):\n",
    "    def __init__(self, d_model=256, n_heads=8, d_ff=512, num_layers=3, dropout=0.1, output_size=OUTPUT_SIZE):\n",
    "        super(InformerDecoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.target_projection = nn.Linear(output_size, d_model)  # Correctly set to OUTPUT_SIZE\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.output_layer = nn.Linear(d_model, output_size)    # Correctly set to OUTPUT_SIZE\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None):\n",
    "        tgt = self.target_projection(tgt)  # [seq_len, batch_size, d_model]\n",
    "        for layer in self.layers:\n",
    "            tgt = layer(tgt, memory, tgt_mask, memory_mask)\n",
    "        output = self.output_layer(tgt)     # [seq_len, batch_size, output_size]\n",
    "        return output\n",
    "\n",
    "class Informer(nn.Module):\n",
    "    def __init__(self, input_size=INPUT_SIZE, d_model=256, n_heads=8, d_ff=512, num_layers=3, dropout=0.1, output_size=OUTPUT_SIZE):\n",
    "        super(Informer, self).__init__()\n",
    "        self.encoder = InformerEncoder(input_size, d_model, n_heads, d_ff, num_layers, dropout)\n",
    "        self.decoder = InformerDecoder(d_model, n_heads, d_ff, num_layers, dropout, output_size)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None, memory_mask=None):\n",
    "        memory = self.encoder(src, src_mask)  # [seq_len, batch_size, d_model]\n",
    "        decoder_output = self.decoder(tgt, memory, tgt_mask, memory_mask)  # [seq_len, batch_size, output_size]\n",
    "        return decoder_output\n",
    "\n",
    "# ----------------------------\n",
    "# Utility Functions\n",
    "# ----------------------------\n",
    "\n",
    "def load_model(checkpoint_path):\n",
    "    \"\"\"\n",
    "    Load the Informer model from a checkpoint.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        messagebox.showerror(\"Error\", f\"Checkpoint file not found at {checkpoint_path}\")\n",
    "        return None\n",
    "    \n",
    "    model = Informer().to(device)\n",
    "    try:\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.eval()\n",
    "        print(f\"Model loaded successfully from {checkpoint_path}\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        messagebox.showerror(\"Error\", f\"Failed to load model: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_h5_dataset(h5_path):\n",
    "    \"\"\"\n",
    "    Load the H5 dataset.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(h5_path):\n",
    "        messagebox.showerror(\"Error\", f\"H5 file not found at {h5_path}\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        h5_file = h5py.File(h5_path, 'r')\n",
    "        inputs = h5_file['inputs_scaled']\n",
    "        outputs = h5_file['outputs_scaled']\n",
    "        print(f\"H5 dataset loaded successfully from {h5_path}\")\n",
    "        return inputs, outputs\n",
    "    except Exception as e:\n",
    "        messagebox.showerror(\"Error\", f\"Failed to load H5 dataset: {e}\")\n",
    "        return None\n",
    "\n",
    "def preprocess_input(input_frame):\n",
    "    \"\"\"\n",
    "    Preprocess the input frame as per model's requirements.\n",
    "    \"\"\"\n",
    "    # Convert to tensor and flatten\n",
    "    transform = transforms.ToTensor()\n",
    "    input_frame = np.squeeze(input_frame)  # Remove extra dimensions if they exist\n",
    "    if input_frame.ndim == 2:  # If grayscale, expand dimensions\n",
    "        input_frame = np.expand_dims(input_frame, axis=-1)\n",
    "    elif input_frame.ndim == 3 and input_frame.shape[0] in {1, 3}:  # If (C, H, W) format, transpose\n",
    "        input_frame = np.transpose(input_frame, (1, 2, 0))\n",
    "\n",
    "    input_pil = Image.fromarray((input_frame * 255).astype(np.uint8))\n",
    "\n",
    "    input_tensor = transform(input_pil).to(device)  # [C, H, W]\n",
    "    input_tensor = input_tensor.view(1, -1)  # [1, C*H*W]\n",
    "    return input_tensor  # [1, 43200]\n",
    "\n",
    "def postprocess_output(output_tensor):\n",
    "    \"\"\"\n",
    "    Postprocess the output tensor to an image.\n",
    "    \"\"\"\n",
    "    # print(f\"Output Tensor Shape (before squeeze): {output_tensor.shape}\")\n",
    "    \n",
    "    # Remove the sequence dimension and reshape\n",
    "    output_tensor = output_tensor.squeeze(0)  # [batch_size, output_size]\n",
    "    # print(f\"Output Tensor Shape (after squeeze): {output_tensor.shape}\")\n",
    "    \n",
    "    output_tensor = output_tensor.cpu().detach().numpy()\n",
    "    \n",
    "    # Verify the output size\n",
    "    expected_size = 3 * OUTPUT_HEIGHT * OUTPUT_WIDTH\n",
    "    if output_tensor.shape[1] != expected_size:\n",
    "        raise ValueError(f\"Unexpected output size: {output_tensor.shape[1]}. Expected {expected_size}.\")\n",
    "    \n",
    "    output_frame = output_tensor[0].reshape(3, OUTPUT_HEIGHT, OUTPUT_WIDTH)  # [3, 180, 320]\n",
    "    output_frame = np.clip(output_frame, 0, 1)\n",
    "    output_frame = np.transpose(output_frame, (1, 2, 0))  # [H, W, C]\n",
    "    output_pil = Image.fromarray((output_frame * 255).astype(np.uint8))\n",
    "    return output_pil\n",
    "\n",
    "def scale_output_to_input(output_pil):\n",
    "    \"\"\"\n",
    "    Scale the output image back to input size.\n",
    "    \"\"\"\n",
    "    # Resize output image to input size (90x160)\n",
    "    scaled_pil = output_pil.resize((INPUT_WIDTH, INPUT_HEIGHT))\n",
    "    scaled_np = np.array(scaled_pil).astype(np.float32) / 255.0  # [H, W, C] in [0,1]\n",
    "    scaled_np = np.transpose(scaled_np, (2, 0, 1))  # [C, H, W]\n",
    "    return scaled_np  # [3, 90, 160]\n",
    "\n",
    "# ----------------------------\n",
    "# GUI Application\n",
    "# ----------------------------\n",
    "\n",
    "class InformerGUI:\n",
    "    def __init__(self, master, h5_path, checkpoint_path):\n",
    "        self.master = master\n",
    "        self.master.title(\"Informer Model Tester\")\n",
    "        self.master.geometry(\"1200x700\")\n",
    "        \n",
    "        # Load model\n",
    "        self.model = load_model(checkpoint_path)\n",
    "        if self.model is None:\n",
    "            self.master.destroy()\n",
    "            return\n",
    "        \n",
    "        # # Inspect model's output size\n",
    "        # print(\"\\nInspecting model's output size...\")\n",
    "        # inspect_model_output_size(self.model)\n",
    "        \n",
    "        # Load H5 dataset\n",
    "        datasets = load_h5_dataset(h5_path)\n",
    "        if datasets is None:\n",
    "            self.master.destroy()\n",
    "            return\n",
    "        self.inputs, self.outputs = datasets\n",
    "        self.total_samples = self.inputs.shape[0]\n",
    "        \n",
    "        # Initialize variables\n",
    "        self.current_input = None  # Tensor\n",
    "        self.current_output = None  # Tensor\n",
    "        \n",
    "        # Create GUI components\n",
    "        self.create_widgets()\n",
    "        \n",
    "        # Select a random frame on startup\n",
    "        self.select_random_frame()\n",
    "        \n",
    "    def create_widgets(self):\n",
    "        # Frame selection\n",
    "        selection_frame = ttk.Frame(self.master)\n",
    "        selection_frame.pack(pady=10)\n",
    "        \n",
    "        self.random_button = ttk.Button(selection_frame, text=\"Select Random Frame\", command=self.select_random_frame)\n",
    "        self.random_button.pack(side=tk.LEFT, padx=5)\n",
    "        \n",
    "        self.predict_button = ttk.Button(selection_frame, text=\"Predict Next Frame\", command=self.predict_next_frame)\n",
    "        self.predict_button.pack(side=tk.LEFT, padx=5)\n",
    "        self.predict_button.config(state=tk.DISABLED)  # Disabled until a frame is selected\n",
    "        \n",
    "        # Images display\n",
    "        images_frame = ttk.Frame(self.master)\n",
    "        images_frame.pack(pady=10)\n",
    "        \n",
    "        # Input Frame\n",
    "        input_label = ttk.Label(images_frame, text=\"Input Frame\")\n",
    "        input_label.grid(row=0, column=0, padx=10, pady=5)\n",
    "        \n",
    "        self.input_canvas = tk.Canvas(images_frame, width=INPUT_WIDTH, height=INPUT_HEIGHT)\n",
    "        self.input_canvas.grid(row=1, column=0, padx=10, pady=5)\n",
    "        \n",
    "        # Output Frame\n",
    "        output_label = ttk.Label(images_frame, text=\"Predicted Output\")\n",
    "        output_label.grid(row=0, column=1, padx=10, pady=5)\n",
    "        \n",
    "        self.output_canvas = tk.Canvas(images_frame, width=OUTPUT_WIDTH, height=OUTPUT_HEIGHT)\n",
    "        self.output_canvas.grid(row=1, column=1, padx=10, pady=5)\n",
    "        \n",
    "        # Status bar\n",
    "        self.status_var = tk.StringVar()\n",
    "        self.status_var.set(\"Ready\")\n",
    "        self.status_bar = ttk.Label(self.master, textvariable=self.status_var, relief=tk.SUNKEN, anchor=tk.W)\n",
    "        self.status_bar.pack(fill=tk.X, side=tk.BOTTOM, ipady=2)\n",
    "        \n",
    "    def select_random_frame(self):\n",
    "        \"\"\"\n",
    "        Select a random frame from the dataset and display it.\n",
    "        \"\"\"\n",
    "        index = random.randint(0, self.total_samples -1)\n",
    "        input_frame = self.inputs[index]  # [3, 90, 160]\n",
    "        \n",
    "        # Preprocess input\n",
    "        input_tensor = preprocess_input(input_frame)  # [1, 43200]\n",
    "        self.current_input = input_tensor\n",
    "        \n",
    "        # Run prediction in a separate thread (initial)\n",
    "        threading.Thread(target=self.run_prediction, args=(input_tensor, False)).start()\n",
    "        \n",
    "    def predict_next_frame(self):\n",
    "        \"\"\"\n",
    "        Use the predicted output as input for the next prediction.\n",
    "        \"\"\"\n",
    "        if self.current_output is None:\n",
    "            messagebox.showwarning(\"No Prediction\", \"Please select a frame first.\")\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            # Convert model output tensor to PIL image\n",
    "            output_pil = postprocess_output(self.current_output)  # Convert tensor to image\n",
    "\n",
    "            # Resize output to match input size (90x160)\n",
    "            scaled_pil = output_pil.resize((INPUT_WIDTH, INPUT_HEIGHT))\n",
    "\n",
    "            # Convert resized image back to tensor\n",
    "            transform = transforms.ToTensor()\n",
    "            scaled_tensor = transform(scaled_pil).to(device)  # Shape: [3, 90, 160]\n",
    "            scaled_tensor = scaled_tensor.view(1, -1)  # Flatten to [1, 43200]\n",
    "\n",
    "            # Store new input for next prediction\n",
    "            self.current_input = scaled_tensor\n",
    "\n",
    "            # Run prediction in a separate thread (recursive prediction)\n",
    "            threading.Thread(target=self.run_prediction, args=(scaled_tensor, True)).start()\n",
    "\n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Prediction Error\", f\"An error occurred during recursive prediction: {e}\")\n",
    "\n",
    "\n",
    "        \n",
    "    def run_prediction(self, input_tensor, is_recursive=False):\n",
    "        \"\"\"\n",
    "        Run the model prediction and update the GUI.\n",
    "\n",
    "        Args:\n",
    "            input_tensor (torch.Tensor): The preprocessed input tensor.\n",
    "            is_recursive (bool): Flag indicating if this prediction is recursive.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.status_var.set(\"Predicting...\")\n",
    "            self.master.update_idletasks()\n",
    "\n",
    "            # Ensure input tensor shape is correct\n",
    "            src = input_tensor.unsqueeze(0)  # Ensure shape is [1, 1, INPUT_SIZE]\n",
    "\n",
    "            if not is_recursive:\n",
    "                # For the initial prediction, initialize tgt with zeros\n",
    "                tgt = torch.zeros(1, 1, OUTPUT_SIZE).to(device)  # [1, 1, 172800]\n",
    "            else:\n",
    "                # For recursive predictions, use the previous output as tgt\n",
    "                tgt = self.current_output  # [1, 1, 172800]\n",
    "\n",
    "            # Forward pass through the model\n",
    "            with torch.no_grad():\n",
    "                output_tensor = self.model(src, tgt)  # [1, 1, 172800]\n",
    "\n",
    "            # print(f\"Model Output Shape: {output_tensor.shape}\")  # Debugging\n",
    "\n",
    "            self.current_output = output_tensor  # Store for next prediction\n",
    "\n",
    "            # Convert output tensor to displayable image\n",
    "            output_pil = postprocess_output(output_tensor)  # Convert model output to an image\n",
    "\n",
    "            # Convert input tensor back to a PIL image for display\n",
    "            input_pil = self.tensor_to_pil(input_tensor)\n",
    "\n",
    "            # Update GUI with images\n",
    "            self.display_images(input_pil, output_pil)\n",
    "\n",
    "            # Enable the predict button\n",
    "            self.predict_button.config(state=tk.NORMAL)\n",
    "\n",
    "            self.status_var.set(\"Prediction complete.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Prediction Error\", f\"An error occurred during prediction: {e}\")\n",
    "            self.status_var.set(\"Prediction failed.\")\n",
    "\n",
    "    def tensor_to_pil(self, tensor):\n",
    "        \"\"\"\n",
    "        Convert input tensor back to PIL Image for display.\n",
    "        \"\"\"\n",
    "        tensor = tensor.squeeze(0)  # [43200]\n",
    "        tensor = tensor.view(3, INPUT_HEIGHT, INPUT_WIDTH)  # [3, 90, 160]\n",
    "        tensor = tensor.cpu().detach().numpy()\n",
    "        tensor = np.clip(tensor, 0, 1)\n",
    "        tensor = np.transpose(tensor, (1, 2, 0))  # [H, W, C]\n",
    "        input_pil = Image.fromarray((tensor * 255).astype(np.uint8))\n",
    "        return input_pil\n",
    "    \n",
    "    def display_images(self, input_pil, output_pil):\n",
    "        \"\"\"\n",
    "        Display the input and output images on the GUI.\n",
    "        \"\"\"\n",
    "        # Convert images to ImageTk\n",
    "        input_tk = ImageTk.PhotoImage(input_pil)\n",
    "        output_tk = ImageTk.PhotoImage(output_pil)\n",
    "        \n",
    "        # Keep references to avoid garbage collection\n",
    "        self.input_image = input_tk\n",
    "        self.output_image = output_tk\n",
    "        \n",
    "        # Display on canvases\n",
    "        self.input_canvas.create_image(0, 0, anchor=tk.NW, image=self.input_image)\n",
    "        self.output_canvas.create_image(0, 0, anchor=tk.NW, image=self.output_image)\n",
    "\n",
    "# ----------------------------\n",
    "# Main Execution\n",
    "# ----------------------------\n",
    "\n",
    "def main():\n",
    "    root = tk.Tk()\n",
    "    app = InformerGUI(root, DEFAULT_H5_PATH, DEFAULT_CHECKPOINT_PATH)\n",
    "    root.mainloop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully from c:\\Users\\keplarV4\\Downloads\\Github\\Graduate School\\Semantic-Communication-Robot\\checkpoints\\informer_trained.pth\n",
      "H5 dataset loaded successfully from c:\\Users\\keplarV4\\Downloads\\Github\\Graduate School\\Semantic-Communication-Robot\\frame_pairs.h5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageTk\n",
    "import tkinter as tk\n",
    "from tkinter import ttk, filedialog, messagebox\n",
    "import random\n",
    "import threading\n",
    "import argparse\n",
    "\n",
    "# ----------------------------\n",
    "# Configuration & Paths\n",
    "# ----------------------------\n",
    "DEFAULT_H5_PATH = os.path.join(os.getcwd(), \"frame_pairs.h5\")  # HDF5 file with datasets\n",
    "DEFAULT_CHECKPOINT_PATH = os.path.join(os.getcwd(), \"checkpoints\", \"informer_epoch_50.pth\")\n",
    "TRAINED_MODEL_PATH = os.path.join(os.getcwd(), \"checkpoints\", \"informer_trained.pth\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Image dimensions\n",
    "INPUT_HEIGHT = 90\n",
    "INPUT_WIDTH = 160\n",
    "OUTPUT_HEIGHT = 180\n",
    "OUTPUT_WIDTH = 320\n",
    "INPUT_SIZE = 3 * INPUT_HEIGHT * INPUT_WIDTH      # 43,200\n",
    "OUTPUT_SIZE = 3 * OUTPUT_HEIGHT * OUTPUT_WIDTH     # 172,800\n",
    "\n",
    "# ----------------------------\n",
    "# Informer Model Definition\n",
    "# ----------------------------\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, src, src_mask=None):\n",
    "        src2, _ = self.self_attn(src, src, src, attn_mask=src_mask)\n",
    "        src = src + self.dropout1(src2)\n",
    "        src = self.norm1(src)\n",
    "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
    "        src = src + self.dropout2(src2)\n",
    "        src = self.norm2(src)\n",
    "        return src\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)\n",
    "        self.cross_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None):\n",
    "        tgt2, _ = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask)\n",
    "        tgt = tgt + self.dropout1(tgt2)\n",
    "        tgt = self.norm1(tgt)\n",
    "        tgt2, _ = self.cross_attn(tgt, memory, memory, attn_mask=memory_mask)\n",
    "        tgt = tgt + self.dropout2(tgt2)\n",
    "        tgt = self.norm2(tgt)\n",
    "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n",
    "        tgt = tgt + self.dropout3(tgt2)\n",
    "        tgt = self.norm3(tgt)\n",
    "        return tgt\n",
    "\n",
    "class InformerEncoder(nn.Module):\n",
    "    def __init__(self, input_size, d_model=256, n_heads=8, d_ff=512, num_layers=3, dropout=0.1):\n",
    "        super(InformerEncoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.input_projection = nn.Linear(input_size, d_model)\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, src_mask=None):\n",
    "        src = self.input_projection(src)  # [seq_len, batch, d_model]\n",
    "        src = self.dropout(src)\n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask)\n",
    "        return src\n",
    "\n",
    "class InformerDecoder(nn.Module):\n",
    "    def __init__(self, d_model=256, n_heads=8, d_ff=512, num_layers=3, dropout=0.1, output_size=OUTPUT_SIZE):\n",
    "        super(InformerDecoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.target_projection = nn.Linear(output_size, d_model)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.output_layer = nn.Linear(d_model, output_size)\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None):\n",
    "        tgt = self.target_projection(tgt)  # [seq_len, batch, d_model]\n",
    "        for layer in self.layers:\n",
    "            tgt = layer(tgt, memory, tgt_mask, memory_mask)\n",
    "        output = self.output_layer(tgt)     # [seq_len, batch, output_size]\n",
    "        return output\n",
    "\n",
    "class Informer(nn.Module):\n",
    "    def __init__(self, input_size=INPUT_SIZE, d_model=256, n_heads=8, d_ff=512, num_layers=3, dropout=0.1, output_size=OUTPUT_SIZE):\n",
    "        super(Informer, self).__init__()\n",
    "        self.encoder = InformerEncoder(input_size, d_model, n_heads, d_ff, num_layers, dropout)\n",
    "        self.decoder = InformerDecoder(d_model, n_heads, d_ff, num_layers, dropout, output_size)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None, memory_mask=None):\n",
    "        memory = self.encoder(src, src_mask)  # [seq_len, batch, d_model]\n",
    "        decoder_output = self.decoder(tgt, memory, tgt_mask, memory_mask)  # [seq_len, batch, output_size]\n",
    "        return decoder_output\n",
    "\n",
    "# ----------------------------\n",
    "# Utility Functions for Inference\n",
    "# ----------------------------\n",
    "def load_model(checkpoint_path):\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        messagebox.showerror(\"Error\", f\"Checkpoint file not found at {checkpoint_path}\")\n",
    "        return None\n",
    "    model = Informer().to(device)\n",
    "    try:\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.eval()\n",
    "        print(f\"Model loaded successfully from {checkpoint_path}\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        messagebox.showerror(\"Error\", f\"Failed to load model: {e}\")\n",
    "        return None\n",
    "\n",
    "# For training, we want the low-res input and high-res target.\n",
    "def load_h5_dataset(h5_path):\n",
    "    if not os.path.exists(h5_path):\n",
    "        messagebox.showerror(\"Error\", f\"H5 file not found at {h5_path}\")\n",
    "        return None\n",
    "    try:\n",
    "        h5_file = h5py.File(h5_path, 'r')\n",
    "        # Use the original low-res inputs and high-res outputs.\n",
    "        inputs = h5_file['inputs_original']    # shape: (N, 3, 90, 160)\n",
    "        outputs = h5_file['outputs_scaled']      # shape: (N, 3, 180, 320)\n",
    "        print(f\"H5 dataset loaded successfully from {h5_path}\")\n",
    "        return inputs, outputs\n",
    "    except Exception as e:\n",
    "        messagebox.showerror(\"Error\", f\"Failed to load H5 dataset: {e}\")\n",
    "        return None\n",
    "\n",
    "def preprocess_input(input_frame):\n",
    "    # Squeeze the frame to remove singleton dimensions\n",
    "    input_frame = np.squeeze(input_frame)\n",
    "    # If the image has a single channel, ensure it has an extra dimension\n",
    "    if input_frame.ndim == 2:\n",
    "        input_frame = np.expand_dims(input_frame, axis=-1)\n",
    "    # If the channel dimension is first (as in [C, H, W]), transpose to [H, W, C]\n",
    "    elif input_frame.ndim == 3 and input_frame.shape[0] in {1, 3}:\n",
    "        input_frame = np.transpose(input_frame, (1, 2, 0))\n",
    "    \n",
    "    # Check the data type and range. If floating and max<=1, assume [0,1]; if max>1, assume [0,255].\n",
    "    if np.issubdtype(input_frame.dtype, np.floating):\n",
    "        if input_frame.max() <= 1:\n",
    "            input_arr = (input_frame * 255).astype(np.uint8)\n",
    "        else:\n",
    "            input_arr = input_frame.astype(np.uint8)\n",
    "    else:\n",
    "        input_arr = input_frame  # Assume already in uint8 or similar\n",
    "\n",
    "    # Convert to PIL image and then to tensor\n",
    "    input_pil = Image.fromarray(input_arr)\n",
    "    transform = transforms.ToTensor()\n",
    "    input_tensor = transform(input_pil).to(device)  # shape: [C, H, W]\n",
    "    input_tensor = input_tensor.view(1, -1)          # reshape to [1, INPUT_SIZE]\n",
    "    return input_tensor\n",
    "\n",
    "def preprocess_target(target_frame):\n",
    "    # Squeeze the frame to remove singleton dimensions\n",
    "    target_frame = np.squeeze(target_frame)\n",
    "    if target_frame.ndim == 2:\n",
    "        target_frame = np.expand_dims(target_frame, axis=-1)\n",
    "    elif target_frame.ndim == 3 and target_frame.shape[0] in {1, 3}:\n",
    "        target_frame = np.transpose(target_frame, (1, 2, 0))\n",
    "    \n",
    "    # Check the data type and range.\n",
    "    if np.issubdtype(target_frame.dtype, np.floating):\n",
    "        if target_frame.max() <= 1:\n",
    "            target_arr = (target_frame * 255).astype(np.uint8)\n",
    "        else:\n",
    "            target_arr = target_frame.astype(np.uint8)\n",
    "    else:\n",
    "        target_arr = target_frame\n",
    "\n",
    "    target_pil = Image.fromarray(target_arr)\n",
    "    transform = transforms.ToTensor()\n",
    "    target_tensor = transform(target_pil).to(device)  # shape: [C, H, W]\n",
    "    target_tensor = target_tensor.view(1, -1)           # reshape to [1, OUTPUT_SIZE]\n",
    "    return target_tensor\n",
    "\n",
    "def postprocess_output(output_tensor):\n",
    "    output_tensor = output_tensor.squeeze(0)  # [batch, OUTPUT_SIZE]\n",
    "    output_tensor = output_tensor.cpu().detach().numpy()\n",
    "    expected_size = 3 * OUTPUT_HEIGHT * OUTPUT_WIDTH\n",
    "    if output_tensor.shape[1] != expected_size:\n",
    "        raise ValueError(f\"Unexpected output size: {output_tensor.shape[1]}. Expected {expected_size}.\")\n",
    "    output_frame = output_tensor[0].reshape(3, OUTPUT_HEIGHT, OUTPUT_WIDTH)\n",
    "    output_frame = np.clip(output_frame, 0, 1)\n",
    "    output_frame = np.transpose(output_frame, (1, 2, 0))\n",
    "    output_pil = Image.fromarray((output_frame * 255).astype(np.uint8))\n",
    "    return output_pil\n",
    "\n",
    "# ----------------------------\n",
    "# PyTorch Dataset for Training\n",
    "# ----------------------------\n",
    "class H5FramePairDataset(Dataset):\n",
    "    def __init__(self, h5_path):\n",
    "        if not os.path.exists(h5_path):\n",
    "            raise FileNotFoundError(f\"H5 file not found at {h5_path}\")\n",
    "        self.h5_file = h5py.File(h5_path, 'r')\n",
    "        # For training, use the low-res inputs and high-res targets.\n",
    "        self.inputs = self.h5_file['inputs_original']   # shape: (N, 3, 90, 160)\n",
    "        self.outputs = self.h5_file['outputs_scaled']     # shape: (N, 3, 180, 320)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.inputs.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_frame = self.inputs[idx]    # [3, 90, 160]\n",
    "        target_frame = self.outputs[idx]    # [3, 180, 320]\n",
    "        input_tensor = preprocess_input(input_frame)    # [1, INPUT_SIZE]\n",
    "        target_tensor = preprocess_target(target_frame)   # [1, OUTPUT_SIZE]\n",
    "        return input_tensor, target_tensor\n",
    "\n",
    "# ----------------------------\n",
    "# Training Functionality\n",
    "# ----------------------------\n",
    "def train_model(h5_path, epochs=10, batch_size=16, learning_rate=1e-3, save_path=TRAINED_MODEL_PATH):\n",
    "    dataset = H5FramePairDataset(h5_path)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    model = Informer().to(device)\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "    print(\"Starting training...\")\n",
    "    for epoch in range(1, epochs+1):\n",
    "        epoch_loss = 0.0\n",
    "        for i, (src, tgt) in enumerate(dataloader):\n",
    "            # DataLoader returns src: [batch, 1, INPUT_SIZE] and tgt: [batch, 1, OUTPUT_SIZE]\n",
    "            src_seq = src.transpose(0, 1)  # shape: [1, batch, INPUT_SIZE]\n",
    "            tgt_seq = tgt.transpose(0, 1)  # shape: [1, batch, OUTPUT_SIZE]\n",
    "            optimizer.zero_grad()\n",
    "            output = model(src_seq, tgt_seq)  # output shape: [1, batch, OUTPUT_SIZE]\n",
    "            loss = criterion(output, tgt_seq)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        print(f\"Epoch [{epoch}/{epochs}], Loss: {avg_loss:.6f}\")\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    torch.save({'model_state_dict': model.state_dict()}, save_path)\n",
    "    print(f\"Training complete. Model saved to {save_path}\")\n",
    "\n",
    "# ----------------------------\n",
    "# GUI Application for Inference\n",
    "# ----------------------------\n",
    "class InformerGUI:\n",
    "    def __init__(self, master, h5_path, checkpoint_path):\n",
    "        self.master = master\n",
    "        self.master.title(\"Informer Model Tester\")\n",
    "        self.master.geometry(\"1200x700\")\n",
    "        self.model = load_model(checkpoint_path)\n",
    "        if self.model is None:\n",
    "            self.master.destroy()\n",
    "            return\n",
    "        datasets = load_h5_dataset(h5_path)\n",
    "        if datasets is None:\n",
    "            self.master.destroy()\n",
    "            return\n",
    "        self.inputs, self.outputs = datasets  # For inference, these can be the scaled datasets.\n",
    "        self.total_samples = self.inputs.shape[0]\n",
    "        self.current_input = None\n",
    "        self.current_output = None\n",
    "        self.create_widgets()\n",
    "        self.select_random_frame()\n",
    "        \n",
    "    def create_widgets(self):\n",
    "        selection_frame = ttk.Frame(self.master)\n",
    "        selection_frame.pack(pady=10)\n",
    "        self.random_button = ttk.Button(selection_frame, text=\"Select Random Frame\", command=self.select_random_frame)\n",
    "        self.random_button.pack(side=tk.LEFT, padx=5)\n",
    "        self.predict_button = ttk.Button(selection_frame, text=\"Predict Next Frame\", command=self.predict_next_frame)\n",
    "        self.predict_button.pack(side=tk.LEFT, padx=5)\n",
    "        self.predict_button.config(state=tk.DISABLED)\n",
    "        images_frame = ttk.Frame(self.master)\n",
    "        images_frame.pack(pady=10)\n",
    "        input_label = ttk.Label(images_frame, text=\"Input Frame\")\n",
    "        input_label.grid(row=0, column=0, padx=10, pady=5)\n",
    "        self.input_canvas = tk.Canvas(images_frame, width=INPUT_WIDTH, height=INPUT_HEIGHT)\n",
    "        self.input_canvas.grid(row=1, column=0, padx=10, pady=5)\n",
    "        output_label = ttk.Label(images_frame, text=\"Predicted Output\")\n",
    "        output_label.grid(row=0, column=1, padx=10, pady=5)\n",
    "        self.output_canvas = tk.Canvas(images_frame, width=OUTPUT_WIDTH, height=OUTPUT_HEIGHT)\n",
    "        self.output_canvas.grid(row=1, column=1, padx=10, pady=5)\n",
    "        self.status_var = tk.StringVar()\n",
    "        self.status_var.set(\"Ready\")\n",
    "        self.status_bar = ttk.Label(self.master, textvariable=self.status_var, relief=tk.SUNKEN, anchor=tk.W)\n",
    "        self.status_bar.pack(fill=tk.X, side=tk.BOTTOM, ipady=2)\n",
    "        \n",
    "    def select_random_frame(self):\n",
    "        index = random.randint(0, self.total_samples - 1)\n",
    "        input_frame = self.inputs[index]\n",
    "        input_tensor = preprocess_input(input_frame)\n",
    "        self.current_input = input_tensor\n",
    "        threading.Thread(target=self.run_prediction, args=(input_tensor, False)).start()\n",
    "        \n",
    "    def predict_next_frame(self):\n",
    "        if self.current_output is None:\n",
    "            messagebox.showwarning(\"No Prediction\", \"Please select a frame first.\")\n",
    "            return\n",
    "        try:\n",
    "            output_pil = postprocess_output(self.current_output)\n",
    "            # Resize output to input dimensions for the next prediction\n",
    "            scaled_pil = output_pil.resize((INPUT_WIDTH, INPUT_HEIGHT))\n",
    "            transform = transforms.ToTensor()\n",
    "            scaled_tensor = transform(scaled_pil).to(device)\n",
    "            scaled_tensor = scaled_tensor.view(1, -1)\n",
    "            self.current_input = scaled_tensor\n",
    "            threading.Thread(target=self.run_prediction, args=(scaled_tensor, True)).start()\n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Prediction Error\", f\"An error occurred: {e}\")\n",
    "\n",
    "    def run_prediction(self, input_tensor, is_recursive=False):\n",
    "        try:\n",
    "            self.status_var.set(\"Predicting...\")\n",
    "            self.master.update_idletasks()\n",
    "            src = input_tensor.unsqueeze(0)  # shape: [1, 1, INPUT_SIZE]\n",
    "            if not is_recursive:\n",
    "                tgt = torch.zeros(1, 1, OUTPUT_SIZE).to(device)\n",
    "            else:\n",
    "                tgt = self.current_output\n",
    "            with torch.no_grad():\n",
    "                src_seq = src.transpose(0, 1)  # shape: [1, 1, INPUT_SIZE]\n",
    "                tgt_seq = tgt.transpose(0, 1)  # shape: [1, 1, OUTPUT_SIZE]\n",
    "                output_tensor = self.model(src_seq, tgt_seq)\n",
    "            self.current_output = output_tensor\n",
    "            output_pil = postprocess_output(output_tensor)\n",
    "            input_pil = self.tensor_to_pil(input_tensor)\n",
    "            self.display_images(input_pil, output_pil)\n",
    "            self.predict_button.config(state=tk.NORMAL)\n",
    "            self.status_var.set(\"Prediction complete.\")\n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Prediction Error\", f\"An error occurred: {e}\")\n",
    "            self.status_var.set(\"Prediction failed.\")\n",
    "\n",
    "    def tensor_to_pil(self, tensor):\n",
    "        tensor = tensor.squeeze(0)  # reshape from [INPUT_SIZE] to [3, INPUT_HEIGHT, INPUT_WIDTH]\n",
    "        tensor = tensor.view(3, INPUT_HEIGHT, INPUT_WIDTH)\n",
    "        tensor = tensor.cpu().detach().numpy()\n",
    "        tensor = np.clip(tensor, 0, 1)\n",
    "        tensor = np.transpose(tensor, (1, 2, 0))\n",
    "        input_pil = Image.fromarray((tensor * 255).astype(np.uint8))\n",
    "        return input_pil\n",
    "\n",
    "    def display_images(self, input_pil, output_pil):\n",
    "        input_tk = ImageTk.PhotoImage(input_pil)\n",
    "        output_tk = ImageTk.PhotoImage(output_pil)\n",
    "        self.input_image = input_tk  # keep a reference to avoid garbage collection\n",
    "        self.output_image = output_tk\n",
    "        self.input_canvas.create_image(0, 0, anchor=tk.NW, image=self.input_image)\n",
    "        self.output_canvas.create_image(0, 0, anchor=tk.NW, image=self.output_image)\n",
    "\n",
    "# ----------------------------\n",
    "# Main Execution with Mode Selection\n",
    "# ----------------------------\n",
    "def main_gui(checkpoint_path):\n",
    "    root = tk.Tk()\n",
    "    app = InformerGUI(root, DEFAULT_H5_PATH, checkpoint_path)\n",
    "    root.mainloop()\n",
    "\n",
    "def main_train():\n",
    "    parser = argparse.ArgumentParser(description=\"Train the Informer model.\")\n",
    "    parser.add_argument(\"--h5_path\", type=str, default=DEFAULT_H5_PATH, help=\"Path to the H5 dataset file.\")\n",
    "    parser.add_argument(\"--epochs\", type=int, default=10, help=\"Number of training epochs.\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=16, help=\"Batch size for training.\")\n",
    "    parser.add_argument(\"--lr\", type=float, default=1e-3, help=\"Learning rate.\")\n",
    "    parser.add_argument(\"--save_path\", type=str, default=TRAINED_MODEL_PATH, help=\"Path to save the trained model.\")\n",
    "    args = parser.parse_args()\n",
    "    train_model(args.h5_path, epochs=args.epochs, batch_size=args.batch_size, learning_rate=args.lr, save_path=args.save_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Informer Model: Train or Test via GUI\")\n",
    "    parser.add_argument(\"--mode\", type=str, default=\"gui\", choices=[\"gui\", \"train\"], help=\"Run mode: 'gui' for testing, 'train' for training.\")\n",
    "    parser.add_argument(\"--predict_only\", action=\"store_true\", help=\"If set, skip training and load the default checkpoint for prediction.\")\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    if args.mode == \"train\":\n",
    "        main_train()\n",
    "    else:\n",
    "        if not args.predict_only and not os.path.exists(TRAINED_MODEL_PATH):\n",
    "            print(\"No trained checkpoint found. Training the model first...\")\n",
    "            train_model(DEFAULT_H5_PATH, epochs=10, batch_size=16, learning_rate=1e-3, save_path=TRAINED_MODEL_PATH)\n",
    "            checkpoint_path = TRAINED_MODEL_PATH\n",
    "        else:\n",
    "            checkpoint_path = DEFAULT_CHECKPOINT_PATH if args.predict_only else TRAINED_MODEL_PATH\n",
    "        main_gui(checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working Model of old frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully from c:\\Users\\keplarV4\\Downloads\\Github\\Graduate School\\Semantic-Communication-Robot\\checkpoints\\informer_trained.pth\n",
      "H5 dataset loaded successfully from c:\\Users\\keplarV4\\Downloads\\Github\\Graduate School\\Semantic-Communication-Robot\\frame_pairs.h5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageTk\n",
    "import tkinter as tk\n",
    "from tkinter import ttk, messagebox\n",
    "import random\n",
    "import threading\n",
    "import argparse\n",
    "\n",
    "# ----------------------------\n",
    "# Global Device\n",
    "# ----------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ----------------------------\n",
    "# Transformer Model Definition\n",
    "# ----------------------------\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.activation = nn.ReLU()\n",
    "    def forward(self, src, src_mask=None):\n",
    "        src2, _ = self.self_attn(src, src, src, attn_mask=src_mask)\n",
    "        src = src + self.dropout1(src2)\n",
    "        src = self.norm1(src)\n",
    "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
    "        src = src + self.dropout2(src2)\n",
    "        src = self.norm2(src)\n",
    "        return src\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)\n",
    "        self.cross_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "        self.activation = nn.ReLU()\n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None):\n",
    "        tgt2, _ = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask)\n",
    "        tgt = tgt + self.dropout1(tgt2)\n",
    "        tgt = self.norm1(tgt)\n",
    "        tgt2, _ = self.cross_attn(tgt, memory, memory, attn_mask=memory_mask)\n",
    "        tgt = tgt + self.dropout2(tgt2)\n",
    "        tgt = self.norm2(tgt)\n",
    "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n",
    "        tgt = tgt + self.dropout3(tgt2)\n",
    "        tgt = self.norm3(tgt)\n",
    "        return tgt\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, input_size, d_model=256, n_heads=8, d_ff=512, num_layers=3, dropout=0.1):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.input_projection = nn.Linear(input_size, d_model)\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, src, src_mask=None):\n",
    "        src = self.input_projection(src)  # [seq_len, batch, d_model]\n",
    "        src = self.dropout(src)\n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask)\n",
    "        return src\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, d_model=256, n_heads=8, d_ff=512, num_layers=3, dropout=0.1, output_size=172800):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.target_projection = nn.Linear(output_size, d_model)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.output_layer = nn.Linear(d_model, output_size)\n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None):\n",
    "        tgt = self.target_projection(tgt)  # [seq_len, batch, d_model]\n",
    "        for layer in self.layers:\n",
    "            tgt = layer(tgt, memory, tgt_mask, memory_mask)\n",
    "        output = self.output_layer(tgt)     # [seq_len, batch, output_size]\n",
    "        return output\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_size=43200, d_model=256, n_heads=8, d_ff=512, num_layers=3, dropout=0.1, output_size=172800):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = TransformerEncoder(input_size, d_model, n_heads, d_ff, num_layers, dropout)\n",
    "        self.decoder = TransformerDecoder(d_model, n_heads, d_ff, num_layers, dropout, output_size)\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None, memory_mask=None):\n",
    "        memory = self.encoder(src, src_mask)  # [seq_len, batch, d_model]\n",
    "        decoder_output = self.decoder(tgt, memory, tgt_mask, memory_mask)  # [seq_len, batch, output_size]\n",
    "        return decoder_output\n",
    "\n",
    "# ----------------------------\n",
    "# Utility Functions\n",
    "# ----------------------------\n",
    "def load_model(checkpoint_path):\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        messagebox.showerror(\"Error\", f\"Checkpoint file not found at {checkpoint_path}\")\n",
    "        return None\n",
    "    model = Transformer().to(device)\n",
    "    try:\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.eval()\n",
    "        print(f\"Model loaded successfully from {checkpoint_path}\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        messagebox.showerror(\"Error\", f\"Failed to load model: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_h5_dataset(h5_path):\n",
    "    if not os.path.exists(h5_path):\n",
    "        messagebox.showerror(\"Error\", f\"H5 file not found at {h5_path}\")\n",
    "        return None\n",
    "    try:\n",
    "        h5_file = h5py.File(h5_path, 'r')\n",
    "        inputs = h5_file['inputs_original']    # [N, 3, 90, 160]\n",
    "        outputs = h5_file['outputs_scaled']      # [N, 3, 180, 320]\n",
    "        print(f\"H5 dataset loaded successfully from {h5_path}\")\n",
    "        return inputs, outputs\n",
    "    except Exception as e:\n",
    "        messagebox.showerror(\"Error\", f\"Failed to load H5 dataset: {e}\")\n",
    "        return None\n",
    "\n",
    "def preprocess_input(input_frame):\n",
    "    input_frame = np.squeeze(input_frame)\n",
    "    if input_frame.ndim == 2:\n",
    "        input_frame = np.expand_dims(input_frame, axis=-1)\n",
    "    elif input_frame.ndim == 3 and input_frame.shape[0] in {1, 3}:\n",
    "        input_frame = np.transpose(input_frame, (1, 2, 0))\n",
    "    if np.issubdtype(input_frame.dtype, np.floating) and input_frame.max() <= 1:\n",
    "        input_arr = (input_frame * 255).astype(np.uint8)\n",
    "    else:\n",
    "        input_arr = input_frame.astype(np.uint8)\n",
    "    input_pil = Image.fromarray(input_arr)\n",
    "    transform = transforms.ToTensor()\n",
    "    input_tensor = transform(input_pil).to(device)  # [C, H, W]\n",
    "    input_tensor = input_tensor.view(1, -1)          # [1, 43200]\n",
    "    return input_tensor\n",
    "\n",
    "def preprocess_target(target_frame):\n",
    "    target_frame = np.squeeze(target_frame)\n",
    "    if target_frame.ndim == 2:\n",
    "        target_frame = np.expand_dims(target_frame, axis=-1)\n",
    "    elif target_frame.ndim == 3 and target_frame.shape[0] in {1, 3}:\n",
    "        target_frame = np.transpose(target_frame, (1, 2, 0))\n",
    "    if np.issubdtype(target_frame.dtype, np.floating) and target_frame.max() <= 1:\n",
    "        target_arr = (target_frame * 255).astype(np.uint8)\n",
    "    else:\n",
    "        target_arr = target_frame.astype(np.uint8)\n",
    "    target_pil = Image.fromarray(target_arr)\n",
    "    transform = transforms.ToTensor()\n",
    "    target_tensor = transform(target_pil).to(device)  # [C, H, W]\n",
    "    target_tensor = target_tensor.view(1, -1)           # [1, 172800]\n",
    "    return target_tensor\n",
    "\n",
    "def postprocess_output(output_tensor):\n",
    "    output_tensor = output_tensor.squeeze(0)  # [batch, 172800]\n",
    "    output_tensor = output_tensor.cpu().detach().numpy()\n",
    "    expected_size = 3 * 180 * 320\n",
    "    if output_tensor.shape[1] != expected_size:\n",
    "        raise ValueError(f\"Unexpected output size: {output_tensor.shape[1]}. Expected {expected_size}.\")\n",
    "    output_frame = output_tensor[0].reshape(3, 180, 320)\n",
    "    output_frame = np.clip(output_frame, 0, 1)\n",
    "    output_frame = np.transpose(output_frame, (1, 2, 0))\n",
    "    output_pil = Image.fromarray((output_frame * 255).astype(np.uint8))\n",
    "    return output_pil\n",
    "\n",
    "# ----------------------------\n",
    "# PyTorch Dataset for Training\n",
    "# ----------------------------\n",
    "class H5FramePairDataset(Dataset):\n",
    "    def __init__(self, h5_path):\n",
    "        if not os.path.exists(h5_path):\n",
    "            raise FileNotFoundError(f\"H5 file not found at {h5_path}\")\n",
    "        self.h5_file = h5py.File(h5_path, 'r')\n",
    "        self.inputs = self.h5_file['inputs_original']   # [N, 3, 90, 160]\n",
    "        self.outputs = self.h5_file['outputs_scaled']     # [N, 3, 180, 320]\n",
    "    def __len__(self):\n",
    "        return self.inputs.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        input_frame = self.inputs[idx]\n",
    "        target_frame = self.outputs[idx]\n",
    "        input_tensor = preprocess_input(input_frame)    # [1, 43200]\n",
    "        target_tensor = preprocess_target(target_frame)   # [1, 172800]\n",
    "        return input_tensor, target_tensor\n",
    "\n",
    "# ----------------------------\n",
    "# Training Functionality\n",
    "# ----------------------------\n",
    "def train_model(h5_path, epochs, batch_size, learning_rate, save_path, config):\n",
    "    dataset = H5FramePairDataset(h5_path)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    model = Transformer().to(device)\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "    print(\"Starting training...\")\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_loss = 0.0\n",
    "        for i, (src, tgt) in enumerate(dataloader):\n",
    "            # src: [batch, 1, 43200], tgt: [batch, 1, 172800]\n",
    "            src_seq = src.transpose(0, 1)  # [1, batch, 43200]\n",
    "            tgt_seq = tgt.transpose(0, 1)  # [1, batch, 172800]\n",
    "            optimizer.zero_grad()\n",
    "            output = model(src_seq, tgt_seq)  # [1, batch, 172800]\n",
    "            loss = criterion(output, tgt_seq)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        print(f\"Epoch [{epoch}/{epochs}], Loss: {avg_loss:.6f}\")\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    torch.save({'model_state_dict': model.state_dict()}, save_path)\n",
    "    print(f\"Training complete. Model saved to {save_path}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Utility to compute tensor size in KB\n",
    "# ----------------------------\n",
    "def tensor_size_kb(tensor):\n",
    "    return tensor.element_size() * tensor.nelement() / 1024\n",
    "\n",
    "# ----------------------------\n",
    "# GUI Application for Inference and Play\n",
    "# ----------------------------\n",
    "class TransformerGUI:\n",
    "    def __init__(self, master, h5_path, checkpoint_path, config):\n",
    "        self.config = config\n",
    "        self.master = master\n",
    "        self.master.title(\"Transformer Model Tester\")\n",
    "        self.master.geometry(\"1200x700\")\n",
    "        self.model = load_model(checkpoint_path)\n",
    "        if self.model is None:\n",
    "            self.master.destroy()\n",
    "            return\n",
    "        datasets = load_h5_dataset(h5_path)\n",
    "        if datasets is None:\n",
    "            self.master.destroy()\n",
    "            return\n",
    "        self.inputs, self.outputs = datasets\n",
    "        self.total_samples = self.inputs.shape[0]\n",
    "        self.current_input = None\n",
    "        self.current_output = None\n",
    "        self.playing = False\n",
    "        self.create_widgets()\n",
    "        self.select_random_frame()\n",
    "        \n",
    "    def create_widgets(self):\n",
    "        selection_frame = ttk.Frame(self.master)\n",
    "        selection_frame.pack(pady=10)\n",
    "        self.random_button = ttk.Button(selection_frame, text=\"Select Random Frame\", command=self.select_random_frame)\n",
    "        self.random_button.pack(side=tk.LEFT, padx=5)\n",
    "        self.predict_button = ttk.Button(selection_frame, text=\"Predict Next Frame\", command=self.predict_next_frame)\n",
    "        self.predict_button.pack(side=tk.LEFT, padx=5)\n",
    "        self.play_button = ttk.Button(selection_frame, text=\"Play\", command=self.toggle_play)\n",
    "        self.play_button.pack(side=tk.LEFT, padx=5)\n",
    "        self.predict_button.config(state=tk.DISABLED)\n",
    "        \n",
    "        images_frame = ttk.Frame(self.master)\n",
    "        images_frame.pack(pady=10)\n",
    "        input_label = ttk.Label(images_frame, text=\"Input Frame\")\n",
    "        input_label.grid(row=0, column=0, padx=10, pady=5)\n",
    "        self.input_canvas = tk.Canvas(images_frame, width=self.config[\"input_width\"], height=self.config[\"input_height\"])\n",
    "        self.input_canvas.grid(row=1, column=0, padx=10, pady=5)\n",
    "        output_label = ttk.Label(images_frame, text=\"Predicted Output\")\n",
    "        output_label.grid(row=0, column=1, padx=10, pady=5)\n",
    "        self.output_canvas = tk.Canvas(images_frame, width=self.config[\"output_width\"], height=self.config[\"output_height\"])\n",
    "        self.output_canvas.grid(row=1, column=1, padx=10, pady=5)\n",
    "        \n",
    "        self.status_var = tk.StringVar()\n",
    "        self.status_var.set(\"Ready\")\n",
    "        self.status_bar = ttk.Label(self.master, textvariable=self.status_var, relief=tk.SUNKEN, anchor=tk.W)\n",
    "        self.status_bar.pack(fill=tk.X, side=tk.BOTTOM, ipady=2)\n",
    "        \n",
    "        # New label to display tensor sizes (in KB)\n",
    "        self.size_var = tk.StringVar()\n",
    "        self.size_var.set(\"Sizes: Input: 0 KB, Latent: 0 KB, Output: 0 KB\")\n",
    "        self.size_label = ttk.Label(self.master, textvariable=self.size_var, relief=tk.RIDGE, anchor=tk.W)\n",
    "        self.size_label.pack(fill=tk.X, side=tk.BOTTOM, ipady=2)\n",
    "        \n",
    "    def select_random_frame(self):\n",
    "        index = random.randint(0, self.total_samples - 1)\n",
    "        input_frame = self.inputs[index]\n",
    "        input_tensor = preprocess_input(input_frame)\n",
    "        self.current_input = input_tensor\n",
    "        threading.Thread(target=self.run_prediction, args=(input_tensor, False)).start()\n",
    "        \n",
    "    def predict_next_frame(self):\n",
    "        if self.current_output is None:\n",
    "            messagebox.showwarning(\"No Prediction\", \"Please select a frame first.\")\n",
    "            return\n",
    "        try:\n",
    "            output_pil = postprocess_output(self.current_output)\n",
    "            scaled_pil = output_pil.resize((self.config[\"input_width\"], self.config[\"input_height\"]))\n",
    "            transform = transforms.ToTensor()\n",
    "            scaled_tensor = transform(scaled_pil).to(device)\n",
    "            scaled_tensor = scaled_tensor.view(1, -1)\n",
    "            self.current_input = scaled_tensor\n",
    "            threading.Thread(target=self.run_prediction, args=(scaled_tensor, True)).start()\n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Prediction Error\", f\"An error occurred: {e}\")\n",
    "\n",
    "    def run_prediction(self, input_tensor, is_recursive=False):\n",
    "        try:\n",
    "            self.status_var.set(\"Predicting...\")\n",
    "            self.master.update_idletasks()\n",
    "            src = input_tensor.unsqueeze(0)  # [1, 1, INPUT_SIZE]\n",
    "            if not is_recursive:\n",
    "                tgt = torch.zeros(1, 1, self.config[\"output_size\"]).to(device)\n",
    "            else:\n",
    "                tgt = self.current_output\n",
    "            with torch.no_grad():\n",
    "                src_seq = src.transpose(0, 1)  # [1, 1, INPUT_SIZE]\n",
    "                tgt_seq = tgt.transpose(0, 1)  # [1, 1, OUTPUT_SIZE]\n",
    "                # Compute the encoder latent space separately:\n",
    "                memory = self.model.encoder(src_seq)\n",
    "                output_tensor = self.model.decoder(tgt_seq, memory)\n",
    "            self.current_output = output_tensor\n",
    "            output_pil = postprocess_output(output_tensor)\n",
    "            input_pil = self.tensor_to_pil(input_tensor)\n",
    "            self.display_images(input_pil, output_pil)\n",
    "            self.predict_button.config(state=tk.NORMAL)\n",
    "            self.status_var.set(\"Prediction complete.\")\n",
    "            # Update size info:\n",
    "            self.update_size_info(input_tensor, memory, output_tensor)\n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Prediction Error\", f\"An error occurred: {e}\")\n",
    "            self.status_var.set(\"Prediction failed.\")\n",
    "\n",
    "    def update_size_info(self, input_tensor, memory_tensor, output_tensor):\n",
    "        # Compute sizes in kilobytes.\n",
    "        in_size = tensor_size_kb(input_tensor)\n",
    "        mem_size = tensor_size_kb(memory_tensor)\n",
    "        out_size = tensor_size_kb(output_tensor)\n",
    "        self.size_var.set(f\"Sizes: Input: {in_size:.1f} KB, Latent: {mem_size:.1f} KB, Output: {out_size:.1f} KB\")\n",
    "\n",
    "    def tensor_to_pil(self, tensor):\n",
    "        tensor = tensor.squeeze(0)  # [INPUT_SIZE]\n",
    "        tensor = tensor.view(3, self.config[\"input_height\"], self.config[\"input_width\"])\n",
    "        tensor = tensor.cpu().detach().numpy()\n",
    "        tensor = np.clip(tensor, 0, 1)\n",
    "        tensor = np.transpose(tensor, (1, 2, 0))\n",
    "        input_pil = Image.fromarray((tensor * 255).astype(np.uint8))\n",
    "        return input_pil\n",
    "\n",
    "    def display_images(self, input_pil, output_pil):\n",
    "        input_tk = ImageTk.PhotoImage(input_pil)\n",
    "        output_tk = ImageTk.PhotoImage(output_pil)\n",
    "        self.input_image = input_tk\n",
    "        self.output_image = output_tk\n",
    "        self.input_canvas.create_image(0, 0, anchor=tk.NW, image=self.input_image)\n",
    "        self.output_canvas.create_image(0, 0, anchor=tk.NW, image=self.output_image)\n",
    "        \n",
    "    def toggle_play(self):\n",
    "        self.playing = not self.playing\n",
    "        if self.playing:\n",
    "            self.play_button.config(text=\"Stop\")\n",
    "            self.play_loop()\n",
    "        else:\n",
    "            self.play_button.config(text=\"Play\")\n",
    "            \n",
    "    def play_loop(self):\n",
    "        if not self.playing:\n",
    "            return\n",
    "        self.predict_next_frame()\n",
    "        delay = int(1000 / self.config[\"fps\"])  # milliseconds delay based on fps\n",
    "        self.master.after(delay, self.play_loop)\n",
    "\n",
    "def tensor_size_kb(tensor):\n",
    "    return tensor.element_size() * tensor.nelement() / 1024\n",
    "\n",
    "# ----------------------------\n",
    "# Main Execution\n",
    "# ----------------------------\n",
    "def main():\n",
    "    # Configuration dictionary (the only config)\n",
    "    config = {\n",
    "        \"h5_path\": os.path.join(os.getcwd(), \"frame_pairs.h5\"),\n",
    "        \"default_checkpoint\": os.path.join(os.getcwd(), \"checkpoints\", \"Transformer_epoch_50.pth\"),\n",
    "        \"trained_checkpoint\": os.path.join(os.getcwd(), \"checkpoints\", \"Transformer_trained.pth\"),\n",
    "        \"epochs\": 10,\n",
    "        \"batch_size\": 16,\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"mode\": \"gui\",  # \"train\" or \"gui\"\n",
    "        \"predict_only\": False,\n",
    "        \"input_height\": 90,\n",
    "        \"input_width\": 160,\n",
    "        \"output_height\": 180,\n",
    "        \"output_width\": 320,\n",
    "        \"input_size\": 43200,    # 3*90*160\n",
    "        \"output_size\": 172800,  # 3*180*320\n",
    "        \"fps\": 2               # frames per second in play mode\n",
    "    }\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description=\"Transformer Model: Train or Test via GUI\")\n",
    "    parser.add_argument(\"--mode\", type=str, default=config[\"mode\"], choices=[\"gui\", \"train\"],\n",
    "                        help=\"Run mode: 'gui' for testing, 'train' for training.\")\n",
    "    parser.add_argument(\"--predict_only\", action=\"store_true\",\n",
    "                        help=\"If set, skip training and load the default checkpoint for prediction.\")\n",
    "    parser.add_argument(\"--epochs\", type=int, default=config[\"epochs\"], help=\"Number of training epochs.\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=config[\"batch_size\"], help=\"Batch size for training.\")\n",
    "    parser.add_argument(\"--lr\", type=float, default=config[\"learning_rate\"], help=\"Learning rate.\")\n",
    "    args, _ = parser.parse_known_args()\n",
    "    \n",
    "    # Update config from args\n",
    "    config[\"mode\"] = args.mode\n",
    "    config[\"predict_only\"] = args.predict_only\n",
    "    config[\"epochs\"] = args.epochs\n",
    "    config[\"batch_size\"] = args.batch_size\n",
    "    config[\"learning_rate\"] = args.lr\n",
    "    \n",
    "    # Override globals using config\n",
    "    global INPUT_HEIGHT, INPUT_WIDTH, OUTPUT_HEIGHT, OUTPUT_WIDTH, INPUT_SIZE, OUTPUT_SIZE\n",
    "    INPUT_HEIGHT = config[\"input_height\"]\n",
    "    INPUT_WIDTH = config[\"input_width\"]\n",
    "    OUTPUT_HEIGHT = config[\"output_height\"]\n",
    "    OUTPUT_WIDTH = config[\"output_width\"]\n",
    "    INPUT_SIZE = config[\"input_size\"]\n",
    "    OUTPUT_SIZE = config[\"output_size\"]\n",
    "    \n",
    "    if config[\"mode\"] == \"train\":\n",
    "        train_model(config[\"h5_path\"], epochs=config[\"epochs\"],\n",
    "                    batch_size=config[\"batch_size\"], learning_rate=config[\"learning_rate\"],\n",
    "                    save_path=config[\"trained_checkpoint\"], config=config)\n",
    "    else:\n",
    "        if not config[\"predict_only\"] and not os.path.exists(config[\"trained_checkpoint\"]):\n",
    "            print(\"No trained checkpoint found. Training the model first...\")\n",
    "            train_model(config[\"h5_path\"], epochs=config[\"epochs\"],\n",
    "                        batch_size=config[\"batch_size\"], learning_rate=config[\"learning_rate\"],\n",
    "                        save_path=config[\"trained_checkpoint\"], config=config)\n",
    "            checkpoint_path = config[\"trained_checkpoint\"]\n",
    "        else:\n",
    "            checkpoint_path = (config[\"default_checkpoint\"] if config[\"predict_only\"]\n",
    "                               else config[\"trained_checkpoint\"])\n",
    "        root = tk.Tk()\n",
    "        app = TransformerGUI(root, config[\"h5_path\"], checkpoint_path, config)\n",
    "        root.mainloop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageTk\n",
    "import tkinter as tk\n",
    "from tkinter import ttk, messagebox\n",
    "import random\n",
    "import threading\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt  # For plotting loss\n",
    "\n",
    "# ----------------------------\n",
    "# Global Device\n",
    "# ----------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ----------------------------\n",
    "# Custom ProbSparse Attention Function\n",
    "# ----------------------------\n",
    "def prob_sparse_attention(query, key, value, top_k_ratio):\n",
    "    \"\"\"\n",
    "    A simplified version of ProbSparse attention.\n",
    "    For each batch element, selects the top_k queries (based on L2 norm)\n",
    "    and computes standard attention for them; for other positions, uses the average.\n",
    "    Input shapes: query, key, value: [L, B, d]\n",
    "    top_k_ratio: fraction of queries to use.\n",
    "    \"\"\"\n",
    "    L, B, d = query.shape\n",
    "    top_k = max(1, int(L * top_k_ratio))\n",
    "    scores = torch.norm(query, p=2, dim=-1)  # [L, B]\n",
    "    default = value.mean(dim=0, keepdim=True)  # [1, B, d]\n",
    "    out = default.expand(L, B, d).clone()\n",
    "    for b in range(B):\n",
    "        indices = scores[:, b].topk(top_k, sorted=False).indices  # [top_k]\n",
    "        q = query[indices, b, :].unsqueeze(1)  # [top_k, 1, d]\n",
    "        k = key[:, b, :].unsqueeze(0)           # [1, L, d]\n",
    "        v = value[:, b, :].unsqueeze(0)         # [1, L, d]\n",
    "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / np.sqrt(d)  # [top_k, L]\n",
    "        attn = torch.softmax(attn_scores, dim=-1)  # [top_k, L]\n",
    "        out_val = torch.matmul(attn, v.squeeze(0))   # [top_k, d]\n",
    "        out[indices, b, :] = out_val\n",
    "    return out\n",
    "\n",
    "# ----------------------------\n",
    "# Transformer Model Definition\n",
    "# ----------------------------\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.sparse = False\n",
    "        self.top_k_ratio = 0.5\n",
    "    def forward(self, src, src_mask=None):\n",
    "        if self.sparse:\n",
    "            src2 = prob_sparse_attention(src, src, src, self.top_k_ratio)\n",
    "        else:\n",
    "            src2, _ = self.self_attn(src, src, src, attn_mask=src_mask)\n",
    "        src = src + self.dropout1(src2)\n",
    "        src = self.norm1(src)\n",
    "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
    "        src = src + self.dropout2(src2)\n",
    "        src = self.norm2(src)\n",
    "        return src\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)\n",
    "        self.cross_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "        self.activation = nn.ReLU()\n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None):\n",
    "        tgt2, _ = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask)\n",
    "        tgt = tgt + self.dropout1(tgt2)\n",
    "        tgt = self.norm1(tgt)\n",
    "        tgt2, _ = self.cross_attn(tgt, memory, memory, attn_mask=memory_mask)\n",
    "        tgt = tgt + self.dropout2(tgt2)\n",
    "        tgt = self.norm2(tgt)\n",
    "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n",
    "        tgt = tgt + self.dropout3(tgt2)\n",
    "        tgt = self.norm3(tgt)\n",
    "        return tgt\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, input_size, d_model=256, n_heads=8, d_ff=512, num_layers=3, dropout=0.1, sparse_attention=False, top_k_ratio=0.5):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.input_projection = nn.Linear(input_size, d_model)\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        for layer in self.layers:\n",
    "            layer.sparse = sparse_attention\n",
    "            layer.top_k_ratio = top_k_ratio\n",
    "    def forward(self, src, src_mask=None):\n",
    "        src = self.input_projection(src)\n",
    "        src = self.dropout(src)\n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask)\n",
    "        return src\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, d_model=256, n_heads=8, d_ff=512, num_layers=3, dropout=0.1, output_size=172800):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.target_projection = nn.Linear(output_size, d_model)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.output_layer = nn.Linear(d_model, output_size)\n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None):\n",
    "        tgt = self.target_projection(tgt)\n",
    "        for layer in self.layers:\n",
    "            tgt = layer(tgt, memory, tgt_mask, memory_mask)\n",
    "        output = self.output_layer(tgt)\n",
    "        return output\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_size=43200, d_model=256, n_heads=8, d_ff=512, num_layers=3, dropout=0.1, output_size=172800, sparse_attention=False, top_k_ratio=0.5):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = TransformerEncoder(input_size, d_model, n_heads, d_ff, num_layers, dropout, sparse_attention, top_k_ratio)\n",
    "        self.decoder = TransformerDecoder(d_model, n_heads, d_ff, num_layers, dropout, output_size)\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None, memory_mask=None):\n",
    "        memory = self.encoder(src, src_mask)\n",
    "        decoder_output = self.decoder(tgt, memory, tgt_mask, memory_mask)\n",
    "        return decoder_output\n",
    "\n",
    "# ----------------------------\n",
    "# Utility Functions\n",
    "# ----------------------------\n",
    "def load_model(checkpoint_path):\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        messagebox.showerror(\"Error\", f\"Checkpoint file not found at {checkpoint_path}\")\n",
    "        return None\n",
    "    model = Transformer().to(device)\n",
    "    try:\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.eval()\n",
    "        print(f\"Model loaded successfully from {checkpoint_path}\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        messagebox.showerror(\"Error\", f\"Failed to load model: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_h5_dataset(h5_path):\n",
    "    if not os.path.exists(h5_path):\n",
    "        messagebox.showerror(\"Error\", f\"H5 file not found at {h5_path}\")\n",
    "        return None\n",
    "    try:\n",
    "        h5_file = h5py.File(h5_path, 'r')\n",
    "        inputs = h5_file['inputs_original']    # [N, 3, 90, 160]\n",
    "        outputs = h5_file['outputs_scaled']      # [N, 3, 180, 320]\n",
    "        print(f\"H5 dataset loaded successfully from {h5_path}\")\n",
    "        return inputs, outputs\n",
    "    except Exception as e:\n",
    "        messagebox.showerror(\"Error\", f\"Failed to load H5 dataset: {e}\")\n",
    "        return None\n",
    "\n",
    "def preprocess_input(input_frame):\n",
    "    input_frame = np.squeeze(input_frame)\n",
    "    if input_frame.ndim == 2:\n",
    "        input_frame = np.expand_dims(input_frame, axis=-1)\n",
    "    elif input_frame.ndim == 3 and input_frame.shape[0] in {1, 3}:\n",
    "        input_frame = np.transpose(input_frame, (1, 2, 0))\n",
    "    if np.issubdtype(input_frame.dtype, np.floating) and input_frame.max() <= 1:\n",
    "        input_arr = (input_frame * 255).astype(np.uint8)\n",
    "    else:\n",
    "        input_arr = input_frame.astype(np.uint8)\n",
    "    input_pil = Image.fromarray(input_arr)\n",
    "    transform = transforms.ToTensor()\n",
    "    input_tensor = transform(input_pil).to(device)\n",
    "    input_tensor = input_tensor.view(1, -1)  # [1, 43200]\n",
    "    return input_tensor\n",
    "\n",
    "def preprocess_target(target_frame):\n",
    "    target_frame = np.squeeze(target_frame)\n",
    "    if target_frame.ndim == 2:\n",
    "        target_frame = np.expand_dims(target_frame, axis=-1)\n",
    "    elif target_frame.ndim == 3 and target_frame.shape[0] in {1, 3}:\n",
    "        target_frame = np.transpose(target_frame, (1, 2, 0))\n",
    "    if np.issubdtype(target_frame.dtype, np.floating) and target_frame.max() <= 1:\n",
    "        target_arr = (target_frame * 255).astype(np.uint8)\n",
    "    else:\n",
    "        target_arr = target_frame.astype(np.uint8)\n",
    "    target_pil = Image.fromarray(target_arr)\n",
    "    transform = transforms.ToTensor()\n",
    "    target_tensor = transform(target_pil).to(device)\n",
    "    target_tensor = target_tensor.view(1, -1)  # [1, 172800]\n",
    "    return target_tensor\n",
    "\n",
    "def postprocess_output(output_tensor):\n",
    "    output_tensor = output_tensor.squeeze(0)  # [batch, 172800]\n",
    "    output_tensor = output_tensor.cpu().detach().numpy()\n",
    "    expected_size = 3 * 180 * 320\n",
    "    if output_tensor.shape[1] != expected_size:\n",
    "        raise ValueError(f\"Unexpected output size: {output_tensor.shape[1]}. Expected {expected_size}.\")\n",
    "    output_frame = output_tensor[0].reshape(3, 180, 320)\n",
    "    output_frame = np.clip(output_frame, 0, 1)\n",
    "    output_frame = np.transpose(output_frame, (1, 2, 0))\n",
    "    output_pil = Image.fromarray((output_frame * 255).astype(np.uint8))\n",
    "    return output_pil\n",
    "\n",
    "# ----------------------------\n",
    "# PyTorch Dataset for Training\n",
    "# ----------------------------\n",
    "class H5FramePairDataset(Dataset):\n",
    "    def __init__(self, h5_path):\n",
    "        if not os.path.exists(h5_path):\n",
    "            raise FileNotFoundError(f\"H5 file not found at {h5_path}\")\n",
    "        self.h5_file = h5py.File(h5_path, 'r')\n",
    "        self.inputs = self.h5_file['inputs_original']   # [N, 3, 90, 160]\n",
    "        self.outputs = self.h5_file['outputs_scaled']     # [N, 3, 180, 320]\n",
    "    def __len__(self):\n",
    "        return self.inputs.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        input_frame = self.inputs[idx]\n",
    "        target_frame = self.outputs[idx]\n",
    "        input_tensor = preprocess_input(input_frame)    # [1, 43200]\n",
    "        target_tensor = preprocess_target(target_frame)   # [1, 172800]\n",
    "        return input_tensor, target_tensor\n",
    "\n",
    "# ----------------------------\n",
    "# Training Functionality\n",
    "# ----------------------------\n",
    "def train_model(h5_path, epochs, batch_size, learning_rate, save_path, config):\n",
    "    dataset = H5FramePairDataset(h5_path)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    model = Transformer(input_size=config[\"input_size\"],\n",
    "                        output_size=config[\"output_size\"],\n",
    "                        sparse_attention=config[\"sparse_attention\"],\n",
    "                        top_k_ratio=config[\"top_k_ratio\"]).to(device)\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "    loss_history = []\n",
    "    print(\"Starting training...\")\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_loss = 0.0\n",
    "        for i, (src, tgt) in enumerate(dataloader):\n",
    "            src_seq = src.transpose(0, 1)  # [1, batch, 43200]\n",
    "            tgt_seq = tgt.transpose(0, 1)  # [1, batch, 172800]\n",
    "            optimizer.zero_grad()\n",
    "            output = model(src_seq, tgt_seq)  # [1, batch, 172800]\n",
    "            loss = criterion(output, tgt_seq)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        loss_history.append(avg_loss)\n",
    "        print(f\"Epoch [{epoch}/{epochs}], Loss: {avg_loss:.6f}\")\n",
    "    # Plot training loss\n",
    "    plt.figure()\n",
    "    plt.plot(range(1, epochs+1), loss_history, marker='o')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training Loss Over Epochs\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    torch.save({'model_state_dict': model.state_dict()}, save_path)\n",
    "    print(f\"Training complete. Model saved to {save_path}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Utility to compute tensor size in KB\n",
    "# ----------------------------\n",
    "def tensor_size_kb(tensor):\n",
    "    return tensor.element_size() * tensor.nelement() / 1024\n",
    "\n",
    "# ----------------------------\n",
    "# GUI Application for Inference and Play\n",
    "# ----------------------------\n",
    "class TransformerGUI:\n",
    "    def __init__(self, master, h5_path, checkpoint_path, config):\n",
    "        self.config = config\n",
    "        self.master = master\n",
    "        self.master.title(\"Transformer Model Tester\")\n",
    "        self.master.geometry(\"1200x700\")\n",
    "        self.model = load_model(checkpoint_path)\n",
    "        if self.model is None:\n",
    "            self.master.destroy()\n",
    "            return\n",
    "        datasets = load_h5_dataset(h5_path)\n",
    "        if datasets is None:\n",
    "            self.master.destroy()\n",
    "            return\n",
    "        self.inputs, self.outputs = datasets\n",
    "        self.total_samples = self.inputs.shape[0]\n",
    "        self.current_input = None\n",
    "        self.current_output = None\n",
    "        self.playing = False\n",
    "        self.create_widgets()\n",
    "        self.select_random_frame()\n",
    "        \n",
    "    def create_widgets(self):\n",
    "        selection_frame = ttk.Frame(self.master)\n",
    "        selection_frame.pack(pady=10)\n",
    "        self.random_button = ttk.Button(selection_frame, text=\"Select Random Frame\", command=self.select_random_frame)\n",
    "        self.random_button.pack(side=tk.LEFT, padx=5)\n",
    "        self.predict_button = ttk.Button(selection_frame, text=\"Predict Next Frame\", command=self.predict_next_frame)\n",
    "        self.predict_button.pack(side=tk.LEFT, padx=5)\n",
    "        self.play_button = ttk.Button(selection_frame, text=\"Play\", command=self.toggle_play)\n",
    "        self.play_button.pack(side=tk.LEFT, padx=5)\n",
    "        self.predict_button.config(state=tk.DISABLED)\n",
    "        images_frame = ttk.Frame(self.master)\n",
    "        images_frame.pack(pady=10)\n",
    "        input_label = ttk.Label(images_frame, text=\"Input Frame\")\n",
    "        input_label.grid(row=0, column=0, padx=10, pady=5)\n",
    "        self.input_canvas = tk.Canvas(images_frame, width=self.config[\"input_width\"], height=self.config[\"input_height\"])\n",
    "        self.input_canvas.grid(row=1, column=0, padx=10, pady=5)\n",
    "        output_label = ttk.Label(images_frame, text=\"Predicted Output\")\n",
    "        output_label.grid(row=0, column=1, padx=10, pady=5)\n",
    "        self.output_canvas = tk.Canvas(images_frame, width=self.config[\"output_width\"], height=self.config[\"output_height\"])\n",
    "        self.output_canvas.grid(row=1, column=1, padx=10, pady=5)\n",
    "        self.status_var = tk.StringVar()\n",
    "        self.status_var.set(\"Ready\")\n",
    "        self.status_bar = ttk.Label(self.master, textvariable=self.status_var, relief=tk.SUNKEN, anchor=tk.W)\n",
    "        self.status_bar.pack(fill=tk.X, side=tk.BOTTOM, ipady=2)\n",
    "        self.size_var = tk.StringVar()\n",
    "        self.size_var.set(\"Sizes: Input: 0 KB, Latent: 0 KB, Output: 0 KB\")\n",
    "        self.size_label = ttk.Label(self.master, textvariable=self.size_var, relief=tk.RIDGE, anchor=tk.W)\n",
    "        self.size_label.pack(fill=tk.X, side=tk.BOTTOM, ipady=2)\n",
    "        \n",
    "    def select_random_frame(self):\n",
    "        index = random.randint(0, self.total_samples - 1)\n",
    "        input_frame = self.inputs[index]\n",
    "        input_tensor = preprocess_input(input_frame)\n",
    "        self.current_input = input_tensor\n",
    "        threading.Thread(target=self.run_prediction, args=(input_tensor, False)).start()\n",
    "        \n",
    "    def predict_next_frame(self):\n",
    "        if self.current_output is None:\n",
    "            messagebox.showwarning(\"No Prediction\", \"Please select a frame first.\")\n",
    "            return\n",
    "        try:\n",
    "            output_pil = postprocess_output(self.current_output)\n",
    "            scaled_pil = output_pil.resize((self.config[\"input_width\"], self.config[\"input_height\"]))\n",
    "            transform = transforms.ToTensor()\n",
    "            scaled_tensor = transform(scaled_pil).to(device)\n",
    "            scaled_tensor = scaled_tensor.view(1, -1)\n",
    "            self.current_input = scaled_tensor\n",
    "            threading.Thread(target=self.run_prediction, args=(scaled_tensor, True)).start()\n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Prediction Error\", f\"An error occurred: {e}\")\n",
    "    \n",
    "    def run_prediction(self, input_tensor, is_recursive=False):\n",
    "        try:\n",
    "            self.status_var.set(\"Predicting...\")\n",
    "            self.master.update_idletasks()\n",
    "            src = input_tensor.unsqueeze(0)  # [1, 1, INPUT_SIZE]\n",
    "            if not is_recursive:\n",
    "                tgt = torch.zeros(1, 1, self.config[\"output_size\"]).to(device)\n",
    "            else:\n",
    "                tgt = self.current_output\n",
    "            with torch.no_grad():\n",
    "                src_seq = src.transpose(0, 1)  # [1, 1, INPUT_SIZE]\n",
    "                tgt_seq = tgt.transpose(0, 1)  # [1, 1, OUTPUT_SIZE]\n",
    "                # Compute latent space from encoder separately.\n",
    "                memory = self.model.encoder(src_seq)\n",
    "                output_tensor = self.model.decoder(tgt_seq, memory)\n",
    "            self.current_output = output_tensor\n",
    "            output_pil = postprocess_output(output_tensor)\n",
    "            input_pil = self.tensor_to_pil(input_tensor)\n",
    "            self.display_images(input_pil, output_pil)\n",
    "            self.predict_button.config(state=tk.NORMAL)\n",
    "            self.status_var.set(\"Prediction complete.\")\n",
    "            self.update_size_info(input_tensor, memory, output_tensor)\n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Prediction Error\", f\"An error occurred: {e}\")\n",
    "            self.status_var.set(\"Prediction failed.\")\n",
    "    \n",
    "    def update_size_info(self, input_tensor, memory_tensor, output_tensor):\n",
    "        in_size = tensor_size_kb(input_tensor)\n",
    "        mem_size = tensor_size_kb(memory_tensor)\n",
    "        out_size = tensor_size_kb(output_tensor)\n",
    "        self.size_var.set(f\"Sizes: Input: {in_size:.1f} KB, Latent: {mem_size:.1f} KB, Output: {out_size:.1f} KB\")\n",
    "    \n",
    "    def tensor_to_pil(self, tensor):\n",
    "        tensor = tensor.squeeze(0)  # [INPUT_SIZE]\n",
    "        tensor = tensor.view(3, self.config[\"input_height\"], self.config[\"input_width\"])\n",
    "        tensor = tensor.cpu().detach().numpy()\n",
    "        tensor = np.clip(tensor, 0, 1)\n",
    "        tensor = np.transpose(tensor, (1, 2, 0))\n",
    "        return Image.fromarray((tensor * 255).astype(np.uint8))\n",
    "    \n",
    "    def display_images(self, input_pil, output_pil):\n",
    "        input_tk = ImageTk.PhotoImage(input_pil)\n",
    "        output_tk = ImageTk.PhotoImage(output_pil)\n",
    "        self.input_image = input_tk\n",
    "        self.output_image = output_tk\n",
    "        self.input_canvas.create_image(0, 0, anchor=tk.NW, image=self.input_image)\n",
    "        self.output_canvas.create_image(0, 0, anchor=tk.NW, image=self.output_image)\n",
    "    \n",
    "    def toggle_play(self):\n",
    "        self.playing = not self.playing\n",
    "        if self.playing:\n",
    "            self.play_button.config(text=\"Stop\")\n",
    "            self.play_loop()\n",
    "        else:\n",
    "            self.play_button.config(text=\"Play\")\n",
    "    \n",
    "    def play_loop(self):\n",
    "        if not self.playing:\n",
    "            return\n",
    "        self.predict_next_frame()\n",
    "        delay = int(1000 / self.config[\"fps\"])\n",
    "        self.master.after(delay, self.play_loop)\n",
    "\n",
    "def tensor_size_kb(tensor):\n",
    "    return tensor.element_size() * tensor.nelement() / 1024\n",
    "\n",
    "# ----------------------------\n",
    "# Main Execution\n",
    "# ----------------------------\n",
    "def main():\n",
    "    # Configuration dictionary (the only config)\n",
    "    config = {\n",
    "        \"h5_path\": os.path.join(os.getcwd(), \"frame_pairs.h5\"),\n",
    "        \"default_checkpoint\": os.path.join(os.getcwd(), \"checkpoints\", \"Transformer_epoch_50.pth\"),\n",
    "        \"trained_checkpoint\": os.path.join(os.getcwd(), \"checkpoints\", \"Transformer_trained.pth\"),\n",
    "        \"epochs\": 10,\n",
    "        \"batch_size\": 16,\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"mode\": \"train\",  # \"train\" or \"gui\"\n",
    "        \"predict_only\": False,\n",
    "        \"input_height\": 90,\n",
    "        \"input_width\": 160,\n",
    "        \"output_height\": 180,\n",
    "        \"output_width\": 320,\n",
    "        \"input_size\": 43200,    # 3*90*160\n",
    "        \"output_size\": 172800,  # 3*180*320\n",
    "        \"fps\": 2,              # frames per second in play mode\n",
    "        \"sparse_attention\": False,  # For this transformer version, standard attention is used.\n",
    "        \"top_k_ratio\": 0.3\n",
    "    }\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description=\"Transformer Model: Train or Test via GUI\")\n",
    "    parser.add_argument(\"--mode\", type=str, default=config[\"mode\"], choices=[\"gui\", \"train\"],\n",
    "                        help=\"Run mode: 'gui' for testing, 'train' for training.\")\n",
    "    parser.add_argument(\"--predict_only\", action=\"store_true\",\n",
    "                        help=\"If set, skip training and load the default checkpoint for prediction.\")\n",
    "    parser.add_argument(\"--epochs\", type=int, default=config[\"epochs\"], help=\"Number of training epochs.\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=config[\"batch_size\"], help=\"Batch size for training.\")\n",
    "    parser.add_argument(\"--lr\", type=float, default=config[\"learning_rate\"], help=\"Learning rate.\")\n",
    "    args, _ = parser.parse_known_args()\n",
    "    \n",
    "    config[\"mode\"] = args.mode\n",
    "    config[\"predict_only\"] = args.predict_only\n",
    "    config[\"epochs\"] = args.epochs\n",
    "    config[\"batch_size\"] = args.batch_size\n",
    "    config[\"learning_rate\"] = args.lr\n",
    "    \n",
    "    global INPUT_HEIGHT, INPUT_WIDTH, OUTPUT_HEIGHT, OUTPUT_WIDTH, INPUT_SIZE, OUTPUT_SIZE\n",
    "    INPUT_HEIGHT = config[\"input_height\"]\n",
    "    INPUT_WIDTH = config[\"input_width\"]\n",
    "    OUTPUT_HEIGHT = config[\"output_height\"]\n",
    "    OUTPUT_WIDTH = config[\"output_width\"]\n",
    "    INPUT_SIZE = config[\"input_size\"]\n",
    "    OUTPUT_SIZE = config[\"output_size\"]\n",
    "    \n",
    "    if config[\"mode\"] == \"train\":\n",
    "        train_model(config[\"h5_path\"], epochs=config[\"epochs\"],\n",
    "                    batch_size=config[\"batch_size\"], learning_rate=config[\"learning_rate\"],\n",
    "                    save_path=config[\"trained_checkpoint\"], config=config)\n",
    "    else:\n",
    "        if not config[\"predict_only\"] and not os.path.exists(config[\"trained_checkpoint\"]):\n",
    "            print(\"No trained checkpoint found. Training the model first...\")\n",
    "            train_model(config[\"h5_path\"], epochs=config[\"epochs\"],\n",
    "                        batch_size=config[\"batch_size\"], learning_rate=config[\"learning_rate\"],\n",
    "                        save_path=config[\"trained_checkpoint\"], config=config)\n",
    "            checkpoint_path = config[\"trained_checkpoint\"]\n",
    "        else:\n",
    "            checkpoint_path = (config[\"default_checkpoint\"] if config[\"predict_only\"]\n",
    "                               else config[\"trained_checkpoint\"])\n",
    "        root = tk.Tk()\n",
    "        app = TransformerGUI(root, config[\"h5_path\"], checkpoint_path, config)\n",
    "        root.mainloop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageTk\n",
    "import tkinter as tk\n",
    "from tkinter import ttk, messagebox\n",
    "import random\n",
    "import threading\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt  # For plotting loss after training\n",
    "\n",
    "# ----------------------------\n",
    "# Global Device\n",
    "# ----------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ----------------------------\n",
    "# Custom ProbSparse Attention Function\n",
    "# ----------------------------\n",
    "def prob_sparse_attention(query, key, value, top_k_ratio):\n",
    "    \"\"\"\n",
    "    A simplified version of ProbSparse attention.\n",
    "    For each batch element, selects the top_k queries (based on L2 norm)\n",
    "    and computes standard attention for them; for other positions, uses the average.\n",
    "    Input shapes: query, key, value: [L, B, d]\n",
    "    top_k_ratio: float in (0,1] indicating fraction of queries to use.\n",
    "    \"\"\"\n",
    "    L, B, d = query.shape\n",
    "    top_k = max(1, int(L * top_k_ratio))\n",
    "    # Compute L2 norm per query vector.\n",
    "    scores = torch.norm(query, p=2, dim=-1)  # [L, B]\n",
    "    # Create output tensor filled with the average value.\n",
    "    default = value.mean(dim=0, keepdim=True)  # [1, B, d]\n",
    "    out = default.expand(L, B, d).clone()\n",
    "    for b in range(B):\n",
    "        indices = scores[:, b].topk(top_k, sorted=False).indices  # [top_k]\n",
    "        q = query[indices, b, :].unsqueeze(1)  # [top_k, 1, d]\n",
    "        k = key[:, b, :].unsqueeze(0)           # [1, L, d]\n",
    "        v = value[:, b, :].unsqueeze(0)         # [1, L, d]\n",
    "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / np.sqrt(d)  # [top_k, L]\n",
    "        attn = torch.softmax(attn_scores, dim=-1)  # [top_k, L]\n",
    "        out_val = torch.matmul(attn, v.squeeze(0))   # [top_k, d]\n",
    "        out[indices, b, :] = out_val\n",
    "    return out\n",
    "\n",
    "# ----------------------------\n",
    "# Informer Model Definition\n",
    "# ----------------------------\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        # Standard attention (if sparse attention is not enabled)\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.activation = nn.ReLU()\n",
    "        # Sparse attention flag and top_k_ratio (set externally)\n",
    "        self.sparse = False\n",
    "        self.top_k_ratio = 0.5\n",
    "    def forward(self, src, src_mask=None):\n",
    "        if self.sparse:\n",
    "            src2 = prob_sparse_attention(src, src, src, self.top_k_ratio)\n",
    "        else:\n",
    "            src2, _ = self.self_attn(src, src, src, attn_mask=src_mask)\n",
    "        src = src + self.dropout1(src2)\n",
    "        src = self.norm1(src)\n",
    "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
    "        src = src + self.dropout2(src2)\n",
    "        src = self.norm2(src)\n",
    "        return src\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)\n",
    "        self.cross_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "        self.activation = nn.ReLU()\n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None):\n",
    "        tgt2, _ = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask)\n",
    "        tgt = tgt + self.dropout1(tgt2)\n",
    "        tgt = self.norm1(tgt)\n",
    "        tgt2, _ = self.cross_attn(tgt, memory, memory, attn_mask=memory_mask)\n",
    "        tgt = tgt + self.dropout2(tgt2)\n",
    "        tgt = self.norm2(tgt)\n",
    "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n",
    "        tgt = tgt + self.dropout3(tgt2)\n",
    "        tgt = self.norm3(tgt)\n",
    "        return tgt\n",
    "\n",
    "class InformerEncoder(nn.Module):\n",
    "    def __init__(self, input_size, d_model=256, n_heads=8, d_ff=512, num_layers=3, dropout=0.1, sparse_attention=False, top_k_ratio=0.5):\n",
    "        super(InformerEncoder, self).__init__()\n",
    "        self.input_projection = nn.Linear(input_size, d_model)\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        for layer in self.layers:\n",
    "            layer.sparse = sparse_attention\n",
    "            layer.top_k_ratio = top_k_ratio\n",
    "    def forward(self, src, src_mask=None):\n",
    "        src = self.input_projection(src)\n",
    "        src = self.dropout(src)\n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask)\n",
    "        return src\n",
    "\n",
    "class InformerDecoder(nn.Module):\n",
    "    def __init__(self, d_model=256, n_heads=8, d_ff=512, num_layers=3, dropout=0.1, output_size=172800):\n",
    "        super(InformerDecoder, self).__init__()\n",
    "        self.target_projection = nn.Linear(output_size, d_model)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.output_layer = nn.Linear(d_model, output_size)\n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None):\n",
    "        tgt = self.target_projection(tgt)\n",
    "        for layer in self.layers:\n",
    "            tgt = layer(tgt, memory, tgt_mask, memory_mask)\n",
    "        output = self.output_layer(tgt)\n",
    "        return output\n",
    "\n",
    "class Informer(nn.Module):\n",
    "    def __init__(self, input_size=43200, d_model=256, n_heads=8, d_ff=512, num_layers=3, dropout=0.1, output_size=172800, sparse_attention=False, top_k_ratio=0.5):\n",
    "        super(Informer, self).__init__()\n",
    "        self.encoder = InformerEncoder(input_size, d_model, n_heads, d_ff, num_layers, dropout, sparse_attention, top_k_ratio)\n",
    "        self.decoder = InformerDecoder(d_model, n_heads, d_ff, num_layers, dropout, output_size)\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None, memory_mask=None):\n",
    "        memory = self.encoder(src, src_mask)\n",
    "        decoder_output = self.decoder(tgt, memory, tgt_mask, memory_mask)\n",
    "        return decoder_output\n",
    "\n",
    "# ----------------------------\n",
    "# Utility Functions\n",
    "# ----------------------------\n",
    "def load_model(checkpoint_path):\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        messagebox.showerror(\"Error\", f\"Checkpoint file not found at {checkpoint_path}\")\n",
    "        return None\n",
    "    model = Informer().to(device)\n",
    "    try:\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.eval()\n",
    "        print(f\"Model loaded successfully from {checkpoint_path}\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        messagebox.showerror(\"Error\", f\"Failed to load model: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_h5_dataset(h5_path):\n",
    "    if not os.path.exists(h5_path):\n",
    "        messagebox.showerror(\"Error\", f\"H5 file not found at {h5_path}\")\n",
    "        return None\n",
    "    try:\n",
    "        h5_file = h5py.File(h5_path, 'r')\n",
    "        inputs = h5_file['inputs_original']    # [N, 3, 90, 160]\n",
    "        outputs = h5_file['outputs_scaled']      # [N, 3, 180, 320]\n",
    "        print(f\"H5 dataset loaded successfully from {h5_path}\")\n",
    "        return inputs, outputs\n",
    "    except Exception as e:\n",
    "        messagebox.showerror(\"Error\", f\"Failed to load H5 dataset: {e}\")\n",
    "        return None\n",
    "\n",
    "def preprocess_input(input_frame):\n",
    "    input_frame = np.squeeze(input_frame)\n",
    "    if input_frame.ndim == 2:\n",
    "        input_frame = np.expand_dims(input_frame, axis=-1)\n",
    "    elif input_frame.ndim == 3 and input_frame.shape[0] in {1, 3}:\n",
    "        input_frame = np.transpose(input_frame, (1, 2, 0))\n",
    "    if np.issubdtype(input_frame.dtype, np.floating) and input_frame.max() <= 1:\n",
    "        input_arr = (input_frame * 255).astype(np.uint8)\n",
    "    else:\n",
    "        input_arr = input_frame.astype(np.uint8)\n",
    "    input_pil = Image.fromarray(input_arr)\n",
    "    transform = transforms.ToTensor()\n",
    "    input_tensor = transform(input_pil).to(device)  # [C, H, W]\n",
    "    input_tensor = input_tensor.view(1, -1)          # [1, 43200]\n",
    "    return input_tensor\n",
    "\n",
    "def preprocess_target(target_frame):\n",
    "    target_frame = np.squeeze(target_frame)\n",
    "    if target_frame.ndim == 2:\n",
    "        target_frame = np.expand_dims(target_frame, axis=-1)\n",
    "    elif target_frame.ndim == 3 and target_frame.shape[0] in {1, 3}:\n",
    "        target_frame = np.transpose(target_frame, (1, 2, 0))\n",
    "    if np.issubdtype(target_frame.dtype, np.floating) and target_frame.max() <= 1:\n",
    "        target_arr = (target_frame * 255).astype(np.uint8)\n",
    "    else:\n",
    "        target_arr = target_frame.astype(np.uint8)\n",
    "    target_pil = Image.fromarray(target_arr)\n",
    "    transform = transforms.ToTensor()\n",
    "    target_tensor = transform(target_pil).to(device)  # [C, H, W]\n",
    "    target_tensor = target_tensor.view(1, -1)           # [1, 172800]\n",
    "    return target_tensor\n",
    "\n",
    "def postprocess_output(output_tensor):\n",
    "    output_tensor = output_tensor.squeeze(0)  # [batch, 172800]\n",
    "    output_tensor = output_tensor.cpu().detach().numpy()\n",
    "    expected_size = 3 * 180 * 320\n",
    "    if output_tensor.shape[1] != expected_size:\n",
    "        raise ValueError(f\"Unexpected output size: {output_tensor.shape[1]}. Expected {expected_size}.\")\n",
    "    output_frame = output_tensor[0].reshape(3, 180, 320)\n",
    "    output_frame = np.clip(output_frame, 0, 1)\n",
    "    output_frame = np.transpose(output_frame, (1, 2, 0))\n",
    "    output_pil = Image.fromarray((output_frame * 255).astype(np.uint8))\n",
    "    return output_pil\n",
    "\n",
    "# ----------------------------\n",
    "# PyTorch Dataset for Training\n",
    "# ----------------------------\n",
    "class H5FramePairDataset(Dataset):\n",
    "    def __init__(self, h5_path):\n",
    "        if not os.path.exists(h5_path):\n",
    "            raise FileNotFoundError(f\"H5 file not found at {h5_path}\")\n",
    "        self.h5_file = h5py.File(h5_path, 'r')\n",
    "        self.inputs = self.h5_file['inputs_original']   # [N, 3, 90, 160]\n",
    "        self.outputs = self.h5_file['outputs_scaled']     # [N, 3, 180, 320]\n",
    "    def __len__(self):\n",
    "        return self.inputs.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        input_frame = self.inputs[idx]\n",
    "        target_frame = self.outputs[idx]\n",
    "        input_tensor = preprocess_input(input_frame)    # [1, 43200]\n",
    "        target_tensor = preprocess_target(target_frame)   # [1, 172800]\n",
    "        return input_tensor, target_tensor\n",
    "\n",
    "# ----------------------------\n",
    "# Training Functionality\n",
    "# ----------------------------\n",
    "def train_model(h5_path, epochs, batch_size, learning_rate, save_path, config):\n",
    "    dataset = H5FramePairDataset(h5_path)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    # Initialize model with sparse attention options.\n",
    "    model = Informer(input_size=config[\"input_size\"],\n",
    "                     output_size=config[\"output_size\"],\n",
    "                     sparse_attention=config[\"sparse_attention\"],\n",
    "                     top_k_ratio=config[\"top_k_ratio\"]).to(device)\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "    loss_history = []\n",
    "    print(\"Starting training...\")\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_loss = 0.0\n",
    "        for i, (src, tgt) in enumerate(dataloader):\n",
    "            # src: [batch, 1, 43200], tgt: [batch, 1, 172800]\n",
    "            src_seq = src.transpose(0, 1)  # [1, batch, 43200]\n",
    "            tgt_seq = tgt.transpose(0, 1)  # [1, batch, 172800]\n",
    "            optimizer.zero_grad()\n",
    "            output = model(src_seq, tgt_seq)  # [1, batch, 172800]\n",
    "            loss = criterion(output, tgt_seq)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        loss_history.append(avg_loss)\n",
    "        print(f\"Epoch [{epoch}/{epochs}], Loss: {avg_loss:.6f}\")\n",
    "    # Plot training loss\n",
    "    plt.figure()\n",
    "    plt.plot(range(1, epochs+1), loss_history, marker='o')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training Loss Over Epochs\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    torch.save({'model_state_dict': model.state_dict()}, save_path)\n",
    "    print(f\"Training complete. Model saved to {save_path}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Utility to compute tensor size in KB\n",
    "# ----------------------------\n",
    "def tensor_size_kb(tensor):\n",
    "    return tensor.element_size() * tensor.nelement() / 1024\n",
    "\n",
    "# ----------------------------\n",
    "# GUI Application for Inference and Play\n",
    "# ----------------------------\n",
    "class InformerGUI:\n",
    "    def __init__(self, master, h5_path, checkpoint_path, config):\n",
    "        self.config = config\n",
    "        self.master = master\n",
    "        self.master.title(\"Informer Model Tester\")\n",
    "        self.master.geometry(\"1200x700\")\n",
    "        self.model = load_model(checkpoint_path)\n",
    "        if self.model is None:\n",
    "            self.master.destroy()\n",
    "            return\n",
    "        datasets = load_h5_dataset(h5_path)\n",
    "        if datasets is None:\n",
    "            self.master.destroy()\n",
    "            return\n",
    "        self.inputs, self.outputs = datasets\n",
    "        self.total_samples = self.inputs.shape[0]\n",
    "        self.current_input = None\n",
    "        self.current_output = None\n",
    "        self.playing = False\n",
    "        self.create_widgets()\n",
    "        self.select_random_frame()\n",
    "        \n",
    "    def create_widgets(self):\n",
    "        selection_frame = ttk.Frame(self.master)\n",
    "        selection_frame.pack(pady=10)\n",
    "        self.random_button = ttk.Button(selection_frame, text=\"Select Random Frame\", command=self.select_random_frame)\n",
    "        self.random_button.pack(side=tk.LEFT, padx=5)\n",
    "        self.predict_button = ttk.Button(selection_frame, text=\"Predict Next Frame\", command=self.predict_next_frame)\n",
    "        self.predict_button.pack(side=tk.LEFT, padx=5)\n",
    "        self.play_button = ttk.Button(selection_frame, text=\"Play\", command=self.toggle_play)\n",
    "        self.play_button.pack(side=tk.LEFT, padx=5)\n",
    "        self.predict_button.config(state=tk.DISABLED)\n",
    "        images_frame = ttk.Frame(self.master)\n",
    "        images_frame.pack(pady=10)\n",
    "        input_label = ttk.Label(images_frame, text=\"Input Frame\")\n",
    "        input_label.grid(row=0, column=0, padx=10, pady=5)\n",
    "        self.input_canvas = tk.Canvas(images_frame, width=self.config[\"input_width\"], height=self.config[\"input_height\"])\n",
    "        self.input_canvas.grid(row=1, column=0, padx=10, pady=5)\n",
    "        output_label = ttk.Label(images_frame, text=\"Predicted Output\")\n",
    "        output_label.grid(row=0, column=1, padx=10, pady=5)\n",
    "        self.output_canvas = tk.Canvas(images_frame, width=self.config[\"output_width\"], height=self.config[\"output_height\"])\n",
    "        self.output_canvas.grid(row=1, column=1, padx=10, pady=5)\n",
    "        self.status_var = tk.StringVar()\n",
    "        self.status_var.set(\"Ready\")\n",
    "        self.status_bar = ttk.Label(self.master, textvariable=self.status_var, relief=tk.SUNKEN, anchor=tk.W)\n",
    "        self.status_bar.pack(fill=tk.X, side=tk.BOTTOM, ipady=2)\n",
    "        self.size_var = tk.StringVar()\n",
    "        self.size_var.set(\"Sizes: Input: 0 KB, Latent: 0 KB, Output: 0 KB\")\n",
    "        self.size_label = ttk.Label(self.master, textvariable=self.size_var, relief=tk.RIDGE, anchor=tk.W)\n",
    "        self.size_label.pack(fill=tk.X, side=tk.BOTTOM, ipady=2)\n",
    "        \n",
    "    def select_random_frame(self):\n",
    "        index = random.randint(0, self.total_samples - 1)\n",
    "        input_frame = self.inputs[index]\n",
    "        input_tensor = preprocess_input(input_frame)\n",
    "        self.current_input = input_tensor\n",
    "        threading.Thread(target=self.run_prediction, args=(input_tensor, False)).start()\n",
    "        \n",
    "    def predict_next_frame(self):\n",
    "        if self.current_output is None:\n",
    "            messagebox.showwarning(\"No Prediction\", \"Please select a frame first.\")\n",
    "            return\n",
    "        try:\n",
    "            output_pil = postprocess_output(self.current_output)\n",
    "            scaled_pil = output_pil.resize((self.config[\"input_width\"], self.config[\"input_height\"]))\n",
    "            transform = transforms.ToTensor()\n",
    "            scaled_tensor = transform(scaled_pil).to(device)\n",
    "            scaled_tensor = scaled_tensor.view(1, -1)\n",
    "            self.current_input = scaled_tensor\n",
    "            threading.Thread(target=self.run_prediction, args=(scaled_tensor, True)).start()\n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Prediction Error\", f\"An error occurred: {e}\")\n",
    "\n",
    "    def run_prediction(self, input_tensor, is_recursive=False):\n",
    "        try:\n",
    "            self.status_var.set(\"Predicting...\")\n",
    "            self.master.update_idletasks()\n",
    "            src = input_tensor.unsqueeze(0)  # [1, 1, INPUT_SIZE]\n",
    "            if not is_recursive:\n",
    "                tgt = torch.zeros(1, 1, self.config[\"output_size\"]).to(device)\n",
    "            else:\n",
    "                tgt = self.current_output\n",
    "            with torch.no_grad():\n",
    "                src_seq = src.transpose(0, 1)  # [1, 1, INPUT_SIZE]\n",
    "                tgt_seq = tgt.transpose(0, 1)  # [1, 1, OUTPUT_SIZE]\n",
    "                # Compute latent space from encoder separately.\n",
    "                memory = self.model.encoder(src_seq)\n",
    "                output_tensor = self.model.decoder(tgt_seq, memory)\n",
    "            self.current_output = output_tensor\n",
    "            output_pil = postprocess_output(output_tensor)\n",
    "            input_pil = self.tensor_to_pil(input_tensor)\n",
    "            self.display_images(input_pil, output_pil)\n",
    "            self.predict_button.config(state=tk.NORMAL)\n",
    "            self.status_var.set(\"Prediction complete.\")\n",
    "            self.update_size_info(input_tensor, memory, output_tensor)\n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Prediction Error\", f\"An error occurred: {e}\")\n",
    "            self.status_var.set(\"Prediction failed.\")\n",
    "\n",
    "    def update_size_info(self, input_tensor, memory_tensor, output_tensor):\n",
    "        in_size = tensor_size_kb(input_tensor)\n",
    "        mem_size = tensor_size_kb(memory_tensor)\n",
    "        out_size = tensor_size_kb(output_tensor)\n",
    "        self.size_var.set(f\"Sizes: Input: {in_size:.1f} KB, Latent: {mem_size:.1f} KB, Output: {out_size:.1f} KB\")\n",
    "\n",
    "    def tensor_to_pil(self, tensor):\n",
    "        tensor = tensor.squeeze(0)  # [INPUT_SIZE]\n",
    "        tensor = tensor.view(3, self.config[\"input_height\"], self.config[\"input_width\"])\n",
    "        tensor = tensor.cpu().detach().numpy()\n",
    "        tensor = np.clip(tensor, 0, 1)\n",
    "        tensor = np.transpose(tensor, (1, 2, 0))\n",
    "        return Image.fromarray((tensor * 255).astype(np.uint8))\n",
    "\n",
    "    def display_images(self, input_pil, output_pil):\n",
    "        input_tk = ImageTk.PhotoImage(input_pil)\n",
    "        output_tk = ImageTk.PhotoImage(output_pil)\n",
    "        self.input_image = input_tk\n",
    "        self.output_image = output_tk\n",
    "        self.input_canvas.create_image(0, 0, anchor=tk.NW, image=self.input_image)\n",
    "        self.output_canvas.create_image(0, 0, anchor=tk.NW, image=self.output_image)\n",
    "        \n",
    "    def toggle_play(self):\n",
    "        self.playing = not self.playing\n",
    "        if self.playing:\n",
    "            self.play_button.config(text=\"Stop\")\n",
    "            self.play_loop()\n",
    "        else:\n",
    "            self.play_button.config(text=\"Play\")\n",
    "            \n",
    "    def play_loop(self):\n",
    "        if not self.playing:\n",
    "            return\n",
    "        self.predict_next_frame()\n",
    "        delay = int(1000 / self.config[\"fps\"])\n",
    "        self.master.after(delay, self.play_loop)\n",
    "\n",
    "def tensor_size_kb(tensor):\n",
    "    return tensor.element_size() * tensor.nelement() / 1024\n",
    "\n",
    "# ----------------------------\n",
    "# Main Execution\n",
    "# ----------------------------\n",
    "def main():\n",
    "    # Configuration dictionary (the only config)\n",
    "    config = {\n",
    "        \"h5_path\": os.path.join(os.getcwd(), \"frame_pairs.h5\"),\n",
    "        \"default_checkpoint\": os.path.join(os.getcwd(), \"checkpoints\", \"informer_epoch_50.pth\"),\n",
    "        \"trained_checkpoint\": os.path.join(os.getcwd(), \"checkpoints\", \"informer_trained.pth\"),\n",
    "        \"epochs\": 1,\n",
    "        \"batch_size\": 16,\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"mode\": \"train\",  # \"train\" or \"gui\"\n",
    "        \"predict_only\": False,\n",
    "        \"input_height\": 90,\n",
    "        \"input_width\": 160,\n",
    "        \"output_height\": 180,\n",
    "        \"output_width\": 320,\n",
    "        \"input_size\": 43200,    # 3*90*160\n",
    "        \"output_size\": 172800,  # 3*180*320\n",
    "        \"fps\": 2,              # frames per second in play mode\n",
    "        \"sparse_attention\": True,  # Enable sparse (ProbSparse) attention in encoder\n",
    "        \"top_k_ratio\": 0.3          # Fraction of queries to attend (e.g., top 30%)\n",
    "    }\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description=\"Informer Model: Train or Test via GUI\")\n",
    "    parser.add_argument(\"--mode\", type=str, default=config[\"mode\"], choices=[\"gui\", \"train\"],\n",
    "                        help=\"Run mode: 'gui' for testing, 'train' for training.\")\n",
    "    parser.add_argument(\"--predict_only\", action=\"store_true\",\n",
    "                        help=\"If set, skip training and load the default checkpoint for prediction.\")\n",
    "    parser.add_argument(\"--epochs\", type=int, default=config[\"epochs\"], help=\"Number of training epochs.\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=config[\"batch_size\"], help=\"Batch size for training.\")\n",
    "    parser.add_argument(\"--lr\", type=float, default=config[\"learning_rate\"], help=\"Learning rate.\")\n",
    "    args, _ = parser.parse_known_args()\n",
    "    \n",
    "    config[\"mode\"] = args.mode\n",
    "    config[\"predict_only\"] = args.predict_only\n",
    "    config[\"epochs\"] = args.epochs\n",
    "    config[\"batch_size\"] = args.batch_size\n",
    "    config[\"learning_rate\"] = args.lr\n",
    "    \n",
    "    global INPUT_HEIGHT, INPUT_WIDTH, OUTPUT_HEIGHT, OUTPUT_WIDTH, INPUT_SIZE, OUTPUT_SIZE\n",
    "    INPUT_HEIGHT = config[\"input_height\"]\n",
    "    INPUT_WIDTH = config[\"input_width\"]\n",
    "    OUTPUT_HEIGHT = config[\"output_height\"]\n",
    "    OUTPUT_WIDTH = config[\"output_width\"]\n",
    "    INPUT_SIZE = config[\"input_size\"]\n",
    "    OUTPUT_SIZE = config[\"output_size\"]\n",
    "    \n",
    "    if config[\"mode\"] == \"train\":\n",
    "        train_model(config[\"h5_path\"], epochs=config[\"epochs\"],\n",
    "                    batch_size=config[\"batch_size\"], learning_rate=config[\"learning_rate\"],\n",
    "                    save_path=config[\"trained_checkpoint\"], config=config)\n",
    "    else:\n",
    "        if not config[\"predict_only\"] and not os.path.exists(config[\"trained_checkpoint\"]):\n",
    "            print(\"No trained checkpoint found. Training the model first...\")\n",
    "            train_model(config[\"h5_path\"], epochs=config[\"epochs\"],\n",
    "                        batch_size=config[\"batch_size\"], learning_rate=config[\"learning_rate\"],\n",
    "                        save_path=config[\"trained_checkpoint\"], config=config)\n",
    "            checkpoint_path = config[\"trained_checkpoint\"]\n",
    "        else:\n",
    "            checkpoint_path = (config[\"default_checkpoint\"] if config[\"predict_only\"]\n",
    "                               else config[\"trained_checkpoint\"])\n",
    "        root = tk.Tk()\n",
    "        app = InformerGUI(root, config[\"h5_path\"], checkpoint_path, config)\n",
    "        root.mainloop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available datasets: ['inputs_original', 'inputs_scaled', 'outputs_original', 'outputs_scaled']\n",
      "Inputs shape: (2426, 3, 90, 160)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x300 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the path to your H5 file (adjust if needed)\n",
    "h5_path = os.path.join(os.getcwd(), \"frame_pairs.h5\")\n",
    "\n",
    "# Open the H5 file and inspect the available datasets\n",
    "with h5py.File(h5_path, 'r') as f:\n",
    "    print(\"Available datasets:\", list(f.keys()))\n",
    "    \n",
    "    # Assuming the original inputs are stored under the key 'inputs_original'\n",
    "    inputs = f['inputs_original']\n",
    "    print(\"Inputs shape:\", inputs.shape)  # Expected shape: (N, 3, 90, 160)\n",
    "    \n",
    "    num_samples = 5  # Number of frames to display\n",
    "    fig, axes = plt.subplots(1, num_samples, figsize=(15, 3))\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Get the i-th frame; shape: (3, 90, 160)\n",
    "        frame = inputs[i]\n",
    "        # Transpose to (90, 160, 3) for display\n",
    "        frame_disp = np.transpose(frame, (1, 2, 0))\n",
    "        \n",
    "        # Check the data type and range:\n",
    "        if np.issubdtype(frame_disp.dtype, np.floating):\n",
    "            # If data is float and maximum value is greater than 1, assume it's in [0,255] and normalize\n",
    "            if frame_disp.max() > 1:\n",
    "                frame_disp = frame_disp / 255.0\n",
    "        else:\n",
    "            # If the data is of an integer type but not uint8, convert it to uint8.\n",
    "            frame_disp = frame_disp.astype(np.uint8)\n",
    "        \n",
    "        axes[i].imshow(frame_disp)\n",
    "        axes[i].axis('off')\n",
    "        axes[i].set_title(f\"Frame {i}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
