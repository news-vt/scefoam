{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 (no detections), 89.1ms\n",
      "Speed: 3.0ms preprocess, 89.1ms inference, 23.9ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.10.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:1301: error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvShowImage'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 41\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Write annotated frame to output video and display it\u001b[39;00m\n\u001b[0;32m     40\u001b[0m out\u001b[38;5;241m.\u001b[39mwrite(im0)\n\u001b[1;32m---> 41\u001b[0m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minstance-segmentation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mim0\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cv2\u001b[38;5;241m.\u001b[39mwaitKey(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m&\u001b[39m \u001b[38;5;241m0xFF\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\keplarV4\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ultralytics\\utils\\patches.py:56\u001b[0m, in \u001b[0;36mimshow\u001b[1;34m(winname, mat)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mimshow\u001b[39m(winname: \u001b[38;5;28mstr\u001b[39m, mat: np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m     49\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03m    Displays an image in the specified window.\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;124;03m        mat (np.ndarray): Image to be shown.\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m     \u001b[43m_imshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwinname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43municode_escape\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmat\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.10.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:1301: error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvShowImage'\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from ultralytics.utils.plotting import Annotator, colors\n",
    "\n",
    "# Load the YOLO segmentation model\n",
    "model = YOLO('yolo11x-seg.pt')\n",
    "names = model.model.names  # Correcting to access the 'names' attribute\n",
    "\n",
    "# Define video path and open video file\n",
    "video_path = os.path.join(os.getcwd(), 'OurPlanet.mp4')\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Retrieve video properties\n",
    "w, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\n",
    "\n",
    "# Define output video writer\n",
    "out = cv2.VideoWriter(\"instance-segmentation.avi\", cv2.VideoWriter_fourcc(*\"MJPG\"), fps, (w, h))\n",
    "\n",
    "# Process each frame in the video\n",
    "while True:\n",
    "    ret, im0 = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Run model prediction on the current frame\n",
    "    results = model.predict(im0)\n",
    "    annotator = Annotator(im0, line_width=2)\n",
    "\n",
    "    # Check if segmentation masks are available\n",
    "    if results[0].masks is not None:\n",
    "        clss = results[0].boxes.cls.cpu().tolist()\n",
    "        masks = results[0].masks.xy\n",
    "        for mask, cls in zip(masks, clss):\n",
    "            # Use 'label' instead of 'det_label'\n",
    "            annotator.seg_bbox(mask=mask, mask_color=colors(int(cls), True), label=names[int(cls)])\n",
    "\n",
    "    # Write annotated frame to output video and display it\n",
    "    out.write(im0)\n",
    "    cv2.imshow(\"instance-segmentation\", im0)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "out.release()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n",
      "Failed to capture video frame\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "import numpy as np\n",
    "from PIL import Image, ImageTk\n",
    "import tkinter as tk\n",
    "from tkinter import Label, Frame\n",
    "\n",
    "# Define the Transformer-based Autoencoder model\n",
    "class TransformerAutoencoder(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, emb_dim=512, num_heads=8, num_layers=6):\n",
    "        super(TransformerAutoencoder, self).__init__()\n",
    "        assert img_size % patch_size == 0, 'Image size must be divisible by patch size'\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        self.emb_dim = emb_dim\n",
    "        self.patch_dim = 3 * patch_size * patch_size  # Assuming RGB images\n",
    "\n",
    "        # Linear projection of flattened patches\n",
    "        self.patch_embedding = nn.Linear(self.patch_dim, emb_dim)\n",
    "        # Positional embedding\n",
    "        self.positional_embedding = nn.Parameter(torch.randn(1, self.num_patches, emb_dim))\n",
    "\n",
    "        # Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=emb_dim, nhead=num_heads, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Transformer Decoder\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=emb_dim, nhead=num_heads, batch_first=True)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Output projection to reconstruct patches\n",
    "        self.output_projection = nn.Linear(emb_dim, self.patch_dim)\n",
    "\n",
    "        # Target mask for causal decoding\n",
    "        self.tgt_mask = nn.Transformer.generate_square_subsequent_mask(self.num_patches).to(\n",
    "            torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        # Divide image into patches\n",
    "        x = x.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size)\n",
    "        # x shape: [batch_size, channels, num_patches_h, num_patches_w, patch_size, patch_size]\n",
    "        x = x.permute(0, 2, 3, 1, 4, 5).contiguous()\n",
    "        # x shape: [batch_size, num_patches_h, num_patches_w, channels, patch_size, patch_size]\n",
    "        x = x.view(batch_size, -1, self.patch_dim)\n",
    "        # x shape: [batch_size, num_patches, patch_dim]\n",
    "\n",
    "        # Patch embeddings\n",
    "        x = self.patch_embedding(x)\n",
    "        # x shape: [batch_size, num_patches, emb_dim]\n",
    "\n",
    "        # Add positional encoding\n",
    "        x = x + self.positional_embedding  # [1, num_patches, emb_dim] broadcasts over batch_size\n",
    "\n",
    "        # Encode patches\n",
    "        memory = self.encoder(x)\n",
    "        # memory shape: [batch_size, num_patches, emb_dim]\n",
    "\n",
    "        # Prepare target sequence for decoder (shifted right)\n",
    "        tgt = torch.zeros_like(x)\n",
    "        tgt[:, 1:, :] = x[:, :-1, :]  # Shift input embeddings to the right\n",
    "        tgt = tgt + self.positional_embedding\n",
    "\n",
    "        # Decode patches\n",
    "        output = self.decoder(tgt, memory, tgt_mask=self.tgt_mask)\n",
    "        # output shape: [batch_size, num_patches, emb_dim]\n",
    "\n",
    "        # Project back to patch dimension\n",
    "        reconstructed_patches = self.output_projection(output)\n",
    "        # reconstructed_patches shape: [batch_size, num_patches, patch_dim]\n",
    "\n",
    "        # Reconstruct image from patches\n",
    "        reconstructed = reconstructed_patches.view(\n",
    "            batch_size,\n",
    "            self.img_size // self.patch_size,\n",
    "            self.img_size // self.patch_size,\n",
    "            3,\n",
    "            self.patch_size,\n",
    "            self.patch_size\n",
    "        )\n",
    "        reconstructed = reconstructed.permute(0, 3, 1, 4, 2, 5).contiguous()\n",
    "        reconstructed = reconstructed.view(batch_size, 3, self.img_size, self.img_size)\n",
    "        reconstructed = torch.sigmoid(reconstructed)  # Apply sigmoid to get pixel values between 0 and 1\n",
    "\n",
    "        return memory, reconstructed\n",
    "\n",
    "# Initialize the model, optimizer, and loss function\n",
    "img_size = 224  # Input image resolution\n",
    "autoencoder = TransformerAutoencoder(\n",
    "    img_size=img_size,\n",
    "    patch_size=16,\n",
    "    emb_dim=512,\n",
    "    num_heads=8,\n",
    "    num_layers=6\n",
    ")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "autoencoder.to(device)\n",
    "\n",
    "optimizer = optim.Adam(autoencoder.parameters(), lr=0.0001)\n",
    "criterion = nn.MSELoss()  # Reconstruction loss\n",
    "\n",
    "# Video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Transform to preprocess the frame\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Initialize Tkinter window\n",
    "window = tk.Tk()\n",
    "window.title(\"Real-time Transformer Autoencoder Visualization\")\n",
    "\n",
    "# Frame for organizing the input video, latent space, and reconstructed video\n",
    "main_frame = Frame(window)\n",
    "main_frame.pack()\n",
    "\n",
    "# Labels for each of the three outputs\n",
    "video_label = Label(main_frame, text=\"Input Video\", font=(\"Arial\", 12, \"bold\"))\n",
    "video_label.grid(row=0, column=0)\n",
    "\n",
    "latent_label = Label(main_frame, text=\"Latent Space\", font=(\"Arial\", 12, \"bold\"))\n",
    "latent_label.grid(row=0, column=1)\n",
    "\n",
    "output_label = Label(main_frame, text=\"Output Video\", font=(\"Arial\", 12, \"bold\"))\n",
    "output_label.grid(row=0, column=2)\n",
    "\n",
    "# Label to display the latent space size\n",
    "latent_size_label = Label(window, text=\"\", font=(\"Arial\", 12))\n",
    "latent_size_label.pack()\n",
    "\n",
    "def update_frame():\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to capture video frame\")\n",
    "        window.after(10, update_frame)\n",
    "        return\n",
    "\n",
    "    # Convert frame to PIL image\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    img_pil = Image.fromarray(rgb_frame)\n",
    "    img_resized = img_pil.resize((img_size, img_size))\n",
    "\n",
    "    # Display original frame\n",
    "    img_tk = ImageTk.PhotoImage(image=img_resized)\n",
    "    video_label.imgtk = img_tk\n",
    "    video_label.configure(image=img_tk)\n",
    "\n",
    "    # Preprocess frame for model input\n",
    "    input_image = transform(img_resized).unsqueeze(0).to(device)  # Shape: [1, 3, H, W]\n",
    "\n",
    "    # Perform a training step\n",
    "    autoencoder.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    latent_space, reconstructed = autoencoder(input_image)  # Forward pass\n",
    "    loss = criterion(reconstructed, input_image)            # Compute loss\n",
    "    loss.backward()                                         # Backpropagation\n",
    "\n",
    "    # Gradient clipping for stability\n",
    "    torch.nn.utils.clip_grad_norm_(autoencoder.parameters(), max_norm=1.0)\n",
    "\n",
    "    optimizer.step()                                        # Update weights\n",
    "\n",
    "    # Display the latent space (mean over embedding dimension)\n",
    "    latent_space_np = latent_space.detach().mean(dim=2).cpu().numpy()  # Shape: [batch_size, num_patches]\n",
    "    # Reshape to image\n",
    "    num_patches_side = img_size // autoencoder.patch_size\n",
    "    latent_image = latent_space_np[0].reshape(num_patches_side, num_patches_side)\n",
    "    # Upscale to match image size\n",
    "    latent_image_resized = cv2.resize(latent_image, (img_size, img_size), interpolation=cv2.INTER_CUBIC)\n",
    "    # Normalize and convert to uint8\n",
    "    latent_image_resized = (255 * (latent_image_resized - latent_image_resized.min()) /\n",
    "                            (latent_image_resized.max() - latent_image_resized.min() + 1e-5)).astype(np.uint8)\n",
    "    latent_img = Image.fromarray(latent_image_resized)\n",
    "    latent_img_tk = ImageTk.PhotoImage(image=latent_img)\n",
    "    latent_label.imgtk = latent_img_tk\n",
    "    latent_label.configure(image=latent_img_tk)\n",
    "\n",
    "    # Display the reconstructed output at input resolution\n",
    "    output_np = reconstructed[0].detach().cpu().numpy().transpose(1, 2, 0)  # Convert to HxWxC\n",
    "    output_np = (255 * output_np).astype(np.uint8)\n",
    "    output_img = Image.fromarray(output_np)\n",
    "    output_img_tk = ImageTk.PhotoImage(image=output_img)\n",
    "    output_label.imgtk = output_img_tk\n",
    "    output_label.configure(image=output_img_tk)\n",
    "\n",
    "    # Update the latent space size label\n",
    "    latent_size = latent_space.size()\n",
    "    latent_size_text = f\"Latent Space Size: {list(latent_size)}\"\n",
    "    latent_size_label.config(text=latent_size_text)\n",
    "\n",
    "    # Schedule the next frame update\n",
    "    window.after(1, update_frame)\n",
    "\n",
    "# Start video feed update loop\n",
    "update_frame()\n",
    "\n",
    "# Run Tkinter main loop\n",
    "window.mainloop()\n",
    "\n",
    "# Release the video capture on exit\n",
    "cap.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from PIL import Image, ImageTk\n",
    "import tkinter as tk\n",
    "from tkinter import Label, Frame\n",
    "\n",
    "# Define the Transformer-based Autoencoder model\n",
    "class TransformerAutoencoder(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, emb_dim=512, num_heads=8, num_layers=6):\n",
    "        super(TransformerAutoencoder, self).__init__()\n",
    "        assert img_size % patch_size == 0, 'Image size must be divisible by patch size'\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        self.emb_dim = emb_dim\n",
    "        self.patch_dim = 3 * patch_size * patch_size  # Assuming RGB images\n",
    "\n",
    "        # Linear projection of flattened patches\n",
    "        self.patch_embedding = nn.Linear(self.patch_dim, emb_dim)\n",
    "        # Positional embedding\n",
    "        self.positional_embedding = nn.Parameter(torch.randn(1, self.num_patches, emb_dim))\n",
    "\n",
    "        # Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=emb_dim, nhead=num_heads, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Transformer Decoder\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=emb_dim, nhead=num_heads, batch_first=True)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Output projection to reconstruct patches\n",
    "        self.output_projection = nn.Linear(emb_dim, self.patch_dim)\n",
    "\n",
    "        # Target mask for causal decoding\n",
    "        self.tgt_mask = nn.Transformer.generate_square_subsequent_mask(self.num_patches).to(\n",
    "            torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        )\n",
    "\n",
    "    def _get_patches(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        # Divide image into patches\n",
    "        x = x.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size)\n",
    "        x = x.permute(0, 2, 3, 1, 4, 5).contiguous()\n",
    "        x = x.view(batch_size, -1, self.patch_dim)\n",
    "        return x\n",
    "\n",
    "    def _reconstruct_from_patches(self, patches, batch_size):\n",
    "        # Reconstruct image from patches\n",
    "        reconstructed = patches.view(\n",
    "            batch_size,\n",
    "            self.img_size // self.patch_size,\n",
    "            self.img_size // self.patch_size,\n",
    "            3,\n",
    "            self.patch_size,\n",
    "            self.patch_size\n",
    "        )\n",
    "        reconstructed = reconstructed.permute(0, 3, 1, 4, 2, 5).contiguous()\n",
    "        reconstructed = reconstructed.view(batch_size, 3, self.img_size, self.img_size)\n",
    "        return reconstructed\n",
    "\n",
    "    def encode(self, x):\n",
    "        x_patches = self._get_patches(x)\n",
    "        x_embedded = self.patch_embedding(x_patches) + self.positional_embedding\n",
    "        memory = self.encoder(x_embedded)\n",
    "        return x_embedded, memory\n",
    "\n",
    "    def decode(self, x_embedded, memory):\n",
    "        # Prepare target sequence for decoder (shifted right)\n",
    "        tgt = torch.zeros_like(x_embedded)\n",
    "        tgt[:, 1:, :] = x_embedded[:, :-1, :]\n",
    "        tgt = tgt + self.positional_embedding\n",
    "\n",
    "        # Decode patches\n",
    "        output = self.decoder(tgt, memory, tgt_mask=self.tgt_mask)\n",
    "        reconstructed_patches = self.output_projection(output)\n",
    "        return reconstructed_patches\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x_embedded, memory = self.encode(x)\n",
    "        reconstructed_patches = self.decode(x_embedded, memory)\n",
    "        reconstructed = self._reconstruct_from_patches(reconstructed_patches, batch_size)\n",
    "        reconstructed = torch.sigmoid(reconstructed)  # Apply sigmoid to get pixel values between 0 and 1\n",
    "        return memory, reconstructed\n",
    "\n",
    "# Initialize the model, optimizer, and loss function\n",
    "img_size = 224  # Input image resolution\n",
    "autoencoder = TransformerAutoencoder(\n",
    "    img_size=img_size,\n",
    "    patch_size=16,\n",
    "    emb_dim=512,\n",
    "    num_heads=8,\n",
    "    num_layers=6\n",
    ")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "autoencoder.to(device)\n",
    "\n",
    "optimizer = optim.Adam(autoencoder.parameters(), lr=0.0001)\n",
    "criterion = nn.MSELoss()  # Reconstruction loss\n",
    "\n",
    "# Video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Transform to preprocess the frame\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Initialize Tkinter window\n",
    "window = tk.Tk()\n",
    "window.title(\"Real-time Transformer Autoencoder Visualization\")\n",
    "\n",
    "# Frame for organizing the input video, latent space, and reconstructed video\n",
    "main_frame = Frame(window)\n",
    "main_frame.pack()\n",
    "\n",
    "# Labels for each of the three outputs\n",
    "video_label = Label(main_frame)\n",
    "video_label.grid(row=1, column=0)\n",
    "\n",
    "latent_label = Label(main_frame)\n",
    "latent_label.grid(row=1, column=1)\n",
    "\n",
    "output_label = Label(main_frame)\n",
    "output_label.grid(row=1, column=2)\n",
    "\n",
    "# Titles for each section\n",
    "Label(main_frame, text=\"Input Video\", font=(\"Arial\", 12, \"bold\")).grid(row=0, column=0)\n",
    "Label(main_frame, text=\"Latent Space\", font=(\"Arial\", 12, \"bold\")).grid(row=0, column=1)\n",
    "Label(main_frame, text=\"Output Video\", font=(\"Arial\", 12, \"bold\")).grid(row=0, column=2)\n",
    "\n",
    "# Label to display the latent space size\n",
    "latent_size_label = Label(window, text=\"\", font=(\"Arial\", 12))\n",
    "latent_size_label.pack()\n",
    "\n",
    "def update_frame():\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to capture video frame\")\n",
    "        window.after(10, update_frame)\n",
    "        return\n",
    "\n",
    "    # Convert frame to PIL image\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    img_pil = Image.fromarray(rgb_frame)\n",
    "    img_resized = img_pil.resize((img_size, img_size))\n",
    "\n",
    "    # Display original frame\n",
    "    img_tk = ImageTk.PhotoImage(image=img_resized)\n",
    "    video_label.imgtk = img_tk\n",
    "    video_label.configure(image=img_tk)\n",
    "\n",
    "    # Preprocess frame for model input\n",
    "    input_image = transform(img_resized).unsqueeze(0).to(device)  # Shape: [1, 3, H, W]\n",
    "\n",
    "    # Perform a training step\n",
    "    autoencoder.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    latent_space, reconstructed = autoencoder(input_image)  # Forward pass\n",
    "    loss = criterion(reconstructed, input_image)            # Compute loss\n",
    "    loss.backward()                                         # Backpropagation\n",
    "\n",
    "    # Gradient clipping for stability\n",
    "    torch.nn.utils.clip_grad_norm_(autoencoder.parameters(), max_norm=1.0)\n",
    "\n",
    "    optimizer.step()                                        # Update weights\n",
    "\n",
    "    # Display the latent space (mean over embedding dimension)\n",
    "    latent_space_np = latent_space.detach().mean(dim=2).cpu().numpy()  # Shape: [batch_size, num_patches]\n",
    "    # Reshape to image\n",
    "    num_patches_side = img_size // autoencoder.patch_size\n",
    "    latent_image = latent_space_np[0].reshape(num_patches_side, num_patches_side)\n",
    "    # Upscale to match image size\n",
    "    latent_image_resized = cv2.resize(latent_image, (img_size, img_size), interpolation=cv2.INTER_CUBIC)\n",
    "    # Normalize and convert to uint8\n",
    "    latent_image_resized = (255 * (latent_image_resized - latent_image_resized.min()) /\n",
    "                            (latent_image_resized.max() - latent_image_resized.min() + 1e-5)).astype(np.uint8)\n",
    "    latent_img = Image.fromarray(latent_image_resized)\n",
    "    latent_img_tk = ImageTk.PhotoImage(image=latent_img)\n",
    "    latent_label.imgtk = latent_img_tk\n",
    "    latent_label.configure(image=latent_img_tk)\n",
    "\n",
    "    # Display the reconstructed output at input resolution\n",
    "    output_np = reconstructed[0].detach().cpu().numpy().transpose(1, 2, 0)  # Convert to HxWxC\n",
    "    output_np = (255 * output_np).astype(np.uint8)\n",
    "    output_img = Image.fromarray(output_np)\n",
    "    output_img_tk = ImageTk.PhotoImage(image=output_img)\n",
    "    output_label.imgtk = output_img_tk\n",
    "    output_label.configure(image=output_img_tk)\n",
    "\n",
    "    # Update the latent space size label\n",
    "    latent_size = latent_space.size()\n",
    "    latent_size_text = f\"Latent Space Size: {list(latent_size)}\"\n",
    "    latent_size_label.config(text=latent_size_text)\n",
    "\n",
    "    # Schedule the next frame update\n",
    "    window.after(1, update_frame)\n",
    "\n",
    "# Start video feed update loop\n",
    "update_frame()\n",
    "\n",
    "# Run Tkinter main loop\n",
    "window.mainloop()\n",
    "\n",
    "# Release the video capture on exit\n",
    "cap.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from PIL import Image, ImageTk\n",
    "import tkinter as tk\n",
    "from tkinter import Label, Frame\n",
    "import time\n",
    "import sys\n",
    "\n",
    "# Hashing imports\n",
    "import binascii  # For converting hashes to hex\n",
    "from tkinter import filedialog\n",
    "\n",
    "# Define the Teacher class with the encoder\n",
    "class Teacher(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, emb_dim=512, num_heads=8, num_layers=6):\n",
    "        super(Teacher, self).__init__()\n",
    "        assert img_size % patch_size == 0, 'Image size must be divisible by patch size'\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        self.emb_dim = emb_dim\n",
    "        self.patch_dim = 3 * patch_size * patch_size  # Assuming RGB images\n",
    "\n",
    "        # Linear projection of flattened patches\n",
    "        self.patch_embedding = nn.Linear(self.patch_dim, emb_dim)\n",
    "        # Positional embedding\n",
    "        self.positional_embedding = nn.Parameter(torch.randn(1, self.num_patches, emb_dim))\n",
    "\n",
    "        # Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=emb_dim, nhead=num_heads, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "    def _get_patches(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        # Divide image into patches\n",
    "        x = x.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size)\n",
    "        x = x.permute(0, 2, 3, 1, 4, 5).contiguous()\n",
    "        x = x.view(batch_size, -1, self.patch_dim)\n",
    "        return x\n",
    "\n",
    "    def encode(self, x):\n",
    "        x_patches = self._get_patches(x)\n",
    "        x_embedded = self.patch_embedding(x_patches) + self.positional_embedding\n",
    "        memory = self.encoder(x_embedded)\n",
    "        return x_embedded, memory\n",
    "\n",
    "# Define the Apprentice class with the decoder\n",
    "class Apprentice(nn.Module):\n",
    "    def __init__(self, teacher, img_size=224, patch_size=16, emb_dim=512, num_heads=8, num_layers=6):\n",
    "        super(Apprentice, self).__init__()\n",
    "        self.teacher = teacher\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.emb_dim = emb_dim\n",
    "        self.patch_dim = 3 * patch_size * patch_size  # Assuming RGB images\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "\n",
    "        # Transformer Decoder\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=emb_dim, nhead=num_heads, batch_first=True)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Output projection to reconstruct patches\n",
    "        self.output_projection = nn.Linear(emb_dim, self.patch_dim)\n",
    "\n",
    "        # Target mask for causal decoding\n",
    "        self.tgt_mask = nn.Transformer.generate_square_subsequent_mask(self.num_patches).to(\n",
    "            torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        )\n",
    "\n",
    "    def _reconstruct_from_patches(self, patches, batch_size):\n",
    "        # Reconstruct image from patches\n",
    "        reconstructed = patches.view(\n",
    "            batch_size,\n",
    "            self.img_size // self.patch_size,\n",
    "            self.img_size // self.patch_size,\n",
    "            3,\n",
    "            self.patch_size,\n",
    "            self.patch_size\n",
    "        )\n",
    "        reconstructed = reconstructed.permute(0, 3, 1, 4, 2, 5).contiguous()\n",
    "        reconstructed = reconstructed.view(batch_size, 3, self.img_size, self.img_size)\n",
    "        return reconstructed\n",
    "\n",
    "    def decode(self, x_embedded, memory):\n",
    "        # Prepare target sequence for decoder (shifted right)\n",
    "        tgt = torch.zeros_like(x_embedded)\n",
    "        tgt[:, 1:, :] = x_embedded[:, :-1, :]\n",
    "        tgt = tgt + self.teacher.positional_embedding\n",
    "\n",
    "        # Decode patches\n",
    "        output = self.decoder(tgt, memory, tgt_mask=self.tgt_mask)\n",
    "        reconstructed_patches = self.output_projection(output)\n",
    "        return reconstructed_patches\n",
    "\n",
    "    def forward(self, x_embedded, memory):\n",
    "        batch_size = x_embedded.size(0)\n",
    "        reconstructed_patches = self.decode(x_embedded, memory)\n",
    "        reconstructed = self._reconstruct_from_patches(reconstructed_patches, batch_size)\n",
    "        reconstructed = torch.sigmoid(reconstructed)  # Apply sigmoid to get pixel values between 0 and 1\n",
    "        return reconstructed\n",
    "\n",
    "# Hashing functions\n",
    "def calculate_ahash(frame, hash_size=8):\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    resized_frame = cv2.resize(gray_frame, (hash_size, hash_size))\n",
    "    avg_pixel_value = np.mean(resized_frame)\n",
    "    ahash = ''.join(['1' if pixel > avg_pixel_value else '0' for pixel in resized_frame.flatten()])\n",
    "    return ahash\n",
    "\n",
    "def hamming_distance(hash1, hash2):\n",
    "    return sum(el1 != el2 for el1, el2 in zip(hash1, hash2))\n",
    "\n",
    "def get_min_hamming_distance(frame_hash, knowledge_base_hashes):\n",
    "    if not knowledge_base_hashes:\n",
    "        return 64  # Max possible distance for 8x8 hash\n",
    "    min_distance = 64\n",
    "    for stored_hash in knowledge_base_hashes:\n",
    "        distance = hamming_distance(frame_hash, stored_hash)\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "    return min_distance\n",
    "\n",
    "def get_knowledge_base_size(knowledge_base_hashes):\n",
    "    total_size = sys.getsizeof(knowledge_base_hashes)\n",
    "    for h in knowledge_base_hashes:\n",
    "        total_size += sys.getsizeof(h)\n",
    "    return total_size\n",
    "\n",
    "# Initialize the model components\n",
    "img_size = 224  # Input image resolution\n",
    "teacher = Teacher(\n",
    "    img_size=img_size,\n",
    "    patch_size=16,\n",
    "    emb_dim=512,\n",
    "    num_heads=8,\n",
    "    num_layers=6\n",
    ")\n",
    "apprentice = Apprentice(\n",
    "    teacher=teacher,\n",
    "    img_size=img_size,\n",
    "    patch_size=16,\n",
    "    emb_dim=512,\n",
    "    num_heads=8,\n",
    "    num_layers=6\n",
    ")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "teacher.to(device)\n",
    "apprentice.to(device)\n",
    "\n",
    "optimizer = optim.Adam(list(teacher.parameters()) + list(apprentice.parameters()), lr=0.0001)\n",
    "criterion = nn.MSELoss()  # Reconstruction loss\n",
    "\n",
    "# Video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Transform to preprocess the frame\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Knowledge base to store hashes of previously seen frames\n",
    "knowledge_base_hashes = set()\n",
    "\n",
    "# Initialize Tkinter window\n",
    "window = tk.Tk()\n",
    "window.title(\"Real-time Transformer Autoencoder Visualization with Hashing\")\n",
    "\n",
    "# Frame for organizing the input video, latent space, and reconstructed video\n",
    "main_frame = Frame(window)\n",
    "main_frame.pack()\n",
    "\n",
    "# Labels for each of the three outputs\n",
    "video_label = Label(main_frame)\n",
    "video_label.grid(row=1, column=0)\n",
    "\n",
    "latent_label = Label(main_frame)\n",
    "latent_label.grid(row=1, column=1)\n",
    "\n",
    "output_label = Label(main_frame)\n",
    "output_label.grid(row=1, column=2)\n",
    "\n",
    "# Titles for each section\n",
    "Label(main_frame, text=\"Input Video\", font=(\"Arial\", 12, \"bold\")).grid(row=0, column=0)\n",
    "Label(main_frame, text=\"Latent Space\", font=(\"Arial\", 12, \"bold\")).grid(row=0, column=1)\n",
    "Label(main_frame, text=\"Output Video\", font=(\"Arial\", 12, \"bold\")).grid(row=0, column=2)\n",
    "\n",
    "# Label to display the latent space size\n",
    "latent_size_label = Label(window, text=\"\", font=(\"Arial\", 12))\n",
    "latent_size_label.pack()\n",
    "\n",
    "# Hashing variables\n",
    "similarity_threshold = tk.DoubleVar(value=50.0)\n",
    "knowledge_base_size_var = tk.StringVar(value=\"Knowledge Base Size: 0 bytes\")\n",
    "\n",
    "# Frame for controls\n",
    "controls_frame = tk.Frame(window)\n",
    "controls_frame.pack(pady=5)\n",
    "\n",
    "# Slider for similarity threshold\n",
    "threshold_label = tk.Label(controls_frame, text=\"Similarity Threshold (%)\", font=(\"Helvetica\", 12))\n",
    "threshold_label.pack(side=tk.LEFT, padx=5)\n",
    "threshold_slider = tk.Scale(controls_frame, from_=0, to=100, orient=tk.HORIZONTAL,\n",
    "                            variable=similarity_threshold, length=200)\n",
    "threshold_slider.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "# Force training checkbox\n",
    "force_training_var = tk.BooleanVar(value=False)\n",
    "force_training_checkbox = tk.Checkbutton(controls_frame, text=\"Force Training\", variable=force_training_var)\n",
    "force_training_checkbox.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "# Knowledge base size label\n",
    "knowledge_base_size_label = tk.Label(window, textvariable=knowledge_base_size_var, font=(\"Helvetica\", 12))\n",
    "knowledge_base_size_label.pack()\n",
    "\n",
    "def update_frame():\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to capture video frame\")\n",
    "        window.after(10, update_frame)\n",
    "        return\n",
    "\n",
    "    # Convert frame to PIL image\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    img_pil = Image.fromarray(rgb_frame)\n",
    "    img_resized = img_pil.resize((img_size, img_size))\n",
    "\n",
    "    # Display original frame\n",
    "    img_tk = ImageTk.PhotoImage(image=img_resized)\n",
    "    video_label.imgtk = img_tk\n",
    "    video_label.configure(image=img_tk)\n",
    "\n",
    "    # Preprocess frame for model input\n",
    "    input_image = transform(img_resized).unsqueeze(0).to(device)  # Shape: [1, 3, H, W]\n",
    "\n",
    "    # Hashing logic to determine whether to train or infer\n",
    "    frame_array = np.array(img_resized)\n",
    "    frame_hash = calculate_ahash(frame_array)\n",
    "\n",
    "    min_distance = get_min_hamming_distance(frame_hash, knowledge_base_hashes)\n",
    "    similarity_percentage = (1 - (min_distance / 64)) * 100\n",
    "\n",
    "    # Get the current threshold from the slider\n",
    "    threshold = similarity_threshold.get()\n",
    "\n",
    "    # Check if force training is enabled\n",
    "    if force_training_var.get():\n",
    "        # Force training and include the hash\n",
    "        knowledge_base_hashes.add(frame_hash)\n",
    "\n",
    "        # Perform a training step\n",
    "        teacher.train()\n",
    "        apprentice.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x_embedded, memory = teacher.encode(input_image)\n",
    "        reconstructed = apprentice(x_embedded, memory)  # Forward pass\n",
    "        loss = criterion(reconstructed, input_image)    # Compute loss\n",
    "        loss.backward()                                 # Backpropagation\n",
    "\n",
    "        # Gradient clipping for stability\n",
    "        torch.nn.utils.clip_grad_norm_(list(teacher.parameters()) + list(apprentice.parameters()), max_norm=1.0)\n",
    "\n",
    "        optimizer.step()                                # Update weights\n",
    "    else:\n",
    "        # Decide whether to train or just infer based on similarity\n",
    "        if similarity_percentage < threshold:\n",
    "            # Add new frame hash to knowledge base\n",
    "            knowledge_base_hashes.add(frame_hash)\n",
    "\n",
    "            # Perform a training step\n",
    "            teacher.train()\n",
    "            apprentice.train()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            x_embedded, memory = teacher.encode(input_image)\n",
    "            reconstructed = apprentice(x_embedded, memory)  # Forward pass\n",
    "            loss = criterion(reconstructed, input_image)    # Compute loss\n",
    "            loss.backward()                                 # Backpropagation\n",
    "\n",
    "            # Gradient clipping for stability\n",
    "            torch.nn.utils.clip_grad_norm_(list(teacher.parameters()) + list(apprentice.parameters()), max_norm=1.0)\n",
    "\n",
    "            optimizer.step()                                # Update weights\n",
    "        else:\n",
    "            # Just perform inference\n",
    "            teacher.eval()\n",
    "            apprentice.eval()\n",
    "            with torch.no_grad():\n",
    "                x_embedded, memory = teacher.encode(input_image)\n",
    "                reconstructed = apprentice(x_embedded, memory)\n",
    "\n",
    "    # Display the latent space (mean over embedding dimension)\n",
    "    latent_space = memory\n",
    "    latent_space_np = latent_space.detach().mean(dim=2).cpu().numpy()  # Shape: [batch_size, num_patches]\n",
    "    # Reshape to image\n",
    "    num_patches_side = img_size // teacher.patch_size\n",
    "    latent_image = latent_space_np[0].reshape(num_patches_side, num_patches_side)\n",
    "    # Upscale to match image size\n",
    "    latent_image_resized = cv2.resize(latent_image, (img_size, img_size), interpolation=cv2.INTER_CUBIC)\n",
    "    # Normalize and convert to uint8\n",
    "    latent_image_resized = (255 * (latent_image_resized - latent_image_resized.min()) /\n",
    "                            (latent_image_resized.max() - latent_image_resized.min() + 1e-5)).astype(np.uint8)\n",
    "    latent_img = Image.fromarray(latent_image_resized)\n",
    "    latent_img_tk = ImageTk.PhotoImage(image=latent_img)\n",
    "    latent_label.imgtk = latent_img_tk\n",
    "    latent_label.configure(image=latent_img_tk)\n",
    "\n",
    "    # Display the reconstructed output at input resolution\n",
    "    output_np = reconstructed[0].detach().cpu().numpy().transpose(1, 2, 0)  # Convert to HxWxC\n",
    "    output_np = (255 * output_np).astype(np.uint8)\n",
    "    output_img = Image.fromarray(output_np)\n",
    "    output_img_tk = ImageTk.PhotoImage(image=output_img)\n",
    "    output_label.imgtk = output_img_tk\n",
    "    output_label.configure(image=output_img_tk)\n",
    "\n",
    "    # Update the latent space size label\n",
    "    latent_size = latent_space.size()\n",
    "    latent_size_text = f\"Latent Space Size: {list(latent_size)}\"\n",
    "    latent_size_label.config(text=latent_size_text)\n",
    "\n",
    "    # Update knowledge base size\n",
    "    kb_size_bytes = get_knowledge_base_size(knowledge_base_hashes)\n",
    "    if kb_size_bytes < 1024:\n",
    "        knowledge_base_size_var.set(f\"Knowledge Base Size: {kb_size_bytes} bytes\")\n",
    "    else:\n",
    "        kb_size_kb = kb_size_bytes / 1024\n",
    "        knowledge_base_size_var.set(f\"Knowledge Base Size: {kb_size_kb:.2f} KB\")\n",
    "\n",
    "    # Schedule the next frame update\n",
    "    window.after(1, update_frame)\n",
    "\n",
    "# Start video feed update loop\n",
    "update_frame()\n",
    "\n",
    "# Run Tkinter main loop\n",
    "window.mainloop()\n",
    "\n",
    "# Release the video capture on exit\n",
    "cap.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from PIL import Image, ImageTk\n",
    "import tkinter as tk\n",
    "from tkinter import Label, Frame\n",
    "import time\n",
    "import sys\n",
    "\n",
    "# Hashing imports\n",
    "import binascii  # For converting hashes to hex\n",
    "from tkinter import filedialog\n",
    "\n",
    "# Define the Teacher class with the encoder\n",
    "class Teacher(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=8, emb_dim=512, num_heads=8, num_layers=6):\n",
    "        super(Teacher, self).__init__()\n",
    "        assert img_size % patch_size == 0, 'Image size must be divisible by patch size'\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        self.emb_dim = emb_dim\n",
    "        self.patch_dim = 3 * patch_size * patch_size  # Assuming RGB images\n",
    "\n",
    "        # Linear projection of flattened patches\n",
    "        self.patch_embedding = nn.Linear(self.patch_dim, emb_dim)\n",
    "        # Positional embedding\n",
    "        self.positional_embedding = nn.Parameter(torch.randn(1, self.num_patches, emb_dim))\n",
    "\n",
    "        # Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=emb_dim, nhead=num_heads, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "    def _get_patches(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        # Divide image into patches\n",
    "        x = x.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size)\n",
    "        x = x.permute(0, 2, 3, 1, 4, 5).contiguous()\n",
    "        x = x.view(batch_size, -1, self.patch_dim)\n",
    "        return x\n",
    "\n",
    "    def encode(self, x):\n",
    "        x_patches = self._get_patches(x)\n",
    "        x_embedded = self.patch_embedding(x_patches) + self.positional_embedding\n",
    "        memory = self.encoder(x_embedded)\n",
    "        return x_embedded, memory\n",
    "\n",
    "# Define the Apprentice class with the decoder\n",
    "class Apprentice(nn.Module):\n",
    "    def __init__(self, teacher, img_size=224, patch_size=8, emb_dim=512, num_heads=8, num_layers=6):\n",
    "        super(Apprentice, self).__init__()\n",
    "        self.teacher = teacher\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.emb_dim = emb_dim\n",
    "        self.patch_dim = 3 * patch_size * patch_size  # Assuming RGB images\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "\n",
    "        # Transformer Decoder\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=emb_dim, nhead=num_heads, batch_first=True)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Output projection to reconstruct patches\n",
    "        self.output_projection = nn.Linear(emb_dim, self.patch_dim)\n",
    "\n",
    "        # Target mask for causal decoding\n",
    "        self.tgt_mask = nn.Transformer.generate_square_subsequent_mask(self.num_patches).to(\n",
    "            torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        )\n",
    "\n",
    "    def _reconstruct_from_patches(self, patches, batch_size):\n",
    "        # Reconstruct image from patches\n",
    "        reconstructed = patches.view(\n",
    "            batch_size,\n",
    "            self.img_size // self.patch_size,\n",
    "            self.img_size // self.patch_size,\n",
    "            3,\n",
    "            self.patch_size,\n",
    "            self.patch_size\n",
    "        )\n",
    "        reconstructed = reconstructed.permute(0, 3, 1, 4, 2, 5).contiguous()\n",
    "        reconstructed = reconstructed.view(batch_size, 3, self.img_size, self.img_size)\n",
    "        return reconstructed\n",
    "\n",
    "    def decode(self, x_embedded, memory):\n",
    "        # Prepare target sequence for decoder (shifted right)\n",
    "        tgt = torch.zeros_like(x_embedded)\n",
    "        tgt[:, 1:, :] = x_embedded[:, :-1, :]\n",
    "        tgt = tgt + self.teacher.positional_embedding\n",
    "\n",
    "        # Decode patches\n",
    "        output = self.decoder(tgt, memory, tgt_mask=self.tgt_mask)\n",
    "        reconstructed_patches = self.output_projection(output)\n",
    "        return reconstructed_patches\n",
    "\n",
    "    def forward(self, x_embedded, memory):\n",
    "        batch_size = x_embedded.size(0)\n",
    "        reconstructed_patches = self.decode(x_embedded, memory)\n",
    "        reconstructed = self._reconstruct_from_patches(reconstructed_patches, batch_size)\n",
    "        reconstructed = torch.sigmoid(reconstructed)  # Apply sigmoid to get pixel values between 0 and 1\n",
    "        return reconstructed\n",
    "\n",
    "# Hashing functions\n",
    "def calculate_ahash(frame, hash_size=8):\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    resized_frame = cv2.resize(gray_frame, (hash_size, hash_size))\n",
    "    avg_pixel_value = np.mean(resized_frame)\n",
    "    ahash = ''.join(['1' if pixel > avg_pixel_value else '0' for pixel in resized_frame.flatten()])\n",
    "    return ahash\n",
    "\n",
    "def hamming_distance(hash1, hash2):\n",
    "    return sum(el1 != el2 for el1, el2 in zip(hash1, hash2))\n",
    "\n",
    "def get_min_hamming_distance(frame_hash, knowledge_base_hashes):\n",
    "    if not knowledge_base_hashes:\n",
    "        return 64  # Max possible distance for 8x8 hash\n",
    "    min_distance = 64\n",
    "    for stored_hash in knowledge_base_hashes:\n",
    "        distance = hamming_distance(frame_hash, stored_hash)\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "    return min_distance\n",
    "\n",
    "def get_knowledge_base_size(knowledge_base_hashes):\n",
    "    total_size = sys.getsizeof(knowledge_base_hashes)\n",
    "    for h in knowledge_base_hashes:\n",
    "        total_size += sys.getsizeof(h)\n",
    "    return total_size\n",
    "\n",
    "# Initialize the model components\n",
    "img_size = 224  # Input image resolution\n",
    "patch_size = 8  # Reduced from 16 to 8 for more tiles\n",
    "teacher = Teacher(\n",
    "    img_size=img_size,\n",
    "    patch_size=patch_size,\n",
    "    emb_dim=512,\n",
    "    num_heads=8,\n",
    "    num_layers=6\n",
    ")\n",
    "apprentice = Apprentice(\n",
    "    teacher=teacher,\n",
    "    img_size=img_size,\n",
    "    patch_size=patch_size,\n",
    "    emb_dim=512,\n",
    "    num_heads=8,\n",
    "    num_layers=6\n",
    ")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "teacher.to(device)\n",
    "apprentice.to(device)\n",
    "\n",
    "optimizer = optim.Adam(list(teacher.parameters()) + list(apprentice.parameters()), lr=0.0001)\n",
    "criterion = nn.MSELoss()  # Reconstruction loss\n",
    "\n",
    "# Video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Transform to preprocess the frame\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Knowledge base to store hashes of previously seen frames\n",
    "knowledge_base_hashes = set()\n",
    "\n",
    "# Initialize Tkinter window\n",
    "window = tk.Tk()\n",
    "window.title(\"Real-time Transformer Autoencoder Visualization with Hashing\")\n",
    "\n",
    "# Frame for organizing the input video, latent space, and reconstructed video\n",
    "main_frame = Frame(window)\n",
    "main_frame.pack()\n",
    "\n",
    "# Labels for each of the three outputs\n",
    "video_label = Label(main_frame)\n",
    "video_label.grid(row=1, column=0)\n",
    "\n",
    "latent_label = Label(main_frame)\n",
    "latent_label.grid(row=1, column=1)\n",
    "\n",
    "output_label = Label(main_frame)\n",
    "output_label.grid(row=1, column=2)\n",
    "\n",
    "# Titles for each section\n",
    "Label(main_frame, text=\"Input Video\", font=(\"Arial\", 12, \"bold\")).grid(row=0, column=0)\n",
    "Label(main_frame, text=\"Latent Space\", font=(\"Arial\", 12, \"bold\")).grid(row=0, column=1)\n",
    "Label(main_frame, text=\"Output Video\", font=(\"Arial\", 12, \"bold\")).grid(row=0, column=2)\n",
    "\n",
    "# Label to display the latent space size\n",
    "latent_size_label = Label(window, text=\"\", font=(\"Arial\", 12))\n",
    "latent_size_label.pack()\n",
    "\n",
    "# Hashing variables\n",
    "similarity_threshold = tk.DoubleVar(value=50.0)\n",
    "knowledge_base_size_var = tk.StringVar(value=\"Knowledge Base Size: 0 bytes\")\n",
    "\n",
    "# Frame for controls\n",
    "controls_frame = tk.Frame(window)\n",
    "controls_frame.pack(pady=5)\n",
    "\n",
    "# Mode selection (Train or Inference)\n",
    "mode_var = tk.StringVar(value=\"Inference\")\n",
    "train_radio = tk.Radiobutton(controls_frame, text=\"Train\", variable=mode_var, value=\"Train\")\n",
    "inference_radio = tk.Radiobutton(controls_frame, text=\"Inference\", variable=mode_var, value=\"Inference\")\n",
    "train_radio.pack(side=tk.LEFT, padx=5)\n",
    "inference_radio.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "# Slider for similarity threshold\n",
    "threshold_label = tk.Label(controls_frame, text=\"Similarity Threshold (%)\", font=(\"Helvetica\", 12))\n",
    "threshold_label.pack(side=tk.LEFT, padx=5)\n",
    "threshold_slider = tk.Scale(controls_frame, from_=0, to=100, orient=tk.HORIZONTAL,\n",
    "                            variable=similarity_threshold, length=200)\n",
    "threshold_slider.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "# Knowledge base size label\n",
    "knowledge_base_size_label = tk.Label(window, textvariable=knowledge_base_size_var, font=(\"Helvetica\", 12))\n",
    "knowledge_base_size_label.pack()\n",
    "\n",
    "# Labels to display the sizes\n",
    "size_info_label = tk.Label(window, text=\"\", font=(\"Arial\", 12))\n",
    "size_info_label.pack()\n",
    "\n",
    "def update_frame():\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to capture video frame\")\n",
    "        window.after(10, update_frame)\n",
    "        return\n",
    "\n",
    "    # Convert frame to PIL image\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    img_pil = Image.fromarray(rgb_frame)\n",
    "    img_resized = img_pil.resize((img_size, img_size))\n",
    "\n",
    "    # Display original frame\n",
    "    img_tk = ImageTk.PhotoImage(image=img_resized)\n",
    "    video_label.imgtk = img_tk\n",
    "    video_label.configure(image=img_tk)\n",
    "\n",
    "    # Preprocess frame for model input\n",
    "    input_image = transform(img_resized).unsqueeze(0).to(device)  # Shape: [1, 3, H, W]\n",
    "\n",
    "    # Hashing logic to determine whether to train or infer\n",
    "    frame_array = np.array(img_resized)\n",
    "    frame_hash = calculate_ahash(frame_array)\n",
    "\n",
    "    min_distance = get_min_hamming_distance(frame_hash, knowledge_base_hashes)\n",
    "    similarity_percentage = (1 - (min_distance / 64)) * 100\n",
    "\n",
    "    # Get the current threshold from the slider\n",
    "    threshold = similarity_threshold.get()\n",
    "    mode = mode_var.get()\n",
    "\n",
    "    if mode == \"Train\":\n",
    "        # Force training and include the hash\n",
    "        knowledge_base_hashes.add(frame_hash)\n",
    "\n",
    "        # Perform a training step\n",
    "        teacher.train()\n",
    "        apprentice.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x_embedded, memory = teacher.encode(input_image)\n",
    "        reconstructed = apprentice(x_embedded, memory)  # Forward pass\n",
    "        loss = criterion(reconstructed, input_image)    # Compute loss\n",
    "        loss.backward()                                 # Backpropagation\n",
    "\n",
    "        # Gradient clipping for stability\n",
    "        torch.nn.utils.clip_grad_norm_(list(teacher.parameters()) + list(apprentice.parameters()), max_norm=1.0)\n",
    "\n",
    "        optimizer.step()                                # Update weights\n",
    "    elif mode == \"Inference\":\n",
    "        # Decide whether to train or just infer based on similarity\n",
    "        if similarity_percentage < threshold:\n",
    "            # Add new frame hash to knowledge base\n",
    "            knowledge_base_hashes.add(frame_hash)\n",
    "\n",
    "            # Perform a training step\n",
    "            teacher.train()\n",
    "            apprentice.train()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            x_embedded, memory = teacher.encode(input_image)\n",
    "            reconstructed = apprentice(x_embedded, memory)  # Forward pass\n",
    "            loss = criterion(reconstructed, input_image)    # Compute loss\n",
    "            loss.backward()                                 # Backpropagation\n",
    "\n",
    "            # Gradient clipping for stability\n",
    "            torch.nn.utils.clip_grad_norm_(list(teacher.parameters()) + list(apprentice.parameters()), max_norm=1.0)\n",
    "\n",
    "            optimizer.step()                                # Update weights\n",
    "        else:\n",
    "            # Just perform inference\n",
    "            teacher.eval()\n",
    "            apprentice.eval()\n",
    "            with torch.no_grad():\n",
    "                x_embedded, memory = teacher.encode(input_image)\n",
    "                reconstructed = apprentice(x_embedded, memory)\n",
    "\n",
    "    # Display the latent space (mean over embedding dimension)\n",
    "    latent_space = memory\n",
    "    latent_space_np = latent_space.detach().mean(dim=2).cpu().numpy()  # Shape: [batch_size, num_patches]\n",
    "    # Reshape to image\n",
    "    num_patches_side = img_size // teacher.patch_size\n",
    "    latent_image = latent_space_np[0].reshape(num_patches_side, num_patches_side)\n",
    "    # Upscale to match image size\n",
    "    latent_image_resized = cv2.resize(latent_image, (img_size, img_size), interpolation=cv2.INTER_CUBIC)\n",
    "    # Normalize and convert to uint8\n",
    "    latent_image_resized = (255 * (latent_image_resized - latent_image_resized.min()) /\n",
    "                            (latent_image_resized.max() - latent_image_resized.min() + 1e-5)).astype(np.uint8)\n",
    "    latent_img = Image.fromarray(latent_image_resized)\n",
    "    latent_img_tk = ImageTk.PhotoImage(image=latent_img)\n",
    "    latent_label.imgtk = latent_img_tk\n",
    "    latent_label.configure(image=latent_img_tk)\n",
    "\n",
    "    # Display the reconstructed output at input resolution\n",
    "    output_np = reconstructed[0].detach().cpu().numpy().transpose(1, 2, 0)  # Convert to HxWxC\n",
    "    output_np = (255 * output_np).astype(np.uint8)\n",
    "    output_img = Image.fromarray(output_np)\n",
    "    output_img_tk = ImageTk.PhotoImage(image=output_img)\n",
    "    output_label.imgtk = output_img_tk\n",
    "    output_label.configure(image=output_img_tk)\n",
    "\n",
    "    # Update the latent space size label\n",
    "    latent_size = latent_space.size()\n",
    "    latent_size_text = f\"Latent Space Size: {list(latent_size)}\"\n",
    "    latent_size_label.config(text=latent_size_text)\n",
    "\n",
    "    # Update knowledge base size\n",
    "    kb_size_bytes = get_knowledge_base_size(knowledge_base_hashes)\n",
    "    if kb_size_bytes < 1024:\n",
    "        knowledge_base_size_var.set(f\"Knowledge Base Size: {kb_size_bytes} bytes\")\n",
    "    else:\n",
    "        kb_size_kb = kb_size_bytes / 1024\n",
    "        knowledge_base_size_var.set(f\"Knowledge Base Size: {kb_size_kb:.2f} KB\")\n",
    "\n",
    "    # Calculate sizes\n",
    "    original_size_kb = frame_array.nbytes / 1024\n",
    "    latent_size_kb = latent_space.element_size() * latent_space.nelement() / 1024\n",
    "    reconstructed_size_kb = output_np.nbytes / 1024\n",
    "\n",
    "    # Update the size info label\n",
    "    size_info_text = f\"Original Image Size: {original_size_kb:.2f} KB | Latent Space Size: {latent_size_kb:.2f} KB | Reconstructed Image Size: {reconstructed_size_kb:.2f} KB\"\n",
    "    size_info_label.config(text=size_info_text)\n",
    "\n",
    "    # Schedule the next frame update\n",
    "    window.after(1, update_frame)\n",
    "\n",
    "# Start video feed update loop\n",
    "update_frame()\n",
    "\n",
    "# Run Tkinter main loop\n",
    "window.mainloop()\n",
    "\n",
    "# Release the video capture on exit\n",
    "cap.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\keplarV4\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_compile.py:24: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information\n",
      "  return torch._dynamo.disable(fn, recursive)(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from PIL import Image, ImageTk, ImageOps\n",
    "import tkinter as tk\n",
    "from tkinter import Label, Frame, OptionMenu\n",
    "import time\n",
    "import sys\n",
    "\n",
    "# Hashing imports\n",
    "import binascii  # For converting hashes to hex\n",
    "from tkinter import filedialog\n",
    "\n",
    "# Import for pHash\n",
    "import scipy.fftpack\n",
    "\n",
    "# Define the Teacher class with the encoder and latent space projection\n",
    "class Teacher(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, emb_dim=256, num_heads=8, num_layers=6, latent_dim=64):\n",
    "        super(Teacher, self).__init__()\n",
    "        assert img_size % patch_size == 0, 'Image size must be divisible by patch size'\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        self.emb_dim = emb_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.patch_dim = 3 * patch_size * patch_size  # Assuming RGB images\n",
    "\n",
    "        # Linear projection of flattened patches\n",
    "        self.patch_embedding = nn.Linear(self.patch_dim, emb_dim)\n",
    "        # Positional embedding\n",
    "        self.positional_embedding = nn.Parameter(torch.randn(1, self.num_patches, emb_dim))\n",
    "\n",
    "        # Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=emb_dim, nhead=num_heads, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Projection to reduce the latent space dimensionality\n",
    "        self.latent_projection = nn.Linear(self.num_patches * emb_dim, latent_dim)\n",
    "\n",
    "    def _get_patches(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        # Divide image into patches\n",
    "        x = x.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size)\n",
    "        x = x.permute(0, 2, 3, 1, 4, 5).contiguous()\n",
    "        x = x.view(batch_size, -1, self.patch_dim)\n",
    "        return x\n",
    "\n",
    "    def encode(self, x):\n",
    "        x_patches = self._get_patches(x)\n",
    "        x_embedded = self.patch_embedding(x_patches) + self.positional_embedding\n",
    "        memory = self.encoder(x_embedded)  # Shape: [batch_size, num_patches, emb_dim]\n",
    "\n",
    "        # Flatten memory and project to latent space\n",
    "        memory_flat = memory.view(x.size(0), -1)  # Shape: [batch_size, num_patches * emb_dim]\n",
    "        latent = self.latent_projection(memory_flat)  # Shape: [batch_size, latent_dim]\n",
    "        return x_embedded, memory, latent\n",
    "\n",
    "# Define the Apprentice class with the decoder\n",
    "class Apprentice(nn.Module):\n",
    "    def __init__(self, teacher, img_size=224, patch_size=16, emb_dim=256, num_heads=8, num_layers=6, latent_dim=64):\n",
    "        super(Apprentice, self).__init__()\n",
    "        self.teacher = teacher\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.emb_dim = emb_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.patch_dim = 3 * patch_size * patch_size  # Assuming RGB images\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "\n",
    "        # Projection to expand latent space back to memory size\n",
    "        self.latent_expand = nn.Linear(latent_dim, self.num_patches * emb_dim)\n",
    "\n",
    "        # Transformer Decoder\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=emb_dim, nhead=num_heads, batch_first=True)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Output projection to reconstruct patches\n",
    "        self.output_projection = nn.Linear(emb_dim, self.patch_dim)\n",
    "\n",
    "        # Target mask for causal decoding\n",
    "        self.tgt_mask = nn.Transformer.generate_square_subsequent_mask(self.num_patches).to(\n",
    "            torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        )\n",
    "\n",
    "    def _reconstruct_from_patches(self, patches, batch_size):\n",
    "        # Reconstruct image from patches\n",
    "        reconstructed = patches.view(\n",
    "            batch_size,\n",
    "            self.img_size // self.patch_size,\n",
    "            self.img_size // self.patch_size,\n",
    "            3,\n",
    "            self.patch_size,\n",
    "            self.patch_size\n",
    "        )\n",
    "        reconstructed = reconstructed.permute(0, 3, 1, 4, 2, 5).contiguous()\n",
    "        reconstructed = reconstructed.view(batch_size, 3, self.img_size, self.img_size)\n",
    "        return reconstructed\n",
    "\n",
    "    def decode(self, x_embedded, latent):\n",
    "        # Expand latent space back to memory size\n",
    "        memory_expanded = self.latent_expand(latent)  # Shape: [batch_size, num_patches * emb_dim]\n",
    "        memory = memory_expanded.view(-1, self.num_patches, self.emb_dim)  # Shape: [batch_size, num_patches, emb_dim]\n",
    "\n",
    "        # Prepare target sequence for decoder (shifted right)\n",
    "        tgt = torch.zeros_like(x_embedded)\n",
    "        tgt[:, 1:, :] = x_embedded[:, :-1, :]\n",
    "        tgt = tgt + self.teacher.positional_embedding\n",
    "\n",
    "        # Decode patches\n",
    "        output = self.decoder(tgt, memory, tgt_mask=self.tgt_mask)\n",
    "        reconstructed_patches = self.output_projection(output)\n",
    "        return reconstructed_patches\n",
    "\n",
    "    def forward(self, x_embedded, latent):\n",
    "        batch_size = x_embedded.size(0)\n",
    "        reconstructed_patches = self.decode(x_embedded, latent)\n",
    "        reconstructed = self._reconstruct_from_patches(reconstructed_patches, batch_size)\n",
    "        reconstructed = torch.sigmoid(reconstructed)  # Apply sigmoid to get pixel values between 0 and 1\n",
    "        return reconstructed\n",
    "\n",
    "# Hashing functions\n",
    "def calculate_ahash(frame, hash_size=8):\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    resized_frame = cv2.resize(gray_frame, (hash_size, hash_size))\n",
    "    avg_pixel_value = np.mean(resized_frame)\n",
    "    ahash = ''.join(['1' if pixel > avg_pixel_value else '0' for pixel in resized_frame.flatten()])\n",
    "    return ahash\n",
    "\n",
    "def calculate_dhash(frame, hash_size=8):\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    resized = cv2.resize(gray, (hash_size + 1, hash_size))\n",
    "    diff = resized[:, 1:] > resized[:, :-1]\n",
    "    dhash = ''.join(['1' if val else '0' for val in diff.flatten()])\n",
    "    return dhash\n",
    "\n",
    "def calculate_phash(frame, hash_size=8):\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    resized = cv2.resize(gray, (hash_size, hash_size))\n",
    "    dct = scipy.fftpack.dct(scipy.fftpack.dct(resized.T, norm='ortho').T, norm='ortho')\n",
    "    dct_low_freq = dct[:8, :8]\n",
    "    med = np.median(dct_low_freq)\n",
    "    phash = ''.join(['1' if val > med else '0' for val in dct_low_freq.flatten()])\n",
    "    return phash\n",
    "\n",
    "def calculate_whash(frame, hash_size=8):\n",
    "    # This is a simplified version using Haar wavelets\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    resized = cv2.resize(gray, (hash_size, hash_size))\n",
    "    coeffs = pywt.wavedec2(resized, 'haar', level=1)\n",
    "    LL = coeffs[0]\n",
    "    med = np.median(LL)\n",
    "    whash = ''.join(['1' if val > med else '0' for val in LL.flatten()])\n",
    "    return whash\n",
    "\n",
    "def hamming_distance(hash1, hash2):\n",
    "    return sum(c1 != c2 for c1, c2 in zip(hash1, hash2))\n",
    "\n",
    "def get_min_hamming_distance(frame_hash, knowledge_base_hashes):\n",
    "    if not knowledge_base_hashes:\n",
    "        return len(frame_hash)  # Max possible distance\n",
    "    min_distance = len(frame_hash)\n",
    "    for stored_hash in knowledge_base_hashes:\n",
    "        distance = hamming_distance(frame_hash, stored_hash)\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "    return min_distance\n",
    "\n",
    "def get_knowledge_base_size(knowledge_base_hashes):\n",
    "    total_size = sys.getsizeof(knowledge_base_hashes)\n",
    "    for h in knowledge_base_hashes:\n",
    "        total_size += sys.getsizeof(h)\n",
    "    return total_size\n",
    "\n",
    "# Initialize the model components\n",
    "img_size = 224  # Input image resolution\n",
    "patch_size = 16\n",
    "emb_dim = 256\n",
    "latent_dim = 64  # Smaller latent space\n",
    "teacher = Teacher(\n",
    "    img_size=img_size,\n",
    "    patch_size=patch_size,\n",
    "    emb_dim=emb_dim,\n",
    "    num_heads=8,\n",
    "    num_layers=6,\n",
    "    latent_dim=latent_dim\n",
    ")\n",
    "apprentice = Apprentice(\n",
    "    teacher=teacher,\n",
    "    img_size=img_size,\n",
    "    patch_size=patch_size,\n",
    "    emb_dim=emb_dim,\n",
    "    num_heads=8,\n",
    "    num_layers=6,\n",
    "    latent_dim=latent_dim\n",
    ")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "teacher.to(device)\n",
    "apprentice.to(device)\n",
    "\n",
    "optimizer = optim.Adam(list(teacher.parameters()) + list(apprentice.parameters()), lr=0.0001)\n",
    "criterion = nn.MSELoss()  # Reconstruction loss\n",
    "\n",
    "# Video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Transform to preprocess the frame\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Knowledge base to store hashes of previously seen frames\n",
    "knowledge_base_hashes = set()\n",
    "\n",
    "# Initialize Tkinter window\n",
    "window = tk.Tk()\n",
    "window.title(\"Real-time Transformer Autoencoder Visualization with Hashing\")\n",
    "\n",
    "# Frame for organizing the input video, latent space, and reconstructed video\n",
    "main_frame = Frame(window)\n",
    "main_frame.pack()\n",
    "\n",
    "# Labels for each of the three outputs\n",
    "video_label = Label(main_frame)\n",
    "video_label.grid(row=1, column=0)\n",
    "\n",
    "latent_label = Label(main_frame)\n",
    "latent_label.grid(row=1, column=1)\n",
    "\n",
    "output_label = Label(main_frame)\n",
    "output_label.grid(row=1, column=2)\n",
    "\n",
    "# Titles for each section\n",
    "Label(main_frame, text=\"Input Video\", font=(\"Arial\", 12, \"bold\")).grid(row=0, column=0)\n",
    "Label(main_frame, text=\"Latent Space\", font=(\"Arial\", 12, \"bold\")).grid(row=0, column=1)\n",
    "Label(main_frame, text=\"Output Video\", font=(\"Arial\", 12, \"bold\")).grid(row=0, column=2)\n",
    "\n",
    "# Label to display the latent space size\n",
    "latent_size_label = Label(window, text=\"\", font=(\"Arial\", 12))\n",
    "latent_size_label.pack()\n",
    "\n",
    "# Hashing variables\n",
    "similarity_threshold = tk.DoubleVar(value=50.0)\n",
    "knowledge_base_size_var = tk.StringVar(value=\"Knowledge Base Size: 0 bytes\")\n",
    "\n",
    "# Frame for controls\n",
    "controls_frame = tk.Frame(window)\n",
    "controls_frame.pack(pady=5)\n",
    "\n",
    "# Mode selection (Train or Inference)\n",
    "mode_var = tk.StringVar(value=\"Inference\")\n",
    "train_radio = tk.Radiobutton(controls_frame, text=\"Train\", variable=mode_var, value=\"Train\")\n",
    "inference_radio = tk.Radiobutton(controls_frame, text=\"Inference\", variable=mode_var, value=\"Inference\")\n",
    "train_radio.pack(side=tk.LEFT, padx=5)\n",
    "inference_radio.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "# Slider for similarity threshold\n",
    "threshold_label = tk.Label(controls_frame, text=\"Similarity Threshold (%)\", font=(\"Helvetica\", 12))\n",
    "threshold_label.pack(side=tk.LEFT, padx=5)\n",
    "threshold_slider = tk.Scale(controls_frame, from_=0, to=100, orient=tk.HORIZONTAL,\n",
    "                            variable=similarity_threshold, length=200)\n",
    "threshold_slider.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "# Hashing method selection\n",
    "hash_methods = [\"aHash\", \"dHash\", \"pHash\"]\n",
    "selected_hash_method = tk.StringVar(value=\"aHash\")\n",
    "hash_method_menu = tk.OptionMenu(controls_frame, selected_hash_method, *hash_methods)\n",
    "hash_method_menu.config(width=10)\n",
    "hash_method_label = tk.Label(controls_frame, text=\"Hash Method:\", font=(\"Helvetica\", 12))\n",
    "hash_method_label.pack(side=tk.LEFT, padx=5)\n",
    "hash_method_menu.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "# Knowledge base size label\n",
    "knowledge_base_size_label = tk.Label(window, textvariable=knowledge_base_size_var, font=(\"Helvetica\", 12))\n",
    "knowledge_base_size_label.pack()\n",
    "\n",
    "# Labels to display the sizes\n",
    "size_info_label = tk.Label(window, text=\"\", font=(\"Arial\", 12))\n",
    "size_info_label.pack()\n",
    "\n",
    "def update_frame():\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to capture video frame\")\n",
    "        window.after(10, update_frame)\n",
    "        return\n",
    "\n",
    "    # Convert frame to PIL image\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    img_pil = Image.fromarray(rgb_frame)\n",
    "    img_resized = img_pil.resize((img_size, img_size))\n",
    "\n",
    "    # Display original frame\n",
    "    img_tk = ImageTk.PhotoImage(image=img_resized)\n",
    "    video_label.imgtk = img_tk\n",
    "    video_label.configure(image=img_tk)\n",
    "\n",
    "    # Preprocess frame for model input\n",
    "    input_image = transform(img_resized).unsqueeze(0).to(device)  # Shape: [1, 3, H, W]\n",
    "\n",
    "    # Hashing logic to determine whether to train or infer\n",
    "    frame_array = np.array(img_resized)\n",
    "    hash_method = selected_hash_method.get()\n",
    "\n",
    "    if hash_method == \"aHash\":\n",
    "        frame_hash = calculate_ahash(frame_array)\n",
    "        max_distance = 64\n",
    "    elif hash_method == \"dHash\":\n",
    "        frame_hash = calculate_dhash(frame_array)\n",
    "        max_distance = 64\n",
    "    elif hash_method == \"pHash\":\n",
    "        frame_hash = calculate_phash(frame_array)\n",
    "        max_distance = 64\n",
    "    else:\n",
    "        frame_hash = calculate_ahash(frame_array)\n",
    "        max_distance = 64\n",
    "\n",
    "    min_distance = get_min_hamming_distance(frame_hash, knowledge_base_hashes)\n",
    "    similarity_percentage = (1 - (min_distance / max_distance)) * 100\n",
    "\n",
    "    # Get the current threshold from the slider\n",
    "    threshold = similarity_threshold.get()\n",
    "    mode = mode_var.get()\n",
    "\n",
    "    if mode == \"Train\":\n",
    "        # Force training and include the hash\n",
    "        knowledge_base_hashes.add(frame_hash)\n",
    "\n",
    "        # Perform a training step\n",
    "        teacher.train()\n",
    "        apprentice.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x_embedded, memory, latent = teacher.encode(input_image)\n",
    "        reconstructed = apprentice(x_embedded, latent)  # Forward pass\n",
    "        loss = criterion(reconstructed, input_image)    # Compute loss\n",
    "        loss.backward()                                 # Backpropagation\n",
    "\n",
    "        # Gradient clipping for stability\n",
    "        torch.nn.utils.clip_grad_norm_(list(teacher.parameters()) + list(apprentice.parameters()), max_norm=1.0)\n",
    "\n",
    "        optimizer.step()                                # Update weights\n",
    "    elif mode == \"Inference\":\n",
    "        # Decide whether to train or just infer based on similarity\n",
    "        if similarity_percentage < threshold:\n",
    "            # Add new frame hash to knowledge base\n",
    "            knowledge_base_hashes.add(frame_hash)\n",
    "\n",
    "            # Perform a training step\n",
    "            teacher.train()\n",
    "            apprentice.train()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            x_embedded, memory, latent = teacher.encode(input_image)\n",
    "            reconstructed = apprentice(x_embedded, latent)  # Forward pass\n",
    "            loss = criterion(reconstructed, input_image)    # Compute loss\n",
    "            loss.backward()                                 # Backpropagation\n",
    "\n",
    "            # Gradient clipping for stability\n",
    "            torch.nn.utils.clip_grad_norm_(list(teacher.parameters()) + list(apprentice.parameters()), max_norm=1.0)\n",
    "\n",
    "            optimizer.step()                                # Update weights\n",
    "        else:\n",
    "            # Just perform inference\n",
    "            teacher.eval()\n",
    "            apprentice.eval()\n",
    "            with torch.no_grad():\n",
    "                x_embedded, memory, latent = teacher.encode(input_image)\n",
    "                reconstructed = apprentice(x_embedded, latent)\n",
    "\n",
    "    # Display the latent space (use the compressed latent vector)\n",
    "    latent_space = latent  # Shape: [batch_size, latent_dim]\n",
    "    latent_space_np = latent_space.detach().cpu().numpy()\n",
    "    # Reshape to square for visualization\n",
    "    latent_dim_sqrt = int(np.ceil(np.sqrt(latent_dim)))\n",
    "    latent_image = np.zeros((latent_dim_sqrt, latent_dim_sqrt))\n",
    "    latent_flat = latent_space_np[0]\n",
    "    latent_image.flat[:latent_flat.size] = latent_flat\n",
    "    # Upscale to match image size\n",
    "    latent_image_resized = cv2.resize(latent_image, (img_size, img_size), interpolation=cv2.INTER_CUBIC)\n",
    "    # Normalize and convert to uint8\n",
    "    latent_image_resized = (255 * (latent_image_resized - latent_image_resized.min()) /\n",
    "                            (latent_image_resized.max() - latent_image_resized.min() + 1e-5)).astype(np.uint8)\n",
    "    latent_img = Image.fromarray(latent_image_resized)\n",
    "    latent_img_tk = ImageTk.PhotoImage(image=latent_img)\n",
    "    latent_label.imgtk = latent_img_tk\n",
    "    latent_label.configure(image=latent_img_tk)\n",
    "\n",
    "    # Display the reconstructed output at input resolution\n",
    "    output_np = reconstructed[0].detach().cpu().numpy().transpose(1, 2, 0)  # Convert to HxWxC\n",
    "    output_np = (255 * output_np).astype(np.uint8)\n",
    "    output_img = Image.fromarray(output_np)\n",
    "    output_img_tk = ImageTk.PhotoImage(image=output_img)\n",
    "    output_label.imgtk = output_img_tk\n",
    "    output_label.configure(image=output_img_tk)\n",
    "\n",
    "    # Update the latent space size label\n",
    "    latent_size = latent_space.size()\n",
    "    latent_size_text = f\"Latent Space Size: {list(latent_space.size())}\"\n",
    "    latent_size_label.config(text=latent_size_text)\n",
    "\n",
    "    # Update knowledge base size\n",
    "    kb_size_bytes = get_knowledge_base_size(knowledge_base_hashes)\n",
    "    if kb_size_bytes < 1024:\n",
    "        knowledge_base_size_var.set(f\"Knowledge Base Size: {kb_size_bytes} bytes\")\n",
    "    else:\n",
    "        kb_size_kb = kb_size_bytes / 1024\n",
    "        knowledge_base_size_var.set(f\"Knowledge Base Size: {kb_size_kb:.2f} KB\")\n",
    "\n",
    "    # Calculate sizes\n",
    "    original_size_kb = frame_array.nbytes / 1024\n",
    "    latent_size_kb = latent_space.element_size() * latent_space.nelement() / 1024\n",
    "    reconstructed_size_kb = output_np.nbytes / 1024\n",
    "\n",
    "    # Update the size info label\n",
    "    size_info_text = f\"Original Image Size: {original_size_kb:.2f} KB | Latent Space Size: {latent_size_kb:.2f} KB | Reconstructed Image Size: {reconstructed_size_kb:.2f} KB\"\n",
    "    size_info_label.config(text=size_info_text)\n",
    "\n",
    "    # Schedule the next frame update\n",
    "    window.after(1, update_frame)\n",
    "\n",
    "# Start video feed update loop\n",
    "update_frame()\n",
    "\n",
    "# Run Tkinter main loop\n",
    "window.mainloop()\n",
    "\n",
    "# Release the video capture on exit\n",
    "cap.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input/Reconstructed Image Size: 588.00 KB\n",
      "Latent Space Size: 0.25 KB\n",
      "Compression Factor (Input/Reconstructed Size to Latent Space Size): 2352.00x smaller\n"
     ]
    }
   ],
   "source": [
    "# Image and latent space dimensions\n",
    "input_width, input_height, channels = 224, 224, 3\n",
    "latent_dim = 64  # Size of the latent space vector\n",
    "\n",
    "# Calculate input and reconstructed image size in bytes\n",
    "image_size_bytes = input_width * input_height * channels * 4  # 32-bit floats\n",
    "image_size_kb = image_size_bytes / 1024  # Convert to KB\n",
    "\n",
    "# Calculate latent space size in bytes\n",
    "latent_size_bytes = latent_dim * 4  # 32-bit floats\n",
    "latent_size_kb = latent_size_bytes / 1024  # Convert to KB\n",
    "\n",
    "# Calculate compression factor\n",
    "compression_factor = image_size_bytes / latent_size_bytes\n",
    "\n",
    "# Display the results\n",
    "print(f\"Input/Reconstructed Image Size: {image_size_kb:.2f} KB\")\n",
    "print(f\"Latent Space Size: {latent_size_kb:.2f} KB\")\n",
    "print(f\"Compression Factor (Input/Reconstructed Size to Latent Space Size): {compression_factor:.2f}x smaller\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
